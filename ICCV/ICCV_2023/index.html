<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
      <link rel="shortcut icon" href="../../img/favicon.ico" />
    <title>ICCV 2023 - AI Papers</title>
    <link rel="stylesheet" href="../../css/theme.css" />
    <link rel="stylesheet" href="../../css/theme_extra.css" />
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/github.min.css" />
        <link href="../../mytheme.css" rel="stylesheet" />
    
      <script>
        // Current page data
        var mkdocs_page_name = "ICCV 2023";
        var mkdocs_page_input_path = "ICCV/ICCV_2023.md";
        var mkdocs_page_url = null;
      </script>
    
    <!--[if lt IE 9]>
      <script src="../../js/html5shiv.min.js"></script>
    <![endif]-->
      <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
      <script>hljs.highlightAll();</script> 
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
          <a href="../.." class="icon icon-home"> AI Papers
        </a><div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../../search.html" method="get">
      <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../..">AI Papers</a>
                </li>
              </ul>
              <p class="caption"><span class="caption-text">ACL</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../ACL/ACL_2023/">ACL 2023</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">CVPR</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../CVPR/CVPR_2023/">CVPR 2023</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../CVPR/CVPR_2024/">CVPR 2024</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">EMNLP</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../EMNLP/EMNLP_2022/">EMNLP 2022</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../EMNLP/EMNLP_2023/">EMNLP 2023</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">ICCV</span></p>
              <ul class="current">
                  <li class="toctree-l1 current"><a class="reference internal current" href="./">ICCV 2023</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">ICLR</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../ICLR/ICLR_2023/">ICLR 2023</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../ICLR/ICLR_2024/">ICLR 2024</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">ICML</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../ICML/ICML_2022/">ICML 2022</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../ICML/ICML_2023/">ICML 2023</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../ICML/ICML_2024/">ICML 2024</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">NeurIPS</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../NeurIPS/NeurIPS_2022/">NeurIPS 2022</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../NeurIPS/NeurIPS_2023/">NeurIPS 2023</a>
                  </li>
              </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      <nav class="wy-nav-top" role="navigation" aria-label="Mobile navigation menu">
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../..">AI Papers</a>
        
      </nav>
      <div class="wy-nav-content">
        <div class="rst-content"><div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../.." class="icon icon-home" aria-label="Docs"></a></li>
          <li class="breadcrumb-item">ICCV</li>
      <li class="breadcrumb-item active">ICCV 2023</li>
    <li class="wy-breadcrumbs-aside">
    </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
            <div class="section" itemprop="articleBody">
              
                <p>Last updated: 2024-09-11 23:58:26. Maintained by <a href="https://wayson.tech/">Weisen Jiang</a>.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">citation</th>
<th style="text-align: center;">date</th>
<th style="text-align: center;">review</th>
<th style="text-align: left;">title (pdf)</th>
<th style="text-align: left;">authors</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">3641</td>
<td style="text-align: center;">2023-04-05</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Kirillov_Segment_Anything_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Kirillov_Segment_Anything_ICCV_2023_paper.pdf">Segment Anything</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">2315</td>
<td style="text-align: center;">2023-02-10</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Zhang_Adding_Conditional_Control_to_Text-to-Image_Diffusion_Models_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Zhang_Adding_Conditional_Control_to_Text-to-Image_Diffusion_Models_ICCV_2023_paper.pdf">Adding Conditional Control to Text-to-Image Diffusion Models</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">717</td>
<td style="text-align: center;">2022-12-19</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Peebles_Scalable_Diffusion_Models_with_Transformers_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Peebles_Scalable_Diffusion_Models_with_Transformers_ICCV_2023_paper.pdf">Scalable Diffusion Models with Transformers</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">630</td>
<td style="text-align: center;">2023-03-20</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Liu_Zero-1-to-3_Zero-shot_One_Image_to_3D_Object_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Liu_Zero-1-to-3_Zero-shot_One_Image_to_3D_Object_ICCV_2023_paper.pdf">Zero-1-to-3: Zero-shot One Image to 3D Object</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">447</td>
<td style="text-align: center;">2022-12-22</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Wu_Tune-A-Video_One-Shot_Tuning_of_Image_Diffusion_Models_for_Text-to-Video_Generation_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Wu_Tune-A-Video_One-Shot_Tuning_of_Image_Diffusion_Models_for_Text-to-Video_Generation_ICCV_2023_paper.pdf">Tune-A-Video: One-Shot Tuning of Image Diffusion Models for Text-to-Video<br />Generation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">385</td>
<td style="text-align: center;">2023-03-24</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Chen_Fantasia3D_Disentangling_Geometry_and_Appearance_for_High-quality_Text-to-3D_Content_Creation_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Chen_Fantasia3D_Disentangling_Geometry_and_Appearance_for_High-quality_Text-to-3D_Content_Creation_ICCV_2023_paper.pdf">Fantasia3D: Disentangling Geometry and Appearance for High-quality Text-to-3D Content<br />Creation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">343</td>
<td style="text-align: center;">2023-02-06</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Esser_Structure_and_Content-Guided_Video_Synthesis_with_Diffusion_Models_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Esser_Structure_and_Content-Guided_Video_Synthesis_with_Diffusion_Models_ICCV_2023_paper.pdf">Structure and Content-Guided Video Synthesis with Diffusion Models</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">338</td>
<td style="text-align: center;">2023-03-23</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Khachatryan_Text2Video-Zero_Text-to-Image_Diffusion_Models_are_Zero-Shot_Video_Generators_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Khachatryan_Text2Video-Zero_Text-to-Image_Diffusion_Models_are_Zero-Shot_Video_Generators_ICCV_2023_paper.pdf">Text2Video-Zero: Text-to-Image Diffusion Models are Zero-Shot Video Generators</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">302</td>
<td style="text-align: center;">2022-11-17</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Chen_DiffusionDet_Diffusion_Model_for_Object_Detection_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Chen_DiffusionDet_Diffusion_Model_for_Object_Detection_ICCV_2023_paper.pdf">DiffusionDet: Diffusion Model for Object Detection</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">288</td>
<td style="text-align: center;">2023-03-14</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Suris_ViperGPT_Visual_Inference_via_Python_Execution_for_Reasoning_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Suris_ViperGPT_Visual_Inference_via_Python_Execution_for_Reasoning_ICCV_2023_paper.pdf">ViperGPT: Visual Inference via Python Execution for Reasoning</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">266</td>
<td style="text-align: center;">2023-03-27</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Zhai_Sigmoid_Loss_for_Language_Image_Pre-Training_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Zhai_Sigmoid_Loss_for_Language_Image_Pre-Training_ICCV_2023_paper.pdf">Sigmoid Loss for Language Image Pre-Training</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">254</td>
<td style="text-align: center;">2022-06-02</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Liu_PETRv2_A_Unified_Framework_for_3D_Perception_from_Multi-Camera_Images_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Liu_PETRv2_A_Unified_Framework_for_3D_Perception_from_Multi-Camera_Images_ICCV_2023_paper.pdf">PETRv2: A Unified Framework for 3D Perception from Multi-Camera<br />Images</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">241</td>
<td style="text-align: center;">2023-03-22</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Haque_Instruct-NeRF2NeRF_Editing_3D_Scenes_with_Instructions_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Haque_Instruct-NeRF2NeRF_Editing_3D_Scenes_with_Instructions_ICCV_2023_paper.pdf">Instruct-NeRF2NeRF: Editing 3D Scenes with Instructions</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">227</td>
<td style="text-align: center;">2023-04-17</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Cao_MasaCtrl_Tuning-Free_Mutual_Self-Attention_Control_for_Consistent_Image_Synthesis_and_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Cao_MasaCtrl_Tuning-Free_Mutual_Self-Attention_Control_for_Consistent_Image_Synthesis_and_ICCV_2023_paper.pdf">MasaCtrl: Tuning-Free Mutual Self-Attention Control for Consistent Image Synthesis<br />and Editing</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">225</td>
<td style="text-align: center;">2023-03-24</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Tang_Make-It-3D_High-fidelity_3D_Creation_from_A_Single_Image_with_Diffusion_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Tang_Make-It-3D_High-fidelity_3D_Creation_from_A_Single_Image_with_Diffusion_ICCV_2023_paper.pdf">Make-It-3D: High-Fidelity 3D Creation from A Single Image with<br />Diffusion Prior</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">223</td>
<td style="text-align: center;">2022-12-08</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Song_LLM-Planner_Few-Shot_Grounded_Planning_for_Embodied_Agents_with_Large_Language_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Song_LLM-Planner_Few-Shot_Grounded_Planning_for_Embodied_Agents_with_Large_Language_ICCV_2023_paper.pdf">LLM-Planner: Few-Shot Grounded Planning for Embodied Agents with Large<br />Language Models</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">203</td>
<td style="text-align: center;">2023-02-27</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Wei_ELITE_Encoding_Visual_Concepts_into_Textual_Embeddings_for_Customized_Text-to-Image_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Wei_ELITE_Encoding_Visual_Concepts_into_Textual_Embeddings_for_Customized_Text-to-Image_ICCV_2023_paper.pdf">ELITE: Encoding Visual Concepts into Textual Embeddings for Customized<br />Text-to-Image Generation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">192</td>
<td style="text-align: center;">2023-03-16</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Kerr_LERF_Language_Embedded_Radiance_Fields_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Kerr_LERF_Language_Embedded_Radiance_Fields_ICCV_2023_paper.pdf">LERF: Language Embedded Radiance Fields</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">167</td>
<td style="text-align: center;">2023-03-23</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Raj_DreamBooth3D_Subject-Driven_Text-to-3D_Generation_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Raj_DreamBooth3D_Subject-Driven_Text-to-3D_Generation_ICCV_2023_paper.pdf">DreamBooth3D: Subject-Driven Text-to-3D Generation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">163</td>
<td style="text-align: center;">2023-04-05</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Chan_Generative_Novel_View_Synthesis_with_3D-Aware_Diffusion_Models_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Chan_Generative_Novel_View_Synthesis_with_3D-Aware_Diffusion_Models_ICCV_2023_paper.pdf">Generative Novel View Synthesis with 3D-Aware Diffusion Models</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">162</td>
<td style="text-align: center;">2022-05-30</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Zhu_Prompt-aligned_Gradient_for_Prompt_Tuning_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Zhu_Prompt-aligned_Gradient_for_Prompt_Tuning_ICCV_2023_paper.pdf">Prompt-aligned Gradient for Prompt Tuning</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">162</td>
<td style="text-align: center;">2023-06-23</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Lindenberger_LightGlue_Local_Feature_Matching_at_Light_Speed_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Lindenberger_LightGlue_Local_Feature_Matching_at_Light_Speed_ICCV_2023_paper.pdf">LightGlue: Local Feature Matching at Light Speed</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">161</td>
<td style="text-align: center;">2023-03-22</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Ceylan_Pix2Video_Video_Editing_using_Image_Diffusion_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Ceylan_Pix2Video_Video_Editing_using_Image_Diffusion_ICCV_2023_paper.pdf">Pix2Video: Video Editing using Image Diffusion</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">156</td>
<td style="text-align: center;">2023-05-17</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Ge_Preserve_Your_Own_Correlation_A_Noise_Prior_for_Video_Diffusion_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Ge_Preserve_Your_Own_Correlation_A_Noise_Prior_for_Video_Diffusion_ICCV_2023_paper.pdf">Preserve Your Own Correlation: A Noise Prior for Video<br />Diffusion Models</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">154</td>
<td style="text-align: center;">2022-11-22</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Zong_DETRs_with_Collaborative_Hybrid_Assignments_Training_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Zong_DETRs_with_Collaborative_Hybrid_Assignments_Training_ICCV_2023_paper.pdf">DETRs with Collaborative Hybrid Assignments Training</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">153</td>
<td style="text-align: center;">2023-03-20</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Han_SVDiff_Compact_Parameter_Space_for_Diffusion_Fine-Tuning_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Han_SVDiff_Compact_Parameter_Space_for_Diffusion_Fine-Tuning_ICCV_2023_paper.pdf">SVDiff: Compact Parameter Space for Diffusion Fine-Tuning</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">151</td>
<td style="text-align: center;">2022-12-05</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Yuan_PhysDiff_Physics-Guided_Human_Motion_Diffusion_Model_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Yuan_PhysDiff_Physics-Guided_Human_Motion_Diffusion_Model_ICCV_2023_paper.pdf">PhysDiff: Physics-Guided Human Motion Diffusion Model</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">150</td>
<td style="text-align: center;">2023-03-13</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Gandikota_Erasing_Concepts_from_Diffusion_Models_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Gandikota_Erasing_Concepts_from_Diffusion_Models_ICCV_2023_paper.pdf">Erasing Concepts from Diffusion Models</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">142</td>
<td style="text-align: center;">2022-12-10</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Wang_NeuS2_Fast_Learning_of_Neural_Implicit_Surfaces_for_Multi-view_Reconstruction_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Wang_NeuS2_Fast_Learning_of_Neural_Implicit_Surfaces_for_Multi-view_Reconstruction_ICCV_2023_paper.pdf">NeuS2: Fast Learning of Neural Implicit Surfaces for Multi-view<br />Reconstruction</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">133</td>
<td style="text-align: center;">2022-09-07</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Pratt_What_Does_a_Platypus_Look_Like_Generating_Customized_Prompts_for_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Pratt_What_Does_a_Platypus_Look_Like_Generating_Customized_Prompts_for_ICCV_2023_paper.pdf">What does a platypus look like? Generating customized prompts<br />for zero-shot image classification</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">118</td>
<td style="text-align: center;">2023-03-21</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Hollein_Text2Room_Extracting_Textured_3D_Meshes_from_2D_Text-to-Image_Models_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Hollein_Text2Room_Extracting_Textured_3D_Meshes_from_2D_Text-to-Image_Models_ICCV_2023_paper.pdf">Text2Room: Extracting Textured 3D Meshes from 2D Text-to-Image Models</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">118</td>
<td style="text-align: center;">2023-03-20</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Chen_Text2Tex_Text-driven_Texture_Synthesis_via_Diffusion_Models_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Chen_Text2Tex_Text-driven_Texture_Synthesis_via_Diffusion_Models_ICCV_2023_paper.pdf">Text2Tex: Text-driven Texture Synthesis via Diffusion Models</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">118</td>
<td style="text-align: center;">2023-03-16</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Wei_SurroundOcc_Multi-camera_3D_Occupancy_Prediction_for_Autonomous_Driving_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Wei_SurroundOcc_Multi-camera_3D_Occupancy_Prediction_for_Autonomous_Driving_ICCV_2023_paper.pdf">SurroundOcc: Multi-Camera 3D Occupancy Prediction for Autonomous Driving</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">116</td>
<td style="text-align: center;">2023-03-03</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Zhao_Unleashing_Text-to-Image_Diffusion_Models_for_Visual_Perception_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Zhao_Unleashing_Text-to-Image_Diffusion_Models_for_Visual_Perception_ICCV_2023_paper.pdf">Unleashing Text-to-Image Diffusion Models for Visual Perception</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">113</td>
<td style="text-align: center;">2023-03-21</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Hu_TIFA_Accurate_and_Interpretable_Text-to-Image_Faithfulness_Evaluation_with_Question_Answering_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Hu_TIFA_Accurate_and_Interpretable_Text-to-Image_Faithfulness_Evaluation_with_Question_Answering_ICCV_2023_paper.pdf">TIFA: Accurate and Interpretable Text-to-Image Faithfulness Evaluation with Question<br />Answering</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">106</td>
<td style="text-align: center;">2023-07-20</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Xie_BoxDiff_Text-to-Image_Synthesis_with_Training-Free_Box-Constrained_Diffusion_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Xie_BoxDiff_Text-to-Image_Synthesis_with_Training-Free_Box-Constrained_Diffusion_ICCV_2023_paper.pdf">BoxDiff: Text-to-Image Synthesis with Training-Free Box-Constrained Diffusion</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">105</td>
<td style="text-align: center;">2023-03-21</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Wang_Exploring_Object-Centric_Temporal_Modeling_for_Efficient_Multi-View_3D_Object_Detection_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Wang_Exploring_Object-Centric_Temporal_Modeling_for_Efficient_Multi-View_3D_Object_Detection_ICCV_2023_paper.pdf">Exploring Object-Centric Temporal Modeling for Efficient Multi-View 3D Object<br />Detection</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">104</td>
<td style="text-align: center;">2023-04-13</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Chen_Single-Stage_Diffusion_NeRF_A_Unified_Approach_to_3D_Generation_and_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Chen_Single-Stage_Diffusion_NeRF_A_Unified_Approach_to_3D_Generation_and_ICCV_2023_paper.pdf">Single-Stage Diffusion NeRF: A Unified Approach to 3D Generation<br />and Reconstruction</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">103</td>
<td style="text-align: center;">2023-01-02</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Liu_CLIP-Driven_Universal_Model_for_Organ_Segmentation_and_Tumor_Detection_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Liu_CLIP-Driven_Universal_Model_for_Organ_Segmentation_and_Tumor_Detection_ICCV_2023_paper.pdf">CLIP-Driven Universal Model for Organ Segmentation and Tumor Detection</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">100</td>
<td style="text-align: center;">2022-02-08</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Cho_DALL-Eval_Probing_the_Reasoning_Skills_and_Social_Biases_of_Text-to-Image_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Cho_DALL-Eval_Probing_the_Reasoning_Skills_and_Social_Biases_of_Text-to-Image_ICCV_2023_paper.pdf">DALL-EVAL: Probing the Reasoning Skills and Social Biases of<br />Text-to-Image Generation Models</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">100</td>
<td style="text-align: center;">2023-03-16</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Li_Large_Selective_Kernel_Network_for_Remote_Sensing_Object_Detection_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Li_Large_Selective_Kernel_Network_for_Remote_Sensing_Object_Detection_ICCV_2023_paper.pdf">Large Selective Kernel Network for Remote Sensing Object Detection</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">99</td>
<td style="text-align: center;">2023-03-23</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Kumari_Ablating_Concepts_in_Text-to-Image_Diffusion_Models_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Kumari_Ablating_Concepts_in_Text-to-Image_Diffusion_Models_ICCV_2023_paper.pdf">Ablating Concepts in Text-to-Image Diffusion Models</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">97</td>
<td style="text-align: center;">2022-10-03</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Huang_CLIP2Point_Transfer_CLIP_to_Point_Cloud_Classification_with_Image-Depth_Pre-Training_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Huang_CLIP2Point_Transfer_CLIP_to_Point_Cloud_Classification_with_Image-Depth_Pre-Training_ICCV_2023_paper.pdf">CLIP2Point: Transfer CLIP to Point Cloud Classification with Image-Depth<br />Pre-Training</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">97</td>
<td style="text-align: center;">2023-08-18</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Chai_StableVideo_Text-driven_Consistency-aware_Diffusion_Video_Editing_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Chai_StableVideo_Text-driven_Consistency-aware_Diffusion_Video_Editing_ICCV_2023_paper.pdf">StableVideo: Text-driven Consistency-aware Diffusion Video Editing</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">96</td>
<td style="text-align: center;">2023-03-16</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Wang_DIRE_for_Diffusion-Generated_Image_Detection_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Wang_DIRE_for_Diffusion-Generated_Image_Detection_ICCV_2023_paper.pdf">DIRE for Diffusion-Generated Image Detection</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">96</td>
<td style="text-align: center;">2023-03-16</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Xia_DiffIR_Efficient_Diffusion_Model_for_Image_Restoration_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Xia_DiffIR_Efficient_Diffusion_Model_for_Image_Restoration_ICCV_2023_paper.pdf">DiffIR: Efficient Diffusion Model for Image Restoration</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">94</td>
<td style="text-align: center;">2023-04-11</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Zhang_OccFormer_Dual-path_Transformer_for_Vision-based_3D_Semantic_Occupancy_Prediction_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Zhang_OccFormer_Dual-path_Transformer_for_Vision-based_3D_Semantic_Occupancy_Prediction_ICCV_2023_paper.pdf">OccFormer: Dual-path Transformer for Vision-based 3D Semantic Occupancy Prediction</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">91</td>
<td style="text-align: center;">2023-03-21</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Wu_DiffuMask_Synthesizing_Images_with_Pixel-level_Annotations_for_Semantic_Segmentation_Using_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Wu_DiffuMask_Synthesizing_Images_with_Pixel-level_Annotations_for_Semantic_Segmentation_Using_ICCV_2023_paper.pdf">DiffuMask: Synthesizing Images with Pixel-level Annotations for Semantic Segmentation<br />Using Diffusion Models</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">89</td>
<td style="text-align: center;">2023-03-07</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Wang_OpenOccupancy_A_Large_Scale_Benchmark_for_Surrounding_Semantic_Occupancy_Perception_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Wang_OpenOccupancy_A_Large_Scale_Benchmark_for_Surrounding_Semantic_Occupancy_Perception_ICCV_2023_paper.pdf">OpenOccupancy: A Large Scale Benchmark for Surrounding Semantic Occupancy<br />Perception</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">89</td>
<td style="text-align: center;">2023-03-28</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Wang_SparseNeRF_Distilling_Depth_Ranking_for_Few-shot_Novel_View_Synthesis_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Wang_SparseNeRF_Distilling_Depth_Ranking_for_Few-shot_Novel_View_Synthesis_ICCV_2023_paper.pdf">SparseNeRF: Distilling Depth Ranking for Few-shot Novel View Synthesis</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">88</td>
<td style="text-align: center;">2022-12-30</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Reed_Scale-MAE_A_Scale-Aware_Masked_Autoencoder_for_Multiscale_Geospatial_Representation_Learning_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Reed_Scale-MAE_A_Scale-Aware_Masked_Autoencoder_for_Multiscale_Geospatial_Representation_Learning_ICCV_2023_paper.pdf">Scale-MAE: A Scale-Aware Masked Autoencoder for Multiscale Geospatial Representation<br />Learning</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">86</td>
<td style="text-align: center;">2023-03-27</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Fernandez_The_Stable_Signature_Rooting_Watermarks_in_Latent_Diffusion_Models_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Fernandez_The_Stable_Signature_Rooting_Watermarks_in_Latent_Diffusion_Models_ICCV_2023_paper.pdf">The Stable Signature: Rooting Watermarks in Latent Diffusion Models</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">85</td>
<td style="text-align: center;">2023-06-08</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Wang_Tracking_Everything_Everywhere_All_at_Once_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Wang_Tracking_Everything_Everywhere_All_at_Once_ICCV_2023_paper.pdf">Tracking Everything Everywhere All at Once</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">83</td>
<td style="text-align: center;">2023-06-05</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Tong_Scene_as_Occupancy_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Tong_Scene_as_Occupancy_ICCV_2023_paper.pdf">Scene as Occupancy</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">82</td>
<td style="text-align: center;">2023-03-16</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Hang_Efficient_Diffusion_Training_via_Min-SNR_Weighting_Strategy_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Hang_Efficient_Diffusion_Training_via_Min-SNR_Weighting_Strategy_ICCV_2023_paper.pdf">Efficient Diffusion Training via Min-SNR Weighting Strategy</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">82</td>
<td style="text-align: center;">2023-02-03</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Ding_MOSE_A_New_Dataset_for_Video_Object_Segmentation_in_Complex_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Ding_MOSE_A_New_Dataset_for_Video_Object_Segmentation_in_Complex_ICCV_2023_paper.pdf">MOSE: A New Dataset for Video Object Segmentation in<br />Complex Scenes</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">82</td>
<td style="text-align: center;">2022-12-15</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Li_Rethinking_Vision_Transformers_for_MobileNet_Size_and_Speed_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Li_Rethinking_Vision_Transformers_for_MobileNet_Size_and_Speed_ICCV_2023_paper.pdf">Rethinking Vision Transformers for MobileNet Size and Speed</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">81</td>
<td style="text-align: center;">2023-03-21</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Jiang_VAD_Vectorized_Scene_Representation_for_Efficient_Autonomous_Driving_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Jiang_VAD_Vectorized_Scene_Representation_for_Efficient_Autonomous_Driving_ICCV_2023_paper.pdf">VAD: Vectorized Scene Representation for Efficient Autonomous Driving</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">79</td>
<td style="text-align: center;">2023-07-21</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Hu_Tri-MipRF_Tri-Mip_Representation_for_Efficient_Anti-Aliasing_Neural_Radiance_Fields_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Hu_Tri-MipRF_Tri-Mip_Representation_for_Efficient_Anti-Aliasing_Neural_Radiance_Fields_ICCV_2023_paper.pdf">Tri-MipRF: Tri-Mip Representation for Efficient Anti-Aliasing Neural Radiance Fields</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">78</td>
<td style="text-align: center;">2023-08-22</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Yeshwanth_ScanNet_A_High-Fidelity_Dataset_of_3D_Indoor_Scenes_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Yeshwanth_ScanNet_A_High-Fidelity_Dataset_of_3D_Indoor_Scenes_ICCV_2023_paper.pdf">ScanNet++: A High-Fidelity Dataset of 3D Indoor Scenes</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">78</td>
<td style="text-align: center;">2023-03-25</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Gao_Masked_Diffusion_Transformer_is_a_Strong_Image_Synthesizer_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Gao_Masked_Diffusion_Transformer_is_a_Strong_Image_Synthesizer_ICCV_2023_paper.pdf">Masked Diffusion Transformer is a Strong Image Synthesizer</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">78</td>
<td style="text-align: center;">2023-03-03</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Tang_Delicate_Textured_Mesh_Recovery_from_NeRF_via_Adaptive_Surface_Refinement_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Tang_Delicate_Textured_Mesh_Recovery_from_NeRF_via_Adaptive_Surface_Refinement_ICCV_2023_paper.pdf">Delicate Textured Mesh Recovery from NeRF via Adaptive Surface<br />Refinement</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">77</td>
<td style="text-align: center;">2022-10-20</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Liu_SimpleClick_Interactive_Image_Segmentation_with_Simple_Vision_Transformers_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Liu_SimpleClick_Interactive_Image_Segmentation_with_Simple_Vision_Transformers_ICCV_2023_paper.pdf">SimpleClick: Interactive Image Segmentation with Simple Vision Transformers</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">77</td>
<td style="text-align: center;">2023-03-17</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Yu_FreeDoM_Training-Free_Energy-Guided_Conditional_Diffusion_Model_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Yu_FreeDoM_Training-Free_Energy-Guided_Conditional_Diffusion_Model_ICCV_2023_paper.pdf">FreeDoM: Training-Free Energy-Guided Conditional Diffusion Model</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">76</td>
<td style="text-align: center;">2023-03-28</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Li_Unmasked_Teacher_Towards_Training-Efficient_Video_Foundation_Models_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Li_Unmasked_Teacher_Towards_Training-Efficient_Video_Foundation_Models_ICCV_2023_paper.pdf">Unmasked Teacher: Towards Training-Efficient Video Foundation Models</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">76</td>
<td style="text-align: center;">2023-03-29</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Erkoc_HyperDiffusion_Generating_Implicit_Neural_Fields_with_Weight-Space_Diffusion_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Erkoc_HyperDiffusion_Generating_Implicit_Neural_Fields_with_Weight-Space_Diffusion_ICCV_2023_paper.pdf">HyperDiffusion: Generating Implicit Neural Fields with Weight-Space Diffusion</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">74</td>
<td style="text-align: center;">2023-04-13</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Shtedritski_What_does_CLIP_know_about_a_red_circle_Visual_prompt_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Shtedritski_What_does_CLIP_know_about_a_red_circle_Visual_prompt_ICCV_2023_paper.pdf">What does CLIP know about a red circle? Visual<br />prompt engineering for VLMs</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">74</td>
<td style="text-align: center;">2023-03-30</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Ji_DDP_Diffusion_Model_for_Dense_Visual_Prediction_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Ji_DDP_Diffusion_Model_for_Dense_Visual_Prediction_ICCV_2023_paper.pdf">DDP: Diffusion Model for Dense Visual Prediction</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">73</td>
<td style="text-align: center;">2022-11-21</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Zhu_PointCLIP_V2_Prompting_CLIP_and_GPT_for_Powerful_3D_Open-world_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Zhu_PointCLIP_V2_Prompting_CLIP_and_GPT_for_Powerful_3D_Open-world_ICCV_2023_paper.pdf">PointCLIP V2: Prompting CLIP and GPT for Powerful 3D<br />Open-world Learning</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">72</td>
<td style="text-align: center;">2023-03-20</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Patashnik_Localizing_Object-Level_Shape_Variations_with_Text-to-Image_Diffusion_Models_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Patashnik_Localizing_Object-Level_Shape_Variations_with_Text-to-Image_Diffusion_Models_ICCV_2023_paper.pdf">Localizing Object-level Shape Variations with Text-to-Image Diffusion Models</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">72</td>
<td style="text-align: center;">2023-04-03</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Zhang_ReMoDiffuse_Retrieval-Augmented_Motion_Diffusion_Model_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Zhang_ReMoDiffuse_Retrieval-Augmented_Motion_Diffusion_Model_ICCV_2023_paper.pdf">ReMoDiffuse: Retrieval-Augmented Motion Diffusion Model</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">68</td>
<td style="text-align: center;">2023-06-14</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Doersch_TAPIR_Tracking_Any_Point_with_Per-Frame_Initialization_and_Temporal_Refinement_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Doersch_TAPIR_Tracking_Any_Point_with_Per-Frame_Initialization_and_Temporal_Refinement_ICCV_2023_paper.pdf">TAPIR: Tracking Any Point with per-frame Initialization and temporal<br />Refinement</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">67</td>
<td style="text-align: center;">2023-02-08</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Li_Q-Diffusion_Quantizing_Diffusion_Models_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Li_Q-Diffusion_Quantizing_Diffusion_Models_ICCV_2023_paper.pdf">Q-Diffusion: Quantizing Diffusion Models</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">67</td>
<td style="text-align: center;">2023-08-01</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Han_FLatten_Transformer_Vision_Transformer_using_Focused_Linear_Attention_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Han_FLatten_Transformer_Vision_Transformer_using_Focused_Linear_Attention_ICCV_2023_paper.pdf">FLatten Transformer: Vision Transformer using Focused Linear Attention</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">67</td>
<td style="text-align: center;">2023-03-24</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Vasu_FastViT_A_Fast_Hybrid_Vision_Transformer_Using_Structural_Reparameterization_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Vasu_FastViT_A_Fast_Hybrid_Vision_Transformer_Using_Structural_Reparameterization_ICCV_2023_paper.pdf">FastViT: A Fast Hybrid Vision Transformer using Structural Reparameterization</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">67</td>
<td style="text-align: center;">2023-06-13</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Szymanowicz_Viewset_Diffusion_0-Image-Conditioned_3D_Generative_Models_from_2D_Data_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Szymanowicz_Viewset_Diffusion_0-Image-Conditioned_3D_Generative_Models_from_2D_Data_ICCV_2023_paper.pdf">Viewset Diffusion: (0-)Image-Conditioned 3D Generative Models from 2D Data</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">66</td>
<td style="text-align: center;">2023-08-24</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Kim_Dense_Text-to-Image_Generation_with_Attention_Modulation_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Kim_Dense_Text-to-Image_Generation_with_Attention_Modulation_ICCV_2023_paper.pdf">Dense Text-to-Image Generation with Attention Modulation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">65</td>
<td style="text-align: center;">2023-03-09</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Kong_Rethinking_Range_View_Representation_for_LiDAR_Segmentation_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Kong_Rethinking_Range_View_Representation_for_LiDAR_Segmentation_ICCV_2023_paper.pdf">Rethinking Range View Representation for LiDAR Segmentation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">64</td>
<td style="text-align: center;">2023-08-07</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Chen_Dual_Aggregation_Transformer_for_Image_Super-Resolution_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Chen_Dual_Aggregation_Transformer_for_Image_Super-Resolution_ICCV_2023_paper.pdf">Dual Aggregation Transformer for Image Super-Resolution</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">64</td>
<td style="text-align: center;">2023-04-09</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Sandstrom_Point-SLAM_Dense_Neural_Point_Cloud-based_SLAM_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Sandstrom_Point-SLAM_Dense_Neural_Point_Cloud-based_SLAM_ICCV_2023_paper.pdf">Point-SLAM: Dense Neural Point Cloud-based SLAM</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">64</td>
<td style="text-align: center;">2022-08-23</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Lee_Hierarchically_Decomposed_Graph_Convolutional_Networks_for_Skeleton-Based_Action_Recognition_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Lee_Hierarchically_Decomposed_Graph_Convolutional_Networks_for_Skeleton-Based_Action_Recognition_ICCV_2023_paper.pdf">Hierarchically Decomposed Graph Convolutional Networks for Skeleton-Based Action Recognition</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">63</td>
<td style="text-align: center;">2023-04-14</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Hertz_Delta_Denoising_Score_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Hertz_Delta_Denoising_Score_ICCV_2023_paper.pdf">Delta Denoising Score</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">62</td>
<td style="text-align: center;">2022-07-26</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Chen_Group_DETR_Fast_DETR_Training_with_Group-Wise_One-to-Many_Assignment_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Chen_Group_DETR_Fast_DETR_Training_with_Group-Wise_One-to-Many_Assignment_ICCV_2023_paper.pdf">Group DETR: Fast DETR Training with Group-Wise One-to-Many Assignment</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">61</td>
<td style="text-align: center;">2023-03-13</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Zhao_DDFM_Denoising_Diffusion_Model_for_Multi-Modality_Image_Fusion_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Zhao_DDFM_Denoising_Diffusion_Model_for_Multi-Modality_Image_Fusion_ICCV_2023_paper.pdf">DDFM: Denoising Diffusion Model for Multi-Modality Image Fusion</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">60</td>
<td style="text-align: center;">2023-08-31</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Xu_InterDiff_Generating_3D_Human-Object_Interactions_with_Physics-Informed_Diffusion_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Xu_InterDiff_Generating_3D_Human-Object_Interactions_with_Physics-Informed_Diffusion_ICCV_2023_paper.pdf">InterDiff: Generating 3D Human-Object Interactions with Physics-Informed Diffusion</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">59</td>
<td style="text-align: center;">2022-10-03</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Hong_Improving_Sample_Quality_of_Diffusion_Models_Using_Self-Attention_Guidance_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Hong_Improving_Sample_Quality_of_Diffusion_Models_Using_Self-Attention_Guidance_ICCV_2023_paper.pdf">Improving Sample Quality of Diffusion Models Using Self-Attention Guidance</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">59</td>
<td style="text-align: center;">2023-03-20</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Peng_EmoTalk_Speech-Driven_Emotional_Disentanglement_for_3D_Face_Animation_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Peng_EmoTalk_Speech-Driven_Emotional_Disentanglement_for_3D_Face_Animation_ICCV_2023_paper.pdf">EmoTalk: Speech-Driven Emotional Disentanglement for 3D Face Animation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">59</td>
<td style="text-align: center;">2022-11-09</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Wu_Exploring_Video_Quality_Assessment_on_User_Generated_Contents_from_Aesthetic_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Wu_Exploring_Video_Quality_Assessment_on_User_Generated_Contents_from_Aesthetic_ICCV_2023_paper.pdf">Exploring Video Quality Assessment on User Generated Contents from<br />Aesthetic and Technical Perspectives</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">59</td>
<td style="text-align: center;">2023-06-06</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Lorraine_ATT3D_Amortized_Text-to-3D_Object_Synthesis_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Lorraine_ATT3D_Amortized_Text-to-3D_Object_Synthesis_ICCV_2023_paper.pdf">ATT3D: Amortized Text-to-3D Object Synthesis</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">58</td>
<td style="text-align: center;">2023-09-05</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Zhang_GO-SLAM_Global_Optimization_for_Consistent_3D_Instant_Reconstruction_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Zhang_GO-SLAM_Global_Optimization_for_Consistent_3D_Instant_Reconstruction_ICCV_2023_paper.pdf">GO-SLAM: Global Optimization for Consistent 3D Instant Reconstruction</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">57</td>
<td style="text-align: center;">2023-03-30</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Jiang_AvatarCraft_Transforming_Text_into_Neural_Human_Avatars_with_Parameterized_Shape_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Jiang_AvatarCraft_Transforming_Text_into_Neural_Human_Avatars_with_Parameterized_Shape_ICCV_2023_paper.pdf">AvatarCraft: Transforming Text into Neural Human Avatars with Parameterized<br />Shape and Pose Control</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">56</td>
<td style="text-align: center;">2023-04-23</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Feng_Score-Based_Diffusion_Models_as_Principled_Priors_for_Inverse_Imaging_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Feng_Score-Based_Diffusion_Models_as_Principled_Priors_for_Inverse_Imaging_ICCV_2023_paper.pdf">Score-Based Diffusion Models as Principled Priors for Inverse Imaging</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">56</td>
<td style="text-align: center;">2023-02-23</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Paiss_Teaching_CLIP_to_Count_to_Ten_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Paiss_Teaching_CLIP_to_Count_to_Ten_ICCV_2023_paper.pdf">Teaching CLIP to Count to Ten</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">54</td>
<td style="text-align: center;">2023-08-18</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Liu_SparseBEV_High-Performance_Sparse_3D_Object_Detection_from_Multi-Camera_Videos_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Liu_SparseBEV_High-Performance_Sparse_3D_Object_Detection_from_Multi-Camera_Videos_ICCV_2023_paper.pdf">SparseBEV: High-Performance Sparse 3D Object Detection from Multi-Camera Videos</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">53</td>
<td style="text-align: center;">2023-07-27</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Zheng_PointOdyssey_A_Large-Scale_Synthetic_Dataset_for_Long-Term_Point_Tracking_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Zheng_PointOdyssey_A_Large-Scale_Synthetic_Dataset_for_Long-Term_Point_Tracking_ICCV_2023_paper.pdf">PointOdyssey: A Large-Scale Synthetic Dataset for Long-Term Point Tracking</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">53</td>
<td style="text-align: center;">2023-03-21</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Sella_Vox-E_Text-Guided_Voxel_Editing_of_3D_Objects_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Sella_Vox-E_Text-Guided_Voxel_Editing_of_3D_Objects_ICCV_2023_paper.pdf">Vox-E: Text-guided Voxel Editing of 3D Objects</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">53</td>
<td style="text-align: center;">2022-07-04</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Li_I-ViT_Integer-only_Quantization_for_Efficient_Vision_Transformer_Inference_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Li_I-ViT_Integer-only_Quantization_for_Efficient_Vision_Transformer_Inference_ICCV_2023_paper.pdf">I-ViT: Integer-only Quantization for Efficient Vision Transformer Inference</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">53</td>
<td style="text-align: center;">2023-07-31</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Lin_UniVTG_Towards_Unified_Video-Language_Temporal_Grounding_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Lin_UniVTG_Towards_Unified_Video-Language_Temporal_Grounding_ICCV_2023_paper.pdf">UniVTG: Towards Unified Video-Language Temporal Grounding</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">53</td>
<td style="text-align: center;">2022-10-12</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Zhu_MotionBERT_A_Unified_Perspective_on_Learning_Human_Motion_Representations_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Zhu_MotionBERT_A_Unified_Perspective_on_Learning_Human_Motion_Representations_ICCV_2023_paper.pdf">MotionBERT: A Unified Perspective on Learning Human Motion Representations</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">52</td>
<td style="text-align: center;">2022-12-30</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Ye_HiTeA_Hierarchical_Temporal-Aware_Video-Language_Pre-training_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Ye_HiTeA_Hierarchical_Temporal-Aware_Video-Language_Pre-training_ICCV_2023_paper.pdf">HiTeA: Hierarchical Temporal-Aware Video-Language Pre-training</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">52</td>
<td style="text-align: center;">2023-03-14</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Orgad_Editing_Implicit_Assumptions_in_Text-to-Image_Diffusion_Models_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Orgad_Editing_Implicit_Assumptions_in_Text-to-Image_Diffusion_Models_ICCV_2023_paper.pdf">Editing Implicit Assumptions in Text-to-Image Diffusion Models</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">52</td>
<td style="text-align: center;">2022-01-03</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Yan_Implicit_Autoencoder_for_Point-Cloud_Self-Supervised_Representation_Learning_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Yan_Implicit_Autoencoder_for_Point-Cloud_Self-Supervised_Representation_Learning_ICCV_2023_paper.pdf">Implicit Autoencoder for Point-Cloud Self-Supervised Representation Learning</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">52</td>
<td style="text-align: center;">2023-04-13</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Ge_Expressive_Text-to-Image_Generation_with_Rich_Text_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Ge_Expressive_Text-to-Image_Generation_with_Rich_Text_ICCV_2023_paper.pdf">Expressive Text-to-Image Generation with Rich Text</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">51</td>
<td style="text-align: center;">2023-09-07</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Cheng_Tracking_Anything_with_Decoupled_Video_Segmentation_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Cheng_Tracking_Anything_with_Decoupled_Video_Segmentation_ICCV_2023_paper.pdf">Tracking Anything with Decoupled Video Segmentation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">51</td>
<td style="text-align: center;">2023-03-23</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Wallace_End-to-End_Diffusion_Latent_Optimization_Improves_Classifier_Guidance_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Wallace_End-to-End_Diffusion_Latent_Optimization_Improves_Classifier_Guidance_ICCV_2023_paper.pdf">End-to-End Diffusion Latent Optimization Improves Classifier Guidance</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">51</td>
<td style="text-align: center;">2023-03-30</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Liang_Iterative_Prompt_Learning_for_Unsupervised_Backlit_Image_Enhancement_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Liang_Iterative_Prompt_Learning_for_Unsupervised_Backlit_Image_Enhancement_ICCV_2023_paper.pdf">Iterative Prompt Learning for Unsupervised Backlit Image Enhancement</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">50</td>
<td style="text-align: center;">2023-08-16</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Ding_MeViS_A_Large-scale_Benchmark_for_Video_Segmentation_with_Motion_Expressions_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Ding_MeViS_A_Large-scale_Benchmark_for_Video_Segmentation_with_Motion_Expressions_ICCV_2023_paper.pdf">MeViS: A Large-scale Benchmark for Video Segmentation with Motion<br />Expressions</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">49</td>
<td style="text-align: center;">2023-03-30</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Kong_Robo3D_Towards_Robust_and_Reliable_3D_Perception_against_Corruptions_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Kong_Robo3D_Towards_Robust_and_Reliable_3D_Perception_against_Corruptions_ICCV_2023_paper.pdf">Robo3D: Towards Robust and Reliable 3D Perception against Corruptions</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">48</td>
<td style="text-align: center;">2023-07-20</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Yin_Metric3D_Towards_Zero-shot_Metric_3D_Prediction_from_A_Single_Image_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Yin_Metric3D_Towards_Zero-shot_Metric_3D_Prediction_from_A_Single_Image_ICCV_2023_paper.pdf">Metric3D: Towards Zero-shot Metric 3D Prediction from A Single<br />Image</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">48</td>
<td style="text-align: center;">2023-08-04</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Liu_Multi-interactive_Feature_Learning_and_a_Full-time_Multi-modality_Benchmark_for_Image_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Liu_Multi-interactive_Feature_Learning_and_a_Full-time_Multi-modality_Benchmark_for_Image_ICCV_2023_paper.pdf">Multi-interactive Feature Learning and a Full-time Multi-modality Benchmark for<br />Image Fusion and Segmentation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">48</td>
<td style="text-align: center;">2023-03-27</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Baldrati_Zero-Shot_Composed_Image_Retrieval_with_Textual_Inversion_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Baldrati_Zero-Shot_Composed_Image_Retrieval_with_Textual_Inversion_ICCV_2023_paper.pdf">Zero-Shot Composed Image Retrieval with Textual Inversion</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">47</td>
<td style="text-align: center;">2023-08-08</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Zhu_3D-VisTA_Pre-trained_Transformer_for_3D_Vision_and_Text_Alignment_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Zhu_3D-VisTA_Pre-trained_Transformer_for_3D_Vision_and_Text_Alignment_ICCV_2023_paper.pdf">3D-VisTA: Pre-trained Transformer for 3D Vision and Text Alignment</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">47</td>
<td style="text-align: center;">2023-04-02</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Wan_UniDexGrasp_Improving_Dexterous_Grasping_Policy_Learning_via_Geometry-Aware_Curriculum_and_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Wan_UniDexGrasp_Improving_Dexterous_Grasping_Policy_Learning_via_Geometry-Aware_Curriculum_and_ICCV_2023_paper.pdf">UniDexGrasp++: Improving Dexterous Grasping Policy Learning via Geometry-aware Curriculum<br />and Iterative Generalist-Specialist Learning</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">47</td>
<td style="text-align: center;">2023-03-10</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Huang_GameFormer_Game-theoretic_Modeling_and_Learning_of_Transformer-based_Interactive_Prediction_and_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Huang_GameFormer_Game-theoretic_Modeling_and_Learning_of_Transformer-based_Interactive_Prediction_and_ICCV_2023_paper.pdf">GameFormer: Game-theoretic Modeling and Learning of Transformer-based Interactive Prediction<br />and Planning for Autonomous Driving</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">46</td>
<td style="text-align: center;">2023-04-09</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Ju_HumanSD_A_Native_Skeleton-Guided_Diffusion_Model_for_Human_Image_Generation_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Ju_HumanSD_A_Native_Skeleton-Guided_Diffusion_Model_for_Human_Image_Generation_ICCV_2023_paper.pdf">HumanSD: A Native Skeleton-Guided Diffusion Model for Human Image<br />Generation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">46</td>
<td style="text-align: center;">2023-02-28</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Liu_DREAM_Efficient_Dataset_Distillation_by_Representative_Matching_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Liu_DREAM_Efficient_Dataset_Distillation_by_Representative_Matching_ICCV_2023_paper.pdf">DREAM: Efficient Dataset Distillation by Representative Matching</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">45</td>
<td style="text-align: center;">2023-08-15</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Wang_StyleDiffusion_Controllable_Disentangled_Style_Transfer_via_Diffusion_Models_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Wang_StyleDiffusion_Controllable_Disentangled_Style_Transfer_via_Diffusion_Models_ICCV_2023_paper.pdf">StyleDiffusion: Controllable Disentangled Style Transfer via Diffusion Models</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">45</td>
<td style="text-align: center;">2023-08-25</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Yi_Diff-Retinex_Rethinking_Low-light_Image_Enhancement_with_A_Generative_Diffusion_Model_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Yi_Diff-Retinex_Rethinking_Low-light_Image_Enhancement_with_A_Generative_Diffusion_Model_ICCV_2023_paper.pdf">Diff-Retinex: Rethinking Low-light Image Enhancement with A Generative Diffusion<br />Model</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">45</td>
<td style="text-align: center;">2022-04-06</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Fang_Unleashing_Vanilla_Vision_Transformer_with_Masked_Image_Modeling_for_Object_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Fang_Unleashing_Vanilla_Vision_Transformer_with_Masked_Image_Modeling_for_Object_ICCV_2023_paper.pdf">Unleashing Vanilla Vision Transformer with Masked Image Modeling for<br />Object Detection</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">44</td>
<td style="text-align: center;">2023-03-31</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Xiang_3D-aware_Image_Generation_using_2D_Diffusion_Models_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Xiang_3D-aware_Image_Generation_using_2D_Diffusion_Models_ICCV_2023_paper.pdf">3D-aware Image Generation using 2D Diffusion Models</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">44</td>
<td style="text-align: center;">2023-09-28</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Seff_MotionLM_Multi-Agent_Motion_Forecasting_as_Language_Modeling_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Seff_MotionLM_Multi-Agent_Motion_Forecasting_as_Language_Modeling_ICCV_2023_paper.pdf">MotionLM: Multi-Agent Motion Forecasting as Language Modeling</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">44</td>
<td style="text-align: center;">2023-03-21</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Shan_Diffusion-Based_3D_Human_Pose_Estimation_with_Multi-Hypothesis_Aggregation_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Shan_Diffusion-Based_3D_Human_Pose_Estimation_with_Multi-Hypothesis_Aggregation_ICCV_2023_paper.pdf">Diffusion-Based 3D Human Pose Estimation with Multi-Hypothesis Aggregation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">44</td>
<td style="text-align: center;">2023-03-14</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Pu_Adaptive_Rotated_Convolution_for_Rotated_Object_Detection_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Pu_Adaptive_Rotated_Convolution_for_Rotated_Object_Detection_ICCV_2023_paper.pdf">Adaptive Rotated Convolution for Rotated Object Detection</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">44</td>
<td style="text-align: center;">2023-08-04</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Li_FB-BEV_BEV_Representation_from_Forward-Backward_View_Transformations_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Li_FB-BEV_BEV_Representation_from_Forward-Backward_View_Transformations_ICCV_2023_paper.pdf">FB-BEV: BEV Representation from Forward-Backward View Transformations</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">43</td>
<td style="text-align: center;">2023-03-31</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Liu_Diffusion_Action_Segmentation_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Liu_Diffusion_Action_Segmentation_ICCV_2023_paper.pdf">Diffusion Action Segmentation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">43</td>
<td style="text-align: center;">2022-03-24</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Zhang_MonoDETR_Depth-guided_Transformer_for_Monocular_3D_Object_Detection_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Zhang_MonoDETR_Depth-guided_Transformer_for_Monocular_3D_Object_Detection_ICCV_2023_paper.pdf">MonoDETR: Depth-guided Transformer for Monocular 3D Object Detection</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">43</td>
<td style="text-align: center;">2023-04-13</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Momeni_Verbs_in_Action_Improving_Verb_Understanding_in_Video-Language_Models_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Momeni_Verbs_in_Action_Improving_Verb_Understanding_in_Video-Language_Models_ICCV_2023_paper.pdf">Verbs in Action: Improving verb understanding in video-language models</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">43</td>
<td style="text-align: center;">2023-03-13</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Bitton-Guetta_Breaking_Common_Sense_WHOOPS_A_Vision-and-Language_Benchmark_of_Synthetic_and_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Bitton-Guetta_Breaking_Common_Sense_WHOOPS_A_Vision-and-Language_Benchmark_of_Synthetic_and_ICCV_2023_paper.pdf">Breaking Common Sense: WHOOPS! A Vision-and-Language Benchmark of Synthetic<br />and Compositional Images</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">42</td>
<td style="text-align: center;">2023-04-11</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Bakr_HRS-Bench_Holistic_Reliable_and_Scalable_Benchmark_for_Text-to-Image_Models_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Bakr_HRS-Bench_Holistic_Reliable_and_Scalable_Benchmark_for_Text-to-Image_Models_ICCV_2023_paper.pdf">HRS-Bench: Holistic, Reliable and Scalable Benchmark for Text-to-Image Models</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">42</td>
<td style="text-align: center;">2023-03-19</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Mikaeili_SKED_Sketch-guided_Text-based_3D_Editing_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Mikaeili_SKED_Sketch-guided_Text-based_3D_Editing_ICCV_2023_paper.pdf">SKED: Sketch-guided Text-based 3D Editing</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">41</td>
<td style="text-align: center;">2023-08-23</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Wang_CLIPN_for_Zero-Shot_OOD_Detection_Teaching_CLIP_to_Say_No_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Wang_CLIPN_for_Zero-Shot_OOD_Detection_Teaching_CLIP_to_Say_No_ICCV_2023_paper.pdf">CLIPN for Zero-Shot OOD Detection: Teaching CLIP to Say<br />No</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">41</td>
<td style="text-align: center;">2023-04-13</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Xie_DiffFit_Unlocking_Transferability_of_Large_Diffusion_Models_via_Simple_Parameter-efficient_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Xie_DiffFit_Unlocking_Transferability_of_Large_Diffusion_Models_via_Simple_Parameter-efficient_ICCV_2023_paper.pdf">DiffFit: Unlocking Transferability of Large Diffusion Models via Simple<br />Parameter-Efficient Fine-Tuning</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">41</td>
<td style="text-align: center;">2021-11-18</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Yang_One-Shot_Generative_Domain_Adaptation_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Yang_One-Shot_Generative_Domain_Adaptation_ICCV_2023_paper.pdf">One-Shot Generative Domain Adaptation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">41</td>
<td style="text-align: center;">2023-03-22</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Hu_SHERF_Generalizable_Human_NeRF_from_a_Single_Image_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Hu_SHERF_Generalizable_Human_NeRF_from_a_Single_Image_ICCV_2023_paper.pdf">SHERF: Generalizable Human NeRF from a Single Image</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">39</td>
<td style="text-align: center;">2023-04-03</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Kim_CRN_Camera_Radar_Net_for_Accurate_Robust_Efficient_3D_Perception_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Kim_CRN_Camera_Radar_Net_for_Accurate_Robust_Efficient_3D_Perception_ICCV_2023_paper.pdf">CRN: Camera Radar Net for Accurate, Robust, Efficient 3D<br />Perception</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">39</td>
<td style="text-align: center;">2022-11-25</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Barquero_BeLFusion_Latent_Diffusion_for_Behavior-Driven_Human_Motion_Prediction_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Barquero_BeLFusion_Latent_Diffusion_for_Behavior-Driven_Human_Motion_Prediction_ICCV_2023_paper.pdf">BeLFusion: Latent Diffusion for Behavior-Driven Human Motion Prediction</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">39</td>
<td style="text-align: center;">2023-04-12</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Butoi_UniverSeg_Universal_Medical_Image_Segmentation_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Butoi_UniverSeg_Universal_Medical_Image_Segmentation_ICCV_2023_paper.pdf">UniverSeg: Universal Medical Image Segmentation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">39</td>
<td style="text-align: center;">2023-03-21</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Yang_Implicit_Neural_Representation_for_Cooperative_Low-light_Image_Enhancement_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Yang_Implicit_Neural_Representation_for_Cooperative_Low-light_Image_Enhancement_ICCV_2023_paper.pdf">Implicit Neural Representation for Cooperative Low-light Image Enhancement</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">39</td>
<td style="text-align: center;">2022-12-16</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Li_RepQ-ViT_Scale_Reparameterization_for_Post-Training_Quantization_of_Vision_Transformers_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Li_RepQ-ViT_Scale_Reparameterization_for_Post-Training_Quantization_of_Vision_Transformers_ICCV_2023_paper.pdf">RepQ-ViT: Scale Reparameterization for Post-Training Quantization of Vision Transformers</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">38</td>
<td style="text-align: center;">2023-08-31</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Ding_PivotNet_Vectorized_Pivot_Learning_for_End-to-end_HD_Map_Construction_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Ding_PivotNet_Vectorized_Pivot_Learning_for_End-to-end_HD_Map_Construction_ICCV_2023_paper.pdf">PivotNet: Vectorized Pivot Learning for End-to-end HD Map Construction</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">38</td>
<td style="text-align: center;">2022-11-29</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Holmquist_DiffPose_Multi-hypothesis_Human_Pose_Estimation_using_Diffusion_Models_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Holmquist_DiffPose_Multi-hypothesis_Human_Pose_Estimation_using_Diffusion_Models_ICCV_2023_paper.pdf">DiffPose: Multi-hypothesis Human Pose Estimation using Diffusion Models</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">38</td>
<td style="text-align: center;">2023-07-11</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Pramanick_EgoVLPv2_Egocentric_Video-Language_Pre-training_with_Fusion_in_the_Backbone_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Pramanick_EgoVLPv2_Egocentric_Video-Language_Pre-training_with_Fusion_in_the_Backbone_ICCV_2023_paper.pdf">EgoVLPv2: Egocentric Video-Language Pre-training with Fusion in the Backbone</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">37</td>
<td style="text-align: center;">2023-06-12</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Roth_Waffling_Around_for_Performance_Visual_Classification_with_Random_Words_and_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Roth_Waffling_Around_for_Performance_Visual_Classification_with_Random_Words_and_ICCV_2023_paper.pdf">Waffling around for Performance: Visual Classification with Random Words<br />and Broad Concepts</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">37</td>
<td style="text-align: center;">2023-03-15</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Shi_VideoFlow_Exploiting_Temporal_Cues_for_Multi-frame_Optical_Flow_Estimation_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Shi_VideoFlow_Exploiting_Temporal_Cues_for_Multi-frame_Optical_Flow_Estimation_ICCV_2023_paper.pdf">VideoFlow: Exploiting Temporal Cues for Multi-frame Optical Flow Estimation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">37</td>
<td style="text-align: center;">2023-06-23</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Couairon_Zero-Shot_Spatial_Layout_Conditioning_for_Text-to-Image_Diffusion_Models_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Couairon_Zero-Shot_Spatial_Layout_Conditioning_for_Text-to-Image_Diffusion_Models_ICCV_2023_paper.pdf">Zero-shot spatial layout conditioning for text-to-image diffusion models</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">36</td>
<td style="text-align: center;">2023-09-07</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Zhou_ProPainter_Improving_Propagation_and_Transformer_for_Video_Inpainting_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Zhou_ProPainter_Improving_Propagation_and_Transformer_for_Video_Inpainting_ICCV_2023_paper.pdf">ProPainter: Improving Propagation and Transformer for Video Inpainting</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">36</td>
<td style="text-align: center;">2023-03-17</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Gao_A_Unified_Continual_Learning_Framework_with_General_Parameter-Efficient_Tuning_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Gao_A_Unified_Continual_Learning_Framework_with_General_Parameter-Efficient_Tuning_ICCV_2023_paper.pdf">A Unified Continual Learning Framework with General Parameter-Efficient Tuning</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">36</td>
<td style="text-align: center;">2023-03-15</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Yang_Zero-Shot_Contrastive_Loss_for_Text-Guided_Diffusion_Image_Style_Transfer_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Yang_Zero-Shot_Contrastive_Loss_for_Text-Guided_Diffusion_Image_Style_Transfer_ICCV_2023_paper.pdf">Zero-Shot Contrastive Loss for Text-Guided Diffusion Image Style Transfer</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">36</td>
<td style="text-align: center;">2023-01-23</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Lin_InfiniCity_Infinite-Scale_City_Synthesis_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Lin_InfiniCity_Infinite-Scale_City_Synthesis_ICCV_2023_paper.pdf">InfiniCity: Infinite-Scale City Synthesis</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">36</td>
<td style="text-align: center;">2022-10-18</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Ranasinghe_Perceptual_Grouping_in_Contrastive_Vision-Language_Models_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Ranasinghe_Perceptual_Grouping_in_Contrastive_Vision-Language_Models_ICCV_2023_paper.pdf">Perceptual Grouping in Contrastive Vision-Language Models</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">36</td>
<td style="text-align: center;">2023-08-01</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Jia_DriveAdapter_Breaking_the_Coupling_Barrier_of_Perception_and_Planning_in_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Jia_DriveAdapter_Breaking_the_Coupling_Barrier_of_Perception_and_Planning_in_ICCV_2023_paper.pdf">DriveAdapter: Breaking the Coupling Barrier of Perception and Planning<br />in End-to-End Autonomous Driving</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">36</td>
<td style="text-align: center;">2023-04-03</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Zhu_Not_All_Features_Matter_Enhancing_Few-shot_CLIP_with_Adaptive_Prior_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Zhu_Not_All_Features_Matter_Enhancing_Few-shot_CLIP_with_Adaptive_Prior_ICCV_2023_paper.pdf">Not All Features Matter: Enhancing Few-shot CLIP with Adaptive<br />Prior Refinement</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">35</td>
<td style="text-align: center;">2023-05-02</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Petrovich_TMR_Text-to-Motion_Retrieval_Using_Contrastive_3D_Human_Motion_Synthesis_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Petrovich_TMR_Text-to-Motion_Retrieval_Using_Contrastive_3D_Human_Motion_Synthesis_ICCV_2023_paper.pdf">TMR: Text-to-Motion Retrieval Using Contrastive 3D Human Motion Synthesis</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">35</td>
<td style="text-align: center;">2023-06-13</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Jaeger_Hidden_Biases_of_End-to-End_Driving_Models_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Jaeger_Hidden_Biases_of_End-to-End_Driving_Models_ICCV_2023_paper.pdf">Hidden Biases of End-to-End Driving Models</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">35</td>
<td style="text-align: center;">2023-06-27</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Wang_PoseDiffusion_Solving_Pose_Estimation_via_Diffusion-aided_Bundle_Adjustment_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Wang_PoseDiffusion_Solving_Pose_Estimation_via_Diffusion-aided_Bundle_Adjustment_ICCV_2023_paper.pdf">PoseDiffusion: Solving Pose Estimation via Diffusion-aided Bundle Adjustment</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">34</td>
<td style="text-align: center;">2023-02-27</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Sun_Spatially-Adaptive_Feature_Modulation_for_Efficient_Image_Super-Resolution_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Sun_Spatially-Adaptive_Feature_Modulation_for_Efficient_Image_Super-Resolution_ICCV_2023_paper.pdf">Spatially-Adaptive Feature Modulation for Efficient Image Super-Resolution</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">34</td>
<td style="text-align: center;">2022-10-02</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Ye_IntrinsicNeRF_Learning_Intrinsic_Neural_Radiance_Fields_for_Editable_Novel_View_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Ye_IntrinsicNeRF_Learning_Intrinsic_Neural_Radiance_Fields_for_Editable_Novel_View_ICCV_2023_paper.pdf">IntrinsicNeRF: Learning Intrinsic Neural Radiance Fields for Editable Novel<br />View Synthesis</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">34</td>
<td style="text-align: center;">2022-12-01</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Chen_UniT3D_A_Unified_Transformer_for_3D_Dense_Captioning_and_Visual_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Chen_UniT3D_A_Unified_Transformer_for_3D_Dense_Captioning_and_Visual_ICCV_2023_paper.pdf">UniT3D: A Unified Transformer for 3D Dense Captioning and<br />Visual Grounding</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">34</td>
<td style="text-align: center;">2022-12-30</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Thambiraja_Imitator_Personalized_Speech-driven_3D_Facial_Animation_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Thambiraja_Imitator_Personalized_Speech-driven_3D_Facial_Animation_ICCV_2023_paper.pdf">Imitator: Personalized Speech-driven 3D Facial Animation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">34</td>
<td style="text-align: center;">2023-05-21</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Zhao_Synthesizing_Diverse_Human_Motions_in_3D_Indoor_Scenes_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Zhao_Synthesizing_Diverse_Human_Motions_in_3D_Indoor_Scenes_ICCV_2023_paper.pdf">Synthesizing Diverse Human Motions in 3D Indoor Scenes</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">34</td>
<td style="text-align: center;">2023-04-07</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Wang_V3Det_Vast_Vocabulary_Visual_Detection_Dataset_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Wang_V3Det_Vast_Vocabulary_Visual_Detection_Dataset_ICCV_2023_paper.pdf">V3Det: Vast Vocabulary Visual Detection Dataset</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">33</td>
<td style="text-align: center;">2023-03-28</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Ma_X-Mesh_Towards_Fast_and_Accurate_Text-driven_3D_Stylization_via_Dynamic_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Ma_X-Mesh_Towards_Fast_and_Accurate_Text-driven_3D_Stylization_via_Dynamic_ICCV_2023_paper.pdf">X-Mesh: Towards Fast and Accurate Text-driven 3D Stylization via<br />Dynamic Textual Guidance</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">33</td>
<td style="text-align: center;">2023-05-03</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Dong_AG3D_Learning_to_Generate_3D_Avatars_from_2D_Image_Collections_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Dong_AG3D_Learning_to_Generate_3D_Avatars_from_2D_Image_Collections_ICCV_2023_paper.pdf">AG3D: Learning to Generate 3D Avatars from 2D Image<br />Collections</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">32</td>
<td style="text-align: center;">2022-11-22</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Liu_DOLCE_A_Model-Based_Probabilistic_Diffusion_Framework_for_Limited-Angle_CT_Reconstruction_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Liu_DOLCE_A_Model-Based_Probabilistic_Diffusion_Framework_for_Limited-Angle_CT_Reconstruction_ICCV_2023_paper.pdf">DOLCE: A Model-Based Probabilistic Diffusion Framework for Limited-Angle CT<br />Reconstruction</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">32</td>
<td style="text-align: center;">2023-03-09</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/He_ICL-D3IE_In-Context_Learning_with_Diverse_Demonstrations_Updating_for_Document_Information_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/He_ICL-D3IE_In-Context_Learning_with_Diverse_Demonstrations_Updating_for_Document_Information_ICCV_2023_paper.pdf">ICL-D3IE: In-Context Learning with Diverse Demonstrations Updating for Document<br />Information Extraction</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">32</td>
<td style="text-align: center;">2023-03-17</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Li_No_Fear_of_Classifier_Biases_Neural_Collapse_Inspired_Federated_Learning_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Li_No_Fear_of_Classifier_Biases_Neural_Collapse_Inspired_Federated_Learning_ICCV_2023_paper.pdf">No Fear of Classifier Biases: Neural Collapse Inspired Federated<br />Learning with Synthetic and Fixed Classifier</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">31</td>
<td style="text-align: center;">2023-04-04</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Baldrati_Multimodal_Garment_Designer_Human-Centric_Latent_Diffusion_Models_for_Fashion_Image_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Baldrati_Multimodal_Garment_Designer_Human-Centric_Latent_Diffusion_Models_for_Fashion_Image_ICCV_2023_paper.pdf">Multimodal Garment Designer: Human-Centric Latent Diffusion Models for Fashion<br />Image Editing</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">31</td>
<td style="text-align: center;">2023-03-30</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Zhang_NeILF_Inter-Reflectable_Light_Fields_for_Geometry_and_Material_Estimation_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Zhang_NeILF_Inter-Reflectable_Light_Fields_for_Geometry_and_Material_Estimation_ICCV_2023_paper.pdf">NeILF++: Inter-Reflectable Light Fields for Geometry and Material Estimation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">31</td>
<td style="text-align: center;">2023-03-19</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Deng_NeRF-LOAM_Neural_Implicit_Representation_for_Large-Scale_Incremental_LiDAR_Odometry_and_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Deng_NeRF-LOAM_Neural_Implicit_Representation_for_Large-Scale_Incremental_LiDAR_Odometry_and_ICCV_2023_paper.pdf">NeRF-LOAM: Neural Implicit Representation for Large-Scale Incremental LiDAR Odometry<br />and Mapping</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">31</td>
<td style="text-align: center;">2023-03-23</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Yang_From_Knowledge_Distillation_to_Self-Knowledge_Distillation_A_Unified_Approach_with_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Yang_From_Knowledge_Distillation_to_Self-Knowledge_Distillation_A_Unified_Approach_with_ICCV_2023_paper.pdf">From Knowledge Distillation to Self-Knowledge Distillation: A Unified Approach<br />with Normalized Loss and Customized Soft Labels</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">31</td>
<td style="text-align: center;">2023-07-17</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Lin_Scale-Aware_Modulation_Meet_Transformer_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Lin_Scale-Aware_Modulation_Meet_Transformer_ICCV_2023_paper.pdf">Scale-Aware Modulation Meet Transformer</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">31</td>
<td style="text-align: center;">2023-01-05</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Ning_All_in_Tokens_Unifying_Output_Space_of_Visual_Tasks_via_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Ning_All_in_Tokens_Unifying_Output_Space_of_Visual_Tasks_via_ICCV_2023_paper.pdf">All in Tokens: Unifying Output Space of Visual Tasks<br />via Soft Token</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">31</td>
<td style="text-align: center;">2023-03-29</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Guo_ViewRefer_Grasp_the_Multi-view_Knowledge_for_3D_Visual_Grounding_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Guo_ViewRefer_Grasp_the_Multi-view_Knowledge_for_3D_Visual_Grounding_ICCV_2023_paper.pdf">ViewRefer: Grasp the Multi-view Knowledge for 3D Visual Grounding</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">30</td>
<td style="text-align: center;">2023-03-12</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Zheng_Preventing_Zero-Shot_Transfer_Degradation_in_Continual_Learning_of_Vision-Language_Models_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Zheng_Preventing_Zero-Shot_Transfer_Degradation_in_Continual_Learning_of_Vision-Language_Models_ICCV_2023_paper.pdf">Preventing Zero-Shot Transfer Degradation in Continual Learning of Vision-Language<br />Models</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">30</td>
<td style="text-align: center;">2023-06-29</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Guizilini_Towards_Zero-Shot_Scale-Aware_Monocular_Depth_Estimation_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Guizilini_Towards_Zero-Shot_Scale-Aware_Monocular_Depth_Estimation_ICCV_2023_paper.pdf">Towards Zero-Shot Scale-Aware Monocular Depth Estimation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">30</td>
<td style="text-align: center;">2023-09-28</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Li_MatrixCity_A_Large-scale_City_Dataset_for_City-scale_Neural_Rendering_and_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Li_MatrixCity_A_Large-scale_City_Dataset_for_City-scale_Neural_Rendering_and_ICCV_2023_paper.pdf">MatrixCity: A Large-scale City Dataset for City-scale Neural Rendering<br />and Beyond</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">30</td>
<td style="text-align: center;">2023-03-17</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Xiang_Denoising_Diffusion_Autoencoders_are_Unified_Self-supervised_Learners_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Xiang_Denoising_Diffusion_Autoencoders_are_Unified_Self-supervised_Learners_ICCV_2023_paper.pdf">Denoising Diffusion Autoencoders are Unified Self-supervised Learners</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">30</td>
<td style="text-align: center;">2023-06-06</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Zhang_DVIS_Decoupled_Video_Instance_Segmentation_Framework_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Zhang_DVIS_Decoupled_Video_Instance_Segmentation_Framework_ICCV_2023_paper.pdf">DVIS: Decoupled Video Instance Segmentation Framework</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">30</td>
<td style="text-align: center;">2023-03-27</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Shaker_SwiftFormer_Efficient_Additive_Attention_for_Transformer-based_Real-time_Mobile_Vision_Applications_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Shaker_SwiftFormer_Efficient_Additive_Attention_for_Transformer-based_Real-time_Mobile_Vision_Applications_ICCV_2023_paper.pdf">SwiftFormer: Efficient Additive Attention for Transformer-based Real-time Mobile Vision<br />Applications</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">30</td>
<td style="text-align: center;">2023-09-26</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Wang_Structure_Invariant_Transformation_for_better_Adversarial_Transferability_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Wang_Structure_Invariant_Transformation_for_better_Adversarial_Transferability_ICCV_2023_paper.pdf">Structure Invariant Transformation for better Adversarial Transferability</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">30</td>
<td style="text-align: center;">2023-03-17</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Jin_DiffusionRet_Generative_Text-Video_Retrieval_with_Diffusion_Model_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Jin_DiffusionRet_Generative_Text-Video_Retrieval_with_Diffusion_Model_ICCV_2023_paper.pdf">DiffusionRet: Generative Text-Video Retrieval with Diffusion Model</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">30</td>
<td style="text-align: center;">2023-04-07</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Wu_Harnessing_the_Spatial-Temporal_Attention_of_Diffusion_Models_for_High-Fidelity_Text-to-Image_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Wu_Harnessing_the_Spatial-Temporal_Attention_of_Diffusion_Models_for_High-Fidelity_Text-to-Image_ICCV_2023_paper.pdf">Harnessing the Spatial-Temporal Attention of Diffusion Models for High-Fidelity<br />Text-to-Image Synthesis</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">29</td>
<td style="text-align: center;">2023-04-19</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Kulhanek_Tetra-NeRF_Representing_Neural_Radiance_Fields_Using_Tetrahedra_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Kulhanek_Tetra-NeRF_Representing_Neural_Radiance_Fields_Using_Tetrahedra_ICCV_2023_paper.pdf">Tetra-NeRF: Representing Neural Radiance Fields Using Tetrahedra</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">29</td>
<td style="text-align: center;">2022-11-30</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Vinker_CLIPascene_Scene_Sketching_with_Different_Types_and_Levels_of_Abstraction_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Vinker_CLIPascene_Scene_Sketching_with_Different_Types_and_Levels_of_Abstraction_ICCV_2023_paper.pdf">CLIPascene: Scene Sketching with Different Types and Levels of<br />Abstraction</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">29</td>
<td style="text-align: center;">2023-01-03</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Zhang_Rethinking_Mobile_Block_for_Efficient_Attention-based_Models_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Zhang_Rethinking_Mobile_Block_for_Efficient_Attention-based_Models_ICCV_2023_paper.pdf">Rethinking Mobile Block for Efficient Attention-based Models</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">29</td>
<td style="text-align: center;">2023-07-18</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Li_Efficient_Region-Aware_Neural_Radiance_Fields_for_High-Fidelity_Talking_Portrait_Synthesis_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Li_Efficient_Region-Aware_Neural_Radiance_Fields_for_High-Fidelity_Talking_Portrait_Synthesis_ICCV_2023_paper.pdf">Efficient Region-Aware Neural Radiance Fields for High-Fidelity Talking Portrait<br />Synthesis</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">29</td>
<td style="text-align: center;">2023-05-10</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Luo_Perpetual_Humanoid_Control_for_Real-time_Simulated_Avatars_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Luo_Perpetual_Humanoid_Control_for_Real-time_Simulated_Avatars_ICCV_2023_paper.pdf">Perpetual Humanoid Control for Real-time Simulated Avatars</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">29</td>
<td style="text-align: center;">2023-03-30</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Cascante-Bonilla_Going_Beyond_Nouns_With_Vision__Language_Models_Using_Synthetic_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Cascante-Bonilla_Going_Beyond_Nouns_With_Vision__Language_Models_Using_Synthetic_ICCV_2023_paper.pdf">Going Beyond Nouns With Vision &amp; Language Models Using<br />Synthetic Data</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">29</td>
<td style="text-align: center;">2023-10-01</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Lai_PADCLIP_Pseudo-labeling_with_Adaptive_Debiasing_in_CLIP_for_Unsupervised_Domain_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Lai_PADCLIP_Pseudo-labeling_with_Adaptive_Debiasing_in_CLIP_for_Unsupervised_Domain_ICCV_2023_paper.pdf">PADCLIP: Pseudo-labeling with Adaptive Debiasing in CLIP for Unsupervised<br />Domain Adaptation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">29</td>
<td style="text-align: center;">2022-11-21</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Wen_Parametric_Classification_for_Generalized_Category_Discovery_A_Baseline_Study_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Wen_Parametric_Classification_for_Generalized_Category_Discovery_A_Baseline_Study_ICCV_2023_paper.pdf">Parametric Classification for Generalized Category Discovery: A Baseline Study</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">29</td>
<td style="text-align: center;">2023-07-27</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Cho_PromptStyler_Prompt-driven_Style_Generation_for_Source-free_Domain_Generalization_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Cho_PromptStyler_Prompt-driven_Style_Generation_for_Source-free_Domain_Generalization_ICCV_2023_paper.pdf">PromptStyler: Prompt-driven Style Generation for Source-free Domain Generalization</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">29</td>
<td style="text-align: center;">2023-07-26</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Yang_Spatio-Temporal_Domain_Awareness_for_Multi-Agent_Collaborative_Perception_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Yang_Spatio-Temporal_Domain_Awareness_for_Multi-Agent_Collaborative_Perception_ICCV_2023_paper.pdf">Spatio-Temporal Domain Awareness for Multi-Agent Collaborative Perception</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">29</td>
<td style="text-align: center;">2022-03-15</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Xu_ActFormer_A_GAN-based_Transformer_towards_General_Action-Conditioned_3D_Human_Motion_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Xu_ActFormer_A_GAN-based_Transformer_towards_General_Action-Conditioned_3D_Human_Motion_ICCV_2023_paper.pdf">ActFormer: A GAN-based Transformer towards General Action-Conditioned 3D Human<br />Motion Generation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">29</td>
<td style="text-align: center;">2023-08-15</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Wang_UniTR_A_Unified_and_Efficient_Multi-Modal_Transformer_for_Birds-Eye-View_Representation_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Wang_UniTR_A_Unified_and_Efficient_Multi-Modal_Transformer_for_Birds-Eye-View_Representation_ICCV_2023_paper.pdf">UniTR: A Unified and Efficient Multi-Modal Transformer for Birds-Eye-View<br />Representation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">29</td>
<td style="text-align: center;">2022-12-05</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Cao_SceneRF_Self-Supervised_Monocular_3D_Scene_Reconstruction_with_Radiance_Fields_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Cao_SceneRF_Self-Supervised_Monocular_3D_Scene_Reconstruction_with_Radiance_Fields_ICCV_2023_paper.pdf">SceneRF: Self-Supervised Monocular 3D Scene Reconstruction with Radiance Fields</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">29</td>
<td style="text-align: center;">2023-09-27</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Chen_NeuRBF_A_Neural_Fields_Representation_with_Adaptive_Radial_Basis_Functions_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Chen_NeuRBF_A_Neural_Fields_Representation_with_Adaptive_Radial_Basis_Functions_ICCV_2023_paper.pdf">NeuRBF: A Neural Fields Representation with Adaptive Radial Basis<br />Functions</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">29</td>
<td style="text-align: center;">2022-11-17</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Gururani_SPACE_Speech-driven_Portrait_Animation_with_Controllable_Expression_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Gururani_SPACE_Speech-driven_Portrait_Animation_with_Controllable_Expression_ICCV_2023_paper.pdf">SPACE: Speech-driven Portrait Animation with Controllable Expression</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">29</td>
<td style="text-align: center;">2023-04-12</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Gleize_SiLK_Simple_Learned_Keypoints_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Gleize_SiLK_Simple_Learned_Keypoints_ICCV_2023_paper.pdf">SiLK: Simple Learned Keypoints</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">29</td>
<td style="text-align: center;">2022-11-19</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Dou_TORE_Token_Reduction_for_Efficient_Human_Mesh_Recovery_with_Transformer_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Dou_TORE_Token_Reduction_for_Efficient_Human_Mesh_Recovery_with_Transformer_ICCV_2023_paper.pdf">TORE: Token Reduction for Efficient Human Mesh Recovery with<br />Transformer</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">28</td>
<td style="text-align: center;">2022-11-19</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Zhou_MatrixVT_Efficient_Multi-Camera_to_BEV_Transformation_for_3D_Perception_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Zhou_MatrixVT_Efficient_Multi-Camera_to_BEV_Transformation_for_3D_Perception_ICCV_2023_paper.pdf">MatrixVT: Efficient Multi-Camera to BEV Transformation for 3D Perception</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">28</td>
<td style="text-align: center;">2023-08-21</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Yu_Texture_Generation_on_3D_Meshes_with_Point-UV_Diffusion_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Yu_Texture_Generation_on_3D_Meshes_with_Point-UV_Diffusion_ICCV_2023_paper.pdf">Texture Generation on 3D Meshes with Point-UV Diffusion</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">28</td>
<td style="text-align: center;">2023-04-05</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Gong_TM2D_Bimodality_Driven_3D_Dance_Generation_via_Music-Text_Integration_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Gong_TM2D_Bimodality_Driven_3D_Dance_Generation_via_Music-Text_Integration_ICCV_2023_paper.pdf">TM2D: Bimodality Driven 3D Dance Generation via Music-Text Integration</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">28</td>
<td style="text-align: center;">2023-09-11</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Zhang_Multi3DRefer_Grounding_Text_Description_to_Multiple_3D_Objects_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Zhang_Multi3DRefer_Grounding_Text_Description_to_Multiple_3D_Objects_ICCV_2023_paper.pdf">Multi3DRefer: Grounding Text Description to Multiple 3D Objects</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">28</td>
<td style="text-align: center;">2022-11-15</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Hataya_Will_Large-scale_Generative_Models_Corrupt_Future_Datasets_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Hataya_Will_Large-scale_Generative_Models_Corrupt_Future_Datasets_ICCV_2023_paper.pdf">Will Large-scale Generative Models Corrupt Future Datasets?</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">28</td>
<td style="text-align: center;">2023-05-18</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Sun_Going_Denser_with_Open-Vocabulary_Part_Segmentation_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Sun_Going_Denser_with_Open-Vocabulary_Part_Segmentation_ICCV_2023_paper.pdf">Going Denser with Open-Vocabulary Part Segmentation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">28</td>
<td style="text-align: center;">2023-07-21</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Xu_Bridging_Vision_and_Language_Encoders_Parameter-Efficient_Tuning_for_Referring_Image_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Xu_Bridging_Vision_and_Language_Encoders_Parameter-Efficient_Tuning_for_Referring_Image_ICCV_2023_paper.pdf">Bridging Vision and Language Encoders: Parameter-Efficient Tuning for Referring<br />Image Segmentation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">28</td>
<td style="text-align: center;">2022-11-18</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Hong_LVOS_A_Benchmark_for_Long-term_Video_Object_Segmentation_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Hong_LVOS_A_Benchmark_for_Long-term_Video_Object_Segmentation_ICCV_2023_paper.pdf">LVOS: A Benchmark for Long-term Video Object Segmentation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">28</td>
<td style="text-align: center;">2022-12-09</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Georgescu_Audiovisual_Masked_Autoencoders_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Georgescu_Audiovisual_Masked_Autoencoders_ICCV_2023_paper.pdf">Audiovisual Masked Autoencoders</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">28</td>
<td style="text-align: center;">2023-08-27</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Qiu_MB-TaylorFormer_Multi-Branch_Efficient_Transformer_Expanded_by_Taylor_Formula_for_Image_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Qiu_MB-TaylorFormer_Multi-Branch_Efficient_Transformer_Expanded_by_Taylor_Formula_for_Image_ICCV_2023_paper.pdf">MB-TaylorFormer: Multi-branch Efficient Transformer Expanded by Taylor Formula for<br />Image Dehazing</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">28</td>
<td style="text-align: center;">2023-02-22</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Hu_Open-domain_Visual_Entity_Recognition_Towards_Recognizing_Millions_of_Wikipedia_Entities_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Hu_Open-domain_Visual_Entity_Recognition_Towards_Recognizing_Millions_of_Wikipedia_Entities_ICCV_2023_paper.pdf">Open-domain Visual Entity Recognition: Towards Recognizing Millions of Wikipedia<br />Entities</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">28</td>
<td style="text-align: center;">2023-04-20</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Xiang_HM-ViT_Hetero-Modal_Vehicle-to-Vehicle_Cooperative_Perception_with_Vision_Transformer_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Xiang_HM-ViT_Hetero-Modal_Vehicle-to-Vehicle_Cooperative_Perception_with_Vision_Transformer_ICCV_2023_paper.pdf">HM-ViT: Hetero-modal Vehicle-to-Vehicle Cooperative Perception with Vision Transformer</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">27</td>
<td style="text-align: center;">2023-04-27</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Xie_SparseFusion_Fusing_Multi-Modal_Sparse_Representations_for_Multi-Sensor_3D_Object_Detection_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Xie_SparseFusion_Fusing_Multi-Modal_Sparse_Representations_for_Multi-Sensor_3D_Object_Detection_ICCV_2023_paper.pdf">SparseFusion: Fusing Multi-Modal Sparse Representations for Multi-Sensor 3D Object<br />Detection</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">27</td>
<td style="text-align: center;">2022-11-10</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Qi_High_Quality_Entity_Segmentation_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Qi_High_Quality_Entity_Segmentation_ICCV_2023_paper.pdf">High Quality Entity Segmentation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">27</td>
<td style="text-align: center;">2023-08-11</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Feng_Diverse_Data_Augmentation_with_Diffusions_for_Effective_Test-time_Prompt_Tuning_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Feng_Diverse_Data_Augmentation_with_Diffusions_for_Effective_Test-time_Prompt_Tuning_ICCV_2023_paper.pdf">Diverse Data Augmentation with Diffusions for Effective Test-time Prompt<br />Tuning</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">27</td>
<td style="text-align: center;">2023-07-26</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Lu_Set-level_Guidance_Attack_Boosting_Adversarial_Transferability_of_Vision-Language_Pre-training_Models_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Lu_Set-level_Guidance_Attack_Boosting_Adversarial_Transferability_of_Vision-Language_Pre-training_Models_ICCV_2023_paper.pdf">Set-level Guidance Attack: Boosting Adversarial Transferability of Vision-Language Pre-training<br />Models</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">27</td>
<td style="text-align: center;">2023-04-06</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Wei_Diffusion_Models_as_Masked_Autoencoders_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Wei_Diffusion_Models_as_Masked_Autoencoders_ICCV_2023_paper.pdf">Diffusion Models as Masked Autoencoders</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">27</td>
<td style="text-align: center;">2023-07-28</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Wang_Scaling_Data_Generation_in_Vision-and-Language_Navigation_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Wang_Scaling_Data_Generation_in_Vision-and-Language_Navigation_ICCV_2023_paper.pdf">Scaling Data Generation in Vision-and-Language Navigation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">27</td>
<td style="text-align: center;">2023-07-26</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Aydemir_ADAPT_Efficient_Multi-Agent_Trajectory_Prediction_with_Adaptation_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Aydemir_ADAPT_Efficient_Multi-Agent_Trajectory_Prediction_with_Adaptation_ICCV_2023_paper.pdf">ADAPT: Efficient Multi-Agent Trajectory Prediction with Adaptation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">27</td>
<td style="text-align: center;">2023-04-10</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Liu_Instance_Neural_Radiance_Field_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Liu_Instance_Neural_Radiance_Field_ICCV_2023_paper.pdf">Instance Neural Radiance Field</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">26</td>
<td style="text-align: center;">2023-04-19</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Mirzaei_Reference-guided_Controllable_Inpainting_of_Neural_Radiance_Fields_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Mirzaei_Reference-guided_Controllable_Inpainting_of_Neural_Radiance_Fields_ICCV_2023_paper.pdf">Reference-guided Controllable Inpainting of Neural Radiance Fields</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">26</td>
<td style="text-align: center;">2022-12-07</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Li_FineDance_A_Fine-grained_Choreography_Dataset_for_3D_Full_Body_Dance_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Li_FineDance_A_Fine-grained_Choreography_Dataset_for_3D_Full_Body_Dance_ICCV_2023_paper.pdf">FineDance: A Fine-grained Choreography Dataset for 3D Full Body<br />Dance Generation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">26</td>
<td style="text-align: center;">2023-03-23</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Liang_ENVIDR_Implicit_Differentiable_Renderer_with_Neural_Environment_Lighting_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Liang_ENVIDR_Implicit_Differentiable_Renderer_with_Neural_Environment_Lighting_ICCV_2023_paper.pdf">ENVIDR: Implicit Differentiable Renderer with Neural Environment Lighting</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">26</td>
<td style="text-align: center;">2023-05-16</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Azadi_Make-An-Animation_Large-Scale_Text-conditional_3D_Human_Motion_Generation_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Azadi_Make-An-Animation_Large-Scale_Text-conditional_3D_Human_Motion_Generation_ICCV_2023_paper.pdf">Make-An-Animation: Large-Scale Text-conditional 3D Human Motion Generation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">26</td>
<td style="text-align: center;">2023-08-30</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Khan_Introducing_Language_Guidance_in_Prompt-based_Continual_Learning_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Khan_Introducing_Language_Guidance_in_Prompt-based_Continual_Learning_ICCV_2023_paper.pdf">Introducing Language Guidance in Prompt-based Continual Learning</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">26</td>
<td style="text-align: center;">2023-05-18</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Basu_Inspecting_the_Geographical_Representativeness_of_Images_from_Text-to-Image_Models_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Basu_Inspecting_the_Geographical_Representativeness_of_Images_from_Text-to-Image_Models_ICCV_2023_paper.pdf">Inspecting the Geographical Representativeness of Images from Text-to-Image Models</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">26</td>
<td style="text-align: center;">2023-04-24</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Zhu_Enhancing_Fine-Tuning_Based_Backdoor_Defense_with_Sharpness-Aware_Minimization_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Zhu_Enhancing_Fine-Tuning_Based_Backdoor_Defense_with_Sharpness-Aware_Minimization_ICCV_2023_paper.pdf">Enhancing Fine-Tuning based Backdoor Defense with Sharpness-Aware Minimization</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">26</td>
<td style="text-align: center;">2023-05-08</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Dong_Prompt_Tuning_Inversion_for_Text-driven_Image_Editing_Using_Diffusion_Models_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Dong_Prompt_Tuning_Inversion_for_Text-driven_Image_Editing_Using_Diffusion_Models_ICCV_2023_paper.pdf">Prompt Tuning Inversion for Text-Driven Image Editing Using Diffusion<br />Models</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">26</td>
<td style="text-align: center;">2022-12-04</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Ren_Multiscale_Structure_Guided_Diffusion_for_Image_Deblurring_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Ren_Multiscale_Structure_Guided_Diffusion_for_Image_Deblurring_ICCV_2023_paper.pdf">Multiscale Structure Guided Diffusion for Image Deblurring</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">26</td>
<td style="text-align: center;">2023-09-11</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Zhang_ITI-GEN_Inclusive_Text-to-Image_Generation_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Zhang_ITI-GEN_Inclusive_Text-to-Image_Generation_ICCV_2023_paper.pdf">ITI-Gen: Inclusive Text-to-Image Generation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">26</td>
<td style="text-align: center;">2023-03-22</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Yuan_Make_Encoder_Great_Again_in_3D_GAN_Inversion_through_Geometry_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Yuan_Make_Encoder_Great_Again_in_3D_GAN_Inversion_through_Geometry_ICCV_2023_paper.pdf">Make Encoder Great Again in 3D GAN Inversion through<br />Geometry and Occlusion-Aware Encoding</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">26</td>
<td style="text-align: center;">2023-08-19</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Cheng_Forecast-MAE_Self-supervised_Pre-training_for_Motion_Forecasting_with_Masked_Autoencoders_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Cheng_Forecast-MAE_Self-supervised_Pre-training_for_Motion_Forecasting_with_Masked_Autoencoders_ICCV_2023_paper.pdf">Forecast-MAE: Self-supervised Pre-training for Motion Forecasting with Masked Autoencoders</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">26</td>
<td style="text-align: center;">2023-03-21</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Bahmani_CC3D_Layout-Conditioned_Generation_of_Compositional_3D_Scenes_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Bahmani_CC3D_Layout-Conditioned_Generation_of_Compositional_3D_Scenes_ICCV_2023_paper.pdf">CC3D: Layout-Conditioned Generation of Compositional 3D Scenes</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">26</td>
<td style="text-align: center;">2023-08-24</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Irshad_NeO_360_Neural_Fields_for_Sparse_View_Synthesis_of_Outdoor_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Irshad_NeO_360_Neural_Fields_for_Sparse_View_Synthesis_of_Outdoor_ICCV_2023_paper.pdf">NeO 360: Neural Fields for Sparse View Synthesis of<br />Outdoor Scenes</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">26</td>
<td style="text-align: center;">2022-12-26</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Tian_MonoNeRF_Learning_a_Generalizable_Dynamic_Radiance_Field_from_Monocular_Videos_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Tian_MonoNeRF_Learning_a_Generalizable_Dynamic_Radiance_Field_from_Monocular_Videos_ICCV_2023_paper.pdf">MonoNeRF: Learning a Generalizable Dynamic Radiance Field from Monocular<br />Videos</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">25</td>
<td style="text-align: center;">2023-04-20</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Athanasiou_SINC_Spatial_Composition_of_3D_Human_Motions_for_Simultaneous_Action_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Athanasiou_SINC_Spatial_Composition_of_3D_Human_Motions_for_Simultaneous_Action_ICCV_2023_paper.pdf">SINC: Spatial Composition of 3D Human Motions for Simultaneous<br />Action Generation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">25</td>
<td style="text-align: center;">2023-09-21</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Wu_TinyCLIP_CLIP_Distillation_via_Affinity_Mimicking_and_Weight_Inheritance_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Wu_TinyCLIP_CLIP_Distillation_via_Affinity_Mimicking_and_Weight_Inheritance_ICCV_2023_paper.pdf">TinyCLIP: CLIP Distillation via Affinity Mimicking and Weight Inheritance</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">25</td>
<td style="text-align: center;">2023-08-21</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Berton_EigenPlaces_Training_Viewpoint_Robust_Models_for_Visual_Place_Recognition_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Berton_EigenPlaces_Training_Viewpoint_Robust_Models_for_Visual_Place_Recognition_ICCV_2023_paper.pdf">EigenPlaces: Training Viewpoint Robust Models for Visual Place Recognition</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">25</td>
<td style="text-align: center;">2023-03-16</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Lee_Robust_Evaluation_of_Diffusion-Based_Adversarial_Purification_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Lee_Robust_Evaluation_of_Diffusion-Based_Adversarial_Purification_ICCV_2023_paper.pdf">Robust Evaluation of Diffusion-Based Adversarial Purification</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">25</td>
<td style="text-align: center;">2023-07-27</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Xu_NeRF-Det_Learning_Geometry-Aware_Volumetric_Representation_for_Multi-View_3D_Object_Detection_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Xu_NeRF-Det_Learning_Geometry-Aware_Volumetric_Representation_for_Multi-View_3D_Object_Detection_ICCV_2023_paper.pdf">NeRF-Det: Learning Geometry-Aware Volumetric Representation for Multi-View 3D Object<br />Detection</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">25</td>
<td style="text-align: center;">2023-07-14</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Wang_Improving_Zero-Shot_Generalization_for_CLIP_with_Synthesized_Prompts_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Wang_Improving_Zero-Shot_Generalization_for_CLIP_with_Synthesized_Prompts_ICCV_2023_paper.pdf">Improving Zero-Shot Generalization for CLIP with Synthesized Prompts</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">25</td>
<td style="text-align: center;">2023-08-21</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Yan_UnLoc_A_Unified_Framework_for_Video_Localization_Tasks_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Yan_UnLoc_A_Unified_Framework_for_Video_Localization_Tasks_ICCV_2023_paper.pdf">UnLoc: A Unified Framework for Video Localization Tasks</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">25</td>
<td style="text-align: center;">2023-07-20</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Lu_Urban_Radiance_Field_Representation_with_Deformable_Neural_Mesh_Primitives_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Lu_Urban_Radiance_Field_Representation_with_Deformable_Neural_Mesh_Primitives_ICCV_2023_paper.pdf">Urban Radiance Field Representation with Deformable Neural Mesh Primitives</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">25</td>
<td style="text-align: center;">2023-03-28</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Li_Unify_Align_and_Refine_Multi-Level_Semantic_Alignment_for_Radiology_Report_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Li_Unify_Align_and_Refine_Multi-Level_Semantic_Alignment_for_Radiology_Report_ICCV_2023_paper.pdf">Unify, Align and Refine: Multi-Level Semantic Alignment for Radiology<br />Report Generation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">25</td>
<td style="text-align: center;">2023-09-12</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Wang_Fg-T2M_Fine-Grained_Text-Driven_Human_Motion_Generation_via_Diffusion_Model_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Wang_Fg-T2M_Fine-Grained_Text-Driven_Human_Motion_Generation_via_Diffusion_Model_ICCV_2023_paper.pdf">Fg-T2M: Fine-Grained Text-Driven Human Motion Generation via Diffusion Model</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">25</td>
<td style="text-align: center;">2023-04-11</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Abdelreheem_SATR_Zero-Shot_Semantic_Segmentation_of_3D_Shapes_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Abdelreheem_SATR_Zero-Shot_Semantic_Segmentation_of_3D_Shapes_ICCV_2023_paper.pdf">SATR: Zero-Shot Semantic Segmentation of 3D Shapes</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">25</td>
<td style="text-align: center;">2023-07-24</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Cheng_PRIOR_Prototype_Representation_Joint_Learning_from_Medical_Images_and_Reports_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Cheng_PRIOR_Prototype_Representation_Joint_Learning_from_Medical_Images_and_Reports_ICCV_2023_paper.pdf">PRIOR: Prototype Representation Joint Learning from Medical Images and<br />Reports</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">24</td>
<td style="text-align: center;">2023-08-16</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Yang_ALIP_Adaptive_Language-Image_Pre-Training_with_Synthetic_Caption_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Yang_ALIP_Adaptive_Language-Image_Pre-Training_with_Synthetic_Caption_ICCV_2023_paper.pdf">ALIP: Adaptive Language-Image Pre-training with Synthetic Caption</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">24</td>
<td style="text-align: center;">2023-02-07</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Chen_HumanMAC_Masked_Motion_Completion_for_Human_Motion_Prediction_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Chen_HumanMAC_Masked_Motion_Completion_for_Human_Motion_Prediction_ICCV_2023_paper.pdf">HumanMAC: Masked Motion Completion for Human Motion Prediction</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">24</td>
<td style="text-align: center;">2023-03-15</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/He_Sensitivity-Aware_Visual_Parameter-Efficient_Fine-Tuning_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/He_Sensitivity-Aware_Visual_Parameter-Efficient_Fine-Tuning_ICCV_2023_paper.pdf">Sensitivity-Aware Visual Parameter-Efficient Fine-Tuning</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">24</td>
<td style="text-align: center;">2023-01-16</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Fang_UATVR_Uncertainty-Adaptive_Text-Video_Retrieval_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Fang_UATVR_Uncertainty-Adaptive_Text-Video_Retrieval_ICCV_2023_paper.pdf">UATVR: Uncertainty-Adaptive Text-Video Retrieval</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">24</td>
<td style="text-align: center;">2023-01-02</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Wu_Betrayed_by_Captions_Joint_Caption_Grounding_and_Generation_for_Open_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Wu_Betrayed_by_Captions_Joint_Caption_Grounding_and_Generation_for_Open_ICCV_2023_paper.pdf">Betrayed by Captions: Joint Caption Grounding and Generation for<br />Open Vocabulary Instance Segmentation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">24</td>
<td style="text-align: center;">2023-03-06</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Bansal_CleanCLIP_Mitigating_Data_Poisoning_Attacks_in_Multimodal_Contrastive_Learning_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Bansal_CleanCLIP_Mitigating_Data_Poisoning_Attacks_in_Multimodal_Contrastive_Learning_ICCV_2023_paper.pdf">CleanCLIP: Mitigating Data Poisoning Attacks in Multimodal Contrastive Learning</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">24</td>
<td style="text-align: center;">2023-03-24</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Wang_Zolly_Zoom_Focal_Length_Correctly_for_Perspective-Distorted_Human_Mesh_Reconstruction_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Wang_Zolly_Zoom_Focal_Length_Correctly_for_Perspective-Distorted_Human_Mesh_Reconstruction_ICCV_2023_paper.pdf">Zolly: Zoom Focal Length Correctly for Perspective-Distorted Human Mesh<br />Reconstruction</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">24</td>
<td style="text-align: center;">2023-03-15</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Lee_Improving_3D_Imaging_with_Pre-Trained_Perpendicular_2D_Diffusion_Models_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Lee_Improving_3D_Imaging_with_Pre-Trained_Perpendicular_2D_Diffusion_Models_ICCV_2023_paper.pdf">Improving 3D Imaging with Pre-Trained Perpendicular 2D Diffusion Models</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">24</td>
<td style="text-align: center;">2023-02-14</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Sargent_VQ3D_Learning_a_3D-Aware_Generative_Model_on_ImageNet_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Sargent_VQ3D_Learning_a_3D-Aware_Generative_Model_on_ImageNet_ICCV_2023_paper.pdf">VQ3D: Learning a 3D-Aware Generative Model on ImageNet</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">24</td>
<td style="text-align: center;">2023-03-20</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Chen_Open-vocabulary_Panoptic_Segmentation_with_Embedding_Modulation_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Chen_Open-vocabulary_Panoptic_Segmentation_with_Embedding_Modulation_ICCV_2023_paper.pdf">Open-vocabulary Panoptic Segmentation with Embedding Modulation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">24</td>
<td style="text-align: center;">2022-11-20</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Hirschorn_Normalizing_Flows_for_Human_Pose_Anomaly_Detection_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Hirschorn_Normalizing_Flows_for_Human_Pose_Anomaly_Detection_ICCV_2023_paper.pdf">Normalizing Flows for Human Pose Anomaly Detection</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">24</td>
<td style="text-align: center;">2023-04-09</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Gong_ARNOLD_A_Benchmark_for_Language-Grounded_Task_Learning_with_Continuous_States_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Gong_ARNOLD_A_Benchmark_for_Language-Grounded_Task_Learning_with_Continuous_States_ICCV_2023_paper.pdf">ARNOLD: A Benchmark for Language-Grounded Task Learning With Continuous<br />States in Realistic 3D Scenes</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">24</td>
<td style="text-align: center;">2023-04-14</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Zha_Instance-aware_Dynamic_Prompt_Tuning_for_Pre-trained_Point_Cloud_Models_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Zha_Instance-aware_Dynamic_Prompt_Tuning_for_Pre-trained_Point_Cloud_Models_ICCV_2023_paper.pdf">Instance-aware Dynamic Prompt Tuning for Pre-trained Point Cloud Models</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">24</td>
<td style="text-align: center;">2023-08-18</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Gasperini_Robust_Monocular_Depth_Estimation_under_Challenging_Conditions_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Gasperini_Robust_Monocular_Depth_Estimation_under_Challenging_Conditions_ICCV_2023_paper.pdf">Robust Monocular Depth Estimation under Challenging Conditions</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">24</td>
<td style="text-align: center;">2023-03-23</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Panos_First_Session_Adaptation_A_Strong_Replay-Free_Baseline_for_Class-Incremental_Learning_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Panos_First_Session_Adaptation_A_Strong_Replay-Free_Baseline_for_Class-Incremental_Learning_ICCV_2023_paper.pdf">First Session Adaptation: A Strong Replay-Free Baseline for Class-Incremental<br />Learning</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">24</td>
<td style="text-align: center;">2023-03-20</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Ge_Ref-NeuS_Ambiguity-Reduced_Neural_Implicit_Surface_Learning_for_Multi-View_Reconstruction_with_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Ge_Ref-NeuS_Ambiguity-Reduced_Neural_Implicit_Surface_Learning_for_Multi-View_Reconstruction_with_ICCV_2023_paper.pdf">Ref-NeuS: Ambiguity-Reduced Neural Implicit Surface Learning for Multi-View Reconstruction<br />with Reflection</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">24</td>
<td style="text-align: center;">2023-03-22</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Liu_RegFormer_An_Efficient_Projection-Aware_Transformer_Network_for_Large-Scale_Point_Cloud_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Liu_RegFormer_An_Efficient_Projection-Aware_Transformer_Network_for_Large-Scale_Point_Cloud_ICCV_2023_paper.pdf">RegFormer: An Efficient Projection-Aware Transformer Network for Large-Scale Point<br />Cloud Registration</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">24</td>
<td style="text-align: center;">2023-07-19</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Cheng_DNA-Rendering_A_Diverse_Neural_Actor_Repository_for_High-Fidelity_Human-Centric_Rendering_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Cheng_DNA-Rendering_A_Diverse_Neural_Actor_Repository_for_High-Fidelity_Human-Centric_Rendering_ICCV_2023_paper.pdf">DNA-Rendering: A Diverse Neural Actor Repository for High-Fidelity Human-centric<br />Rendering</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">24</td>
<td style="text-align: center;">2022-12-06</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Gao_Adaptive_Testing_of_Computer_Vision_Models_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Gao_Adaptive_Testing_of_Computer_Vision_Models_ICCV_2023_paper.pdf">Adaptive Testing of Computer Vision Models</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">23</td>
<td style="text-align: center;">2023-04-03</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Zong_Temporal_Enhanced_Training_of_Multi-view_3D_Object_Detector_via_Historical_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Zong_Temporal_Enhanced_Training_of_Multi-view_3D_Object_Detector_via_Historical_ICCV_2023_paper.pdf">Temporal Enhanced Training of Multi-view 3D Object Detector via<br />Historical Object Prediction</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">23</td>
<td style="text-align: center;">2023-09-02</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Zhong_AttT2M_Text-Driven_Human_Motion_Generation_with_Multi-Perspective_Attention_Mechanism_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Zhong_AttT2M_Text-Driven_Human_Motion_Generation_with_Multi-Perspective_Attention_Mechanism_ICCV_2023_paper.pdf">AttT2M: Text-Driven Human Motion Generation with Multi-Perspective Attention Mechanism</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">23</td>
<td style="text-align: center;">2023-07-26</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Yang_AIDE_A_Vision-Driven_Multi-View_Multi-Modal_Multi-Tasking_Dataset_for_Assistive_Driving_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Yang_AIDE_A_Vision-Driven_Multi-View_Multi-Modal_Multi-Tasking_Dataset_for_Assistive_Driving_ICCV_2023_paper.pdf">AIDE: A Vision-Driven Multi-View, Multi-Modal, Multi-Tasking Dataset for Assistive<br />Driving Perception</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">23</td>
<td style="text-align: center;">2023-07-24</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Zheng_Less_is_More_Focus_Attention_for_Efficient_DETR_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Zheng_Less_is_More_Focus_Attention_for_Efficient_DETR_ICCV_2023_paper.pdf">Less is More: Focus Attention for Efficient DETR</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">23</td>
<td style="text-align: center;">2022-11-15</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Dukic_A_Low-Shot_Object_Counting_Network_With_Iterative_Prototype_Adaptation_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Dukic_A_Low-Shot_Object_Counting_Network_With_Iterative_Prototype_Adaptation_ICCV_2023_paper.pdf">A Low-Shot Object Counting Network With Iterative Prototype Adaptation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">23</td>
<td style="text-align: center;">2023-08-18</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Yuan_Small_Object_Detection_via_Coarse-to-fine_Proposal_Generation_and_Imitation_Learning_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Yuan_Small_Object_Detection_via_Coarse-to-fine_Proposal_Generation_and_Imitation_Learning_ICCV_2023_paper.pdf">Small Object Detection via Coarse-to-fine Proposal Generation and Imitation<br />Learning</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">23</td>
<td style="text-align: center;">2022-12-05</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Huang_One-shot_Implicit_Animatable_Avatars_with_Model-based_Priors_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Huang_One-shot_Implicit_Animatable_Avatars_with_Model-based_Priors_ICCV_2023_paper.pdf">One-shot Implicit Animatable Avatars with Model-based Priors</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">23</td>
<td style="text-align: center;">2022-11-26</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Liu_Residual_Pattern_Learning_for_Pixel-Wise_Out-of-Distribution_Detection_in_Semantic_Segmentation_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Liu_Residual_Pattern_Learning_for_Pixel-Wise_Out-of-Distribution_Detection_in_Semantic_Segmentation_ICCV_2023_paper.pdf">Residual Pattern Learning for Pixel-wise Out-of-Distribution Detection in Semantic<br />Segmentation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">23</td>
<td style="text-align: center;">2023-08-28</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Karnewar_HoloFusion_Towards_Photo-realistic_3D_Generative_Modeling_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Karnewar_HoloFusion_Towards_Photo-realistic_3D_Generative_Modeling_ICCV_2023_paper.pdf">HoloFusion: Towards Photo-realistic 3D Generative Modeling</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">23</td>
<td style="text-align: center;">2023-01-03</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Yan_Cross_Modal_Transformer_Towards_Fast_and_Robust_3D_Object_Detection_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Yan_Cross_Modal_Transformer_Towards_Fast_and_Robust_3D_Object_Detection_ICCV_2023_paper.pdf">Cross Modal Transformer: Towards Fast and Robust 3D Object<br />Detection</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">23</td>
<td style="text-align: center;">2023-03-16</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Han_Global_Knowledge_Calibration_for_Fast_Open-Vocabulary_Segmentation_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Han_Global_Knowledge_Calibration_for_Fast_Open-Vocabulary_Segmentation_ICCV_2023_paper.pdf">Global Knowledge Calibration for Fast Open-Vocabulary Segmentation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">23</td>
<td style="text-align: center;">2023-04-21</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Yu_Long-Term_Photometric_Consistent_Novel_View_Synthesis_with_Diffusion_Models_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Yu_Long-Term_Photometric_Consistent_Novel_View_Synthesis_with_Diffusion_Models_ICCV_2023_paper.pdf">Long-Term Photometric Consistent Novel View Synthesis with Diffusion Models</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">23</td>
<td style="text-align: center;">2022-06-18</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Meister_Gender_Artifacts_in_Visual_Datasets_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Meister_Gender_Artifacts_in_Visual_Datasets_ICCV_2023_paper.pdf">Gender Artifacts in Visual Datasets</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">23</td>
<td style="text-align: center;">2023-05-02</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Huang_Neural_LiDAR_Fields_for_Novel_View_Synthesis_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Huang_Neural_LiDAR_Fields_for_Novel_View_Synthesis_ICCV_2023_paper.pdf">Neural LiDAR Fields for Novel View Synthesis</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">23</td>
<td style="text-align: center;">2022-12-07</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Yu_Talking_Head_Generation_with_Probabilistic_Audio-to-Visual_Diffusion_Priors_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Yu_Talking_Head_Generation_with_Probabilistic_Audio-to-Visual_Diffusion_Priors_ICCV_2023_paper.pdf">Talking Head Generation with Probabilistic Audio-to-Visual Diffusion Priors</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">23</td>
<td style="text-align: center;">2023-09-11</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Ye_Diffusion-Guided_Reconstruction_of_Everyday_Hand-Object_Interaction_Clips_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Ye_Diffusion-Guided_Reconstruction_of_Everyday_Hand-Object_Interaction_Clips_ICCV_2023_paper.pdf">Diffusion-Guided Reconstruction of Everyday Hand-Object Interaction Clips</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">22</td>
<td style="text-align: center;">2023-08-09</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Cai_Robust_Object_Modeling_for_Visual_Tracking_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Cai_Robust_Object_Modeling_for_Visual_Tracking_ICCV_2023_paper.pdf">Robust Object Modeling for Visual Tracking</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">22</td>
<td style="text-align: center;">2023-06-20</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Han_Dynamic_Perceiver_for_Efficient_Visual_Recognition_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Han_Dynamic_Perceiver_for_Efficient_Visual_Recognition_ICCV_2023_paper.pdf">Dynamic Perceiver for Efficient Visual Recognition</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">22</td>
<td style="text-align: center;">2023-09-10</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Pan_Effective_Real_Image_Editing_with_Accelerated_Iterative_Diffusion_Inversion_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Pan_Effective_Real_Image_Editing_with_Accelerated_Iterative_Diffusion_Inversion_ICCV_2023_paper.pdf">Effective Real Image Editing with Accelerated Iterative Diffusion Inversion</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">22</td>
<td style="text-align: center;">2023-01-24</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Puy_Using_a_Waffle_Iron_for_Automotive_Point_Cloud_Semantic_Segmentation_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Puy_Using_a_Waffle_Iron_for_Automotive_Point_Cloud_Semantic_Segmentation_ICCV_2023_paper.pdf">Using a Waffle Iron for Automotive Point Cloud Semantic<br />Segmentation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">22</td>
<td style="text-align: center;">2023-09-03</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Shi_EdaDet_Open-Vocabulary_Object_Detection_Using_Early_Dense_Alignment_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Shi_EdaDet_Open-Vocabulary_Object_Detection_Using_Early_Dense_Alignment_ICCV_2023_paper.pdf">EdaDet: Open-Vocabulary Object Detection Using Early Dense Alignment</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">22</td>
<td style="text-align: center;">2023-06-26</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Agarwal_A-STAR_Test-time_Attention_Segregation_and_Retention_for_Text-to-image_Synthesis_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Agarwal_A-STAR_Test-time_Attention_Segregation_and_Retention_for_Text-to-image_Synthesis_ICCV_2023_paper.pdf">A-STAR: Test-time Attention Segregation and Retention for Text-to-image Synthesis</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">22</td>
<td style="text-align: center;">2023-09-11</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Liu_UniSeg_A_Unified_Multi-Modal_LiDAR_Segmentation_Network_and_the_OpenPCSeg_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Liu_UniSeg_A_Unified_Multi-Modal_LiDAR_Segmentation_Network_and_the_OpenPCSeg_ICCV_2023_paper.pdf">UniSeg: A Unified Multi-Modal LiDAR Segmentation Network and the<br />OpenPCSeg Codebase</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">22</td>
<td style="text-align: center;">2023-07-25</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Miao_Spectrum-guided_Multi-granularity_Referring_Video_Object_Segmentation_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Miao_Spectrum-guided_Multi-granularity_Referring_Video_Object_Segmentation_ICCV_2023_paper.pdf">Spectrum-guided Multi-granularity Referring Video Object Segmentation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">22</td>
<td style="text-align: center;">2023-07-17</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Jiang_Revisiting_Scene_Text_Recognition_A_Data_Perspective_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Jiang_Revisiting_Scene_Text_Recognition_A_Data_Perspective_ICCV_2023_paper.pdf">Revisiting Scene Text Recognition: A Data Perspective</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">22</td>
<td style="text-align: center;">2023-09-18</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Wang_Unified_Coarse-to-Fine_Alignment_for_Video-Text_Retrieval_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Wang_Unified_Coarse-to-Fine_Alignment_for_Video-Text_Retrieval_ICCV_2023_paper.pdf">Unified Coarse-to-Fine Alignment for Video-Text Retrieval</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">22</td>
<td style="text-align: center;">2023-03-30</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Yang_SynBody_Synthetic_Dataset_with_Layered_Human_Models_for_3D_Human_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Yang_SynBody_Synthetic_Dataset_with_Layered_Human_Models_for_3D_Human_ICCV_2023_paper.pdf">SynBody: Synthetic Dataset with Layered Human Models for 3D<br />Human Perception and Modeling</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">22</td>
<td style="text-align: center;">2023-09-16</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Jian_AffordPose_A_Large-Scale_Dataset_of_Hand-Object_Interactions_with_Affordance-Driven_Hand_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Jian_AffordPose_A_Large-Scale_Dataset_of_Hand-Object_Interactions_with_Affordance-Driven_Hand_ICCV_2023_paper.pdf">AffordPose: A Large-scale Dataset of Hand-Object Interactions with Affordance-driven<br />Hand Pose</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">22</td>
<td style="text-align: center;">2022-12-10</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/WU_Source-free_Depth_for_Object_Pop-out_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/WU_Source-free_Depth_for_Object_Pop-out_ICCV_2023_paper.pdf">Source-free Depth for Object Pop-out</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">22</td>
<td style="text-align: center;">2023-06-10</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Pan_Aria_Digital_Twin_A_New_Benchmark_Dataset_for_Egocentric_3D_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Pan_Aria_Digital_Twin_A_New_Benchmark_Dataset_for_Egocentric_3D_ICCV_2023_paper.pdf">Aria Digital Twin: A New Benchmark Dataset for Egocentric<br />3D Machine Perception</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">22</td>
<td style="text-align: center;">2023-07-20</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Cheng_General_Image-to-Image_Translation_with_One-Shot_Image_Guidance_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Cheng_General_Image-to-Image_Translation_with_One-Shot_Image_Guidance_ICCV_2023_paper.pdf">General Image-to-Image Translation with One-Shot Image Guidance</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">22</td>
<td style="text-align: center;">2023-07-18</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Wu_OnlineRefer_A_Simple_Online_Baseline_for_Referring_Video_Object_Segmentation_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Wu_OnlineRefer_A_Simple_Online_Baseline_for_Referring_Video_Object_Segmentation_ICCV_2023_paper.pdf">OnlineRefer: A Simple Online Baseline for Referring Video Object<br />Segmentation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">22</td>
<td style="text-align: center;">2023-07-12</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Yu_GLA-GCN_Global-local_Adaptive_Graph_Convolutional_Network_for_3D_Human_Pose_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Yu_GLA-GCN_Global-local_Adaptive_Graph_Convolutional_Network_for_3D_Human_Pose_ICCV_2023_paper.pdf">GLA-GCN: Global-local Adaptive Graph Convolutional Network for 3D Human<br />Pose Estimation from Monocular Video</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">21</td>
<td style="text-align: center;">2023-09-19</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Li_AutoDiffusion_Training-Free_Optimization_of_Time_Steps_and_Architectures_for_Automated_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Li_AutoDiffusion_Training-Free_Optimization_of_Time_Steps_and_Architectures_for_Automated_ICCV_2023_paper.pdf">AutoDiffusion: Training-Free Optimization of Time Steps and Architectures for<br />Automated Diffusion Model Acceleration</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">21</td>
<td style="text-align: center;">2023-10-01</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Pi_Hierarchical_Generation_of_Human-Object_Interactions_with_Diffusion_Probabilistic_Models_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Pi_Hierarchical_Generation_of_Human-Object_Interactions_with_Diffusion_Probabilistic_Models_ICCV_2023_paper.pdf">Hierarchical Generation of Human-Object Interactions with Diffusion Probabilistic Models</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">21</td>
<td style="text-align: center;">2022-12-11</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Ladune_COOL-CHIC_Coordinate-based_Low_Complexity_Hierarchical_Image_Codec_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Ladune_COOL-CHIC_Coordinate-based_Low_Complexity_Hierarchical_Image_Codec_ICCV_2023_paper.pdf">COOL-CHIC: Coordinate-based Low Complexity Hierarchical Image Codec</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">21</td>
<td style="text-align: center;">2023-08-23</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Ren_SG-Former_Self-guided_Transformer_with_Evolving_Token_Reallocation_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Ren_SG-Former_Self-guided_Transformer_with_Evolving_Token_Reallocation_ICCV_2023_paper.pdf">SG-Former: Self-guided Transformer with Evolving Token Reallocation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">21</td>
<td style="text-align: center;">2023-04-03</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Krantz_Navigating_to_Objects_Specified_by_Images_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Krantz_Navigating_to_Objects_Specified_by_Images_ICCV_2023_paper.pdf">Navigating to Objects Specified by Images</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">21</td>
<td style="text-align: center;">2023-03-21</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Zhang_LayoutDiffusion_Improving_Graphic_Layout_Generation_by_Discrete_Diffusion_Probabilistic_Models_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Zhang_LayoutDiffusion_Improving_Graphic_Layout_Generation_by_Discrete_Diffusion_Probabilistic_Models_ICCV_2023_paper.pdf">LayoutDiffusion: Improving Graphic Layout Generation by Discrete Diffusion Probabilistic<br />Models</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">21</td>
<td style="text-align: center;">2023-06-13</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Robert_Efficient_3D_Semantic_Segmentation_with_Superpoint_Transformer_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Robert_Efficient_3D_Semantic_Segmentation_with_Superpoint_Transformer_ICCV_2023_paper.pdf">Efficient 3D Semantic Segmentation with Superpoint Transformer</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">21</td>
<td style="text-align: center;">2023-08-18</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Xu_MonoNeRD_NeRF-like_Representations_for_Monocular_3D_Object_Detection_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Xu_MonoNeRD_NeRF-like_Representations_for_Monocular_3D_Object_Detection_ICCV_2023_paper.pdf">MonoNeRD: NeRF-like Representations for Monocular 3D Object Detection</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">21</td>
<td style="text-align: center;">2023-07-31</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Abdelfattah_CDUL_CLIP-Driven_Unsupervised_Learning_for_Multi-Label_Image_Classification_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Abdelfattah_CDUL_CLIP-Driven_Unsupervised_Learning_for_Multi-Label_Image_Classification_ICCV_2023_paper.pdf">CDUL: CLIP-Driven Unsupervised Learning for Multi-Label Image Classification</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">21</td>
<td style="text-align: center;">2023-05-15</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Van_Landeghem_Document_Understanding_Dataset_and_Evaluation_DUDE_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Van_Landeghem_Document_Understanding_Dataset_and_Evaluation_DUDE_ICCV_2023_paper.pdf">Document Understanding Dataset and Evaluation (DUDE)</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">21</td>
<td style="text-align: center;">2023-07-27</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Zhou_Gloss-Free_Sign_Language_Translation_Improving_from_Visual-Language_Pretraining_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Zhou_Gloss-Free_Sign_Language_Translation_Improving_from_Visual-Language_Pretraining_ICCV_2023_paper.pdf">Gloss-free Sign Language Translation: Improving from Visual-Language Pretraining</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">21</td>
<td style="text-align: center;">2022-09-12</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Kil_PreSTU_Pre-Training_for_Scene-Text_Understanding_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Kil_PreSTU_Pre-Training_for_Scene-Text_Understanding_ICCV_2023_paper.pdf">PreSTU: Pre-Training for Scene-Text Understanding</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">21</td>
<td style="text-align: center;">2023-07-25</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Nandan_Unmasking_Anomalies_in_Road-Scene_Segmentation_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Nandan_Unmasking_Anomalies_in_Road-Scene_Segmentation_ICCV_2023_paper.pdf">Unmasking Anomalies in Road-Scene Segmentation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">21</td>
<td style="text-align: center;">2023-03-22</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Ye_FeatureNeRF_Learning_Generalizable_NeRFs_by_Distilling_Foundation_Models_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Ye_FeatureNeRF_Learning_Generalizable_NeRFs_by_Distilling_Foundation_Models_ICCV_2023_paper.pdf">FeatureNeRF: Learning Generalizable NeRFs by Distilling Foundation Models</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">21</td>
<td style="text-align: center;">2023-07-09</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Zhu_Cross-Modal_Orthogonal_High-Rank_Augmentation_for_RGB-Event_Transformer-Trackers_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Zhu_Cross-Modal_Orthogonal_High-Rank_Augmentation_for_RGB-Event_Transformer-Trackers_ICCV_2023_paper.pdf">Cross-modal Orthogonal High-rank Augmentation for RGB-Event Transformer-trackers</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">20</td>
<td style="text-align: center;">2023-09-22</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Zhou_Cross-Modal_Translation_and_Alignment_for_Survival_Analysis_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Zhou_Cross-Modal_Translation_and_Alignment_for_Survival_Analysis_ICCV_2023_paper.pdf">Cross-Modal Translation and Alignment for Survival Analysis</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">20</td>
<td style="text-align: center;">2023-07-24</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Zhu_A_Good_Student_is_Cooperative_and_Reliable_CNN-Transformer_Collaborative_Learning_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Zhu_A_Good_Student_is_Cooperative_and_Reliable_CNN-Transformer_Collaborative_Learning_ICCV_2023_paper.pdf">A Good Student is Cooperative and Reliable: CNN-Transformer Collaborative<br />Learning for Semantic Segmentation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">20</td>
<td style="text-align: center;">2023-08-21</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Yang_Diffusion_Model_as_Representation_Learner_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Yang_Diffusion_Model_as_Representation_Learner_ICCV_2023_paper.pdf">Diffusion Model as Representation Learner</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">20</td>
<td style="text-align: center;">2022-12-03</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Li_StegaNeRF_Embedding_Invisible_Information_within_Neural_Radiance_Fields_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Li_StegaNeRF_Embedding_Invisible_Information_within_Neural_Radiance_Fields_ICCV_2023_paper.pdf">StegaNeRF: Embedding Invisible Information within Neural Radiance Fields</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">20</td>
<td style="text-align: center;">2023-03-27</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Wang_DyGait_Exploiting_Dynamic_Representations_for_High-performance_Gait_Recognition_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Wang_DyGait_Exploiting_Dynamic_Representations_for_High-performance_Gait_Recognition_ICCV_2023_paper.pdf">DyGait: Exploiting Dynamic Representations for High-performance Gait Recognition</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">20</td>
<td style="text-align: center;">2023-08-28</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Kong_Priority-Centric_Human_Motion_Generation_in_Discrete_Latent_Space_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Kong_Priority-Centric_Human_Motion_Generation_in_Discrete_Latent_Space_ICCV_2023_paper.pdf">Priority-Centric Human Motion Generation in Discrete Latent Space</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">20</td>
<td style="text-align: center;">2023-04-10</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Liu_Detection_Transformer_with_Stable_Matching_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Liu_Detection_Transformer_with_Stable_Matching_ICCV_2023_paper.pdf">Detection Transformer with Stable Matching</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">20</td>
<td style="text-align: center;">2023-04-26</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Sun_Neural-PBIR_Reconstruction_of_Shape_Material_and_Illumination_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Sun_Neural-PBIR_Reconstruction_of_Shape_Material_and_Illumination_ICCV_2023_paper.pdf">Neural-PBIR Reconstruction of Shape, Material, and Illumination</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">20</td>
<td style="text-align: center;">2023-02-16</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Nie_Parallax-Tolerant_Unsupervised_Deep_Image_Stitching_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Nie_Parallax-Tolerant_Unsupervised_Deep_Image_Stitching_ICCV_2023_paper.pdf">Parallax-Tolerant Unsupervised Deep Image Stitching</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">20</td>
<td style="text-align: center;">2022-12-20</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Jiang_Full-Body_Articulated_Human-Object_Interaction_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Jiang_Full-Body_Articulated_Human-Object_Interaction_ICCV_2023_paper.pdf">Full-Body Articulated Human-Object Interaction</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">20</td>
<td style="text-align: center;">2023-04-20</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Tu_Implicit_Temporal_Modeling_with_Learnable_Alignment_for_Video_Recognition_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Tu_Implicit_Temporal_Modeling_with_Learnable_Alignment_for_Video_Recognition_ICCV_2023_paper.pdf">Implicit Temporal Modeling with Learnable Alignment for Video Recognition</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">20</td>
<td style="text-align: center;">2023-01-09</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Lee_Locomotion-Action-Manipulation_Synthesizing_Human-Scene_Interactions_in_Complex_3D_Environments_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Lee_Locomotion-Action-Manipulation_Synthesizing_Human-Scene_Interactions_in_Complex_3D_Environments_ICCV_2023_paper.pdf">Locomotion-Action-Manipulation: Synthesizing Human-Scene Interactions in Complex 3D Environments</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">20</td>
<td style="text-align: center;">2023-09-19</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Shao_NDDepth_Normal-Distance_Assisted_Monocular_Depth_Estimation_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Shao_NDDepth_Normal-Distance_Assisted_Monocular_Depth_Estimation_ICCV_2023_paper.pdf">NDDepth: Normal-Distance Assisted Monocular Depth Estimation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">20</td>
<td style="text-align: center;">2023-09-29</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Wang_HoloAssist_an_Egocentric_Human_Interaction_Dataset_for_Interactive_AI_Assistants_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Wang_HoloAssist_an_Egocentric_Human_Interaction_Dataset_for_Interactive_AI_Assistants_ICCV_2023_paper.pdf">HoloAssist: an Egocentric Human Interaction Dataset for Interactive AI<br />Assistants in the Real World</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">20</td>
<td style="text-align: center;">2023-09-29</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Guo_Forward_Flow_for_Novel_View_Synthesis_of_Dynamic_Scenes_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Guo_Forward_Flow_for_Novel_View_Synthesis_of_Dynamic_Scenes_ICCV_2023_paper.pdf">Forward Flow for Novel View Synthesis of Dynamic Scenes</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">20</td>
<td style="text-align: center;">2023-07-19</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Zhao_Generative_Prompt_Model_for_Weakly_Supervised_Object_Localization_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Zhao_Generative_Prompt_Model_for_Weakly_Supervised_Object_Localization_ICCV_2023_paper.pdf">Generative Prompt Model for Weakly Supervised Object Localization</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">20</td>
<td style="text-align: center;">2023-03-16</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Svitov_DINAR_Diffusion_Inpainting_of_Neural_Textures_for_One-Shot_Human_Avatars_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Svitov_DINAR_Diffusion_Inpainting_of_Neural_Textures_for_One-Shot_Human_Avatars_ICCV_2023_paper.pdf">DINAR: Diffusion Inpainting of Neural Textures for One-Shot Human<br />Avatars</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">20</td>
<td style="text-align: center;">2022-11-26</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Nayal_RbA_Segmenting_Unknown_Regions_Rejected_by_All_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Nayal_RbA_Segmenting_Unknown_Regions_Rejected_by_All_ICCV_2023_paper.pdf">RbA: Segmenting Unknown Regions Rejected by All</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">19</td>
<td style="text-align: center;">2023-03-12</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Chen_Traj-MAE_Masked_Autoencoders_for_Trajectory_Prediction_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Chen_Traj-MAE_Masked_Autoencoders_for_Trajectory_Prediction_ICCV_2023_paper.pdf">Traj-MAE: Masked Autoencoders for Trajectory Prediction</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">19</td>
<td style="text-align: center;">2023-03-21</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Koo_SALAD_Part-Level_Latent_Diffusion_for_3D_Shape_Generation_and_Manipulation_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Koo_SALAD_Part-Level_Latent_Diffusion_for_3D_Shape_Generation_and_Manipulation_ICCV_2023_paper.pdf">SALAD: Part-Level Latent Diffusion for 3D Shape Generation and<br />Manipulation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">19</td>
<td style="text-align: center;">2022-08-19</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Huang_SAFARI_Versatile_and_Efficient_Evaluations_for_Robustness_of_Interpretability_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Huang_SAFARI_Versatile_and_Efficient_Evaluations_for_Robustness_of_Interpretability_ICCV_2023_paper.pdf">SAFARI: Versatile and Efficient Evaluations for Robustness of Interpretability</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">19</td>
<td style="text-align: center;">2023-05-10</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Papantoniou_Relightify_Relightable_3D_Faces_from_a_Single_Image_via_Diffusion_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Papantoniou_Relightify_Relightable_3D_Faces_from_a_Single_Image_via_Diffusion_ICCV_2023_paper.pdf">Relightify: Relightable 3D Faces from a Single Image via<br />Diffusion Models</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">19</td>
<td style="text-align: center;">2023-05-29</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Chen_DiffRate__Differentiable_Compression_Rate_for_Efficient_Vision_Transformers_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Chen_DiffRate__Differentiable_Compression_Rate_for_Efficient_Vision_Transformers_ICCV_2023_paper.pdf">DiffRate : Differentiable Compression Rate for Efficient Vision Transformers</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">19</td>
<td style="text-align: center;">2023-10-01</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Peng_GET_Group_Event_Transformer_for_Event-Based_Vision_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Peng_GET_Group_Event_Transformer_for_Event-Based_Vision_ICCV_2023_paper.pdf">GET: Group Event Transformer for Event-Based Vision</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">19</td>
<td style="text-align: center;">2023-08-29</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Yang_Efficient_Model_Personalization_in_Federated_Learning_via_Client-Specific_Prompt_Generation_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Yang_Efficient_Model_Personalization_in_Federated_Learning_via_Client-Specific_Prompt_Generation_ICCV_2023_paper.pdf">Efficient Model Personalization in Federated Learning via Client-Specific Prompt<br />Generation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">19</td>
<td style="text-align: center;">2023-01-05</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Xu_CiT_Curation_in_Training_for_Effective_Vision-Language_Data_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Xu_CiT_Curation_in_Training_for_Effective_Vision-Language_Data_ICCV_2023_paper.pdf">CiT: Curation in Training for Effective Vision-Language Data</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">19</td>
<td style="text-align: center;">2023-08-08</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Luo_LATR_3D_Lane_Detection_from_Monocular_Images_with_Transformer_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Luo_LATR_3D_Lane_Detection_from_Monocular_Images_with_Transformer_ICCV_2023_paper.pdf">LATR: 3D Lane Detection from Monocular Images with Transformer</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">19</td>
<td style="text-align: center;">2023-02-28</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Luo_BEVPlace_Learning_LiDAR-based_Place_Recognition_using_Birds_Eye_View_Images_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Luo_BEVPlace_Learning_LiDAR-based_Place_Recognition_using_Birds_Eye_View_Images_ICCV_2023_paper.pdf">BEVPlace: Learning LiDAR-based Place Recognition using Birds Eye View<br />Images</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">19</td>
<td style="text-align: center;">2023-07-24</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Wang_GridMM_Grid_Memory_Map_for_Vision-and-Language_Navigation_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Wang_GridMM_Grid_Memory_Map_for_Vision-and-Language_Navigation_ICCV_2023_paper.pdf">GridMM: Grid Memory Map for Vision-and-Language Navigation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">19</td>
<td style="text-align: center;">2023-06-14</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Xu_Multimodal_Optimal_Transport-based_Co-Attention_Transformer_with_Global_Structure_Consistency_for_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Xu_Multimodal_Optimal_Transport-based_Co-Attention_Transformer_with_Global_Structure_Consistency_for_ICCV_2023_paper.pdf">Multimodal Optimal Transport-based Co-Attention Transformer with Global Structure Consistency<br />for Survival Prediction</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">19</td>
<td style="text-align: center;">2023-07-16</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Yang_Cross-Ray_Neural_Radiance_Fields_for_Novel-View_Synthesis_from_Unconstrained_Image_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Yang_Cross-Ray_Neural_Radiance_Fields_for_Novel-View_Synthesis_from_Unconstrained_Image_ICCV_2023_paper.pdf">Cross-Ray Neural Radiance Fields for Novel-view Synthesis from Unconstrained<br />Image Collections</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">19</td>
<td style="text-align: center;">2023-03-15</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Zbinden_Stochastic_Segmentation_with_Conditional_Categorical_Diffusion_Models_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Zbinden_Stochastic_Segmentation_with_Conditional_Categorical_Diffusion_Models_ICCV_2023_paper.pdf">Stochastic Segmentation with Conditional Categorical Diffusion Models</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">19</td>
<td style="text-align: center;">2023-10-01</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Xu_MasQCLIP_for_Open-Vocabulary_Universal_Image_Segmentation_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Xu_MasQCLIP_for_Open-Vocabulary_Universal_Image_Segmentation_ICCV_2023_paper.pdf">MasQCLIP for Open-Vocabulary Universal Image Segmentation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">19</td>
<td style="text-align: center;">2023-06-08</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Liu_Unsupervised_Compositional_Concepts_Discovery_with_Text-to-Image_Generative_Models_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Liu_Unsupervised_Compositional_Concepts_Discovery_with_Text-to-Image_Generative_Models_ICCV_2023_paper.pdf">Unsupervised Compositional Concepts Discovery with Text-to-Image Generative Models</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">19</td>
<td style="text-align: center;">2023-08-23</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Wang_Does_Physical_Adversarial_Example_Really_Matter_to_Autonomous_Driving_Towards_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Wang_Does_Physical_Adversarial_Example_Really_Matter_to_Autonomous_Driving_Towards_ICCV_2023_paper.pdf">Does Physical Adversarial Example Really Matter to Autonomous Driving?<br />Towards System-Level Effect of Adversarial Object Evasion Attack</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">19</td>
<td style="text-align: center;">2023-03-12</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Miao_DDS2M_Self-Supervised_Denoising_Diffusion_Spatio-Spectral_Model_for_Hyperspectral_Image_Restoration_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Miao_DDS2M_Self-Supervised_Denoising_Diffusion_Spatio-Spectral_Model_for_Hyperspectral_Image_Restoration_ICCV_2023_paper.pdf">DDS2M: Self-Supervised Denoising Diffusion Spatio-Spectral Model for Hyperspectral Image<br />Restoration</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">18</td>
<td style="text-align: center;">2023-07-27</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Feng_Clustering_based_Point_Cloud_Representation_Learning_for_3D_Analysis_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Feng_Clustering_based_Point_Cloud_Representation_Learning_for_3D_Analysis_ICCV_2023_paper.pdf">Clustering based Point Cloud Representation Learning for 3D Analysis</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">18</td>
<td style="text-align: center;">2022-10-05</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Derakhshani_Bayesian_Prompt_Learning_for_Image-Language_Model_Generalization_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Derakhshani_Bayesian_Prompt_Learning_for_Image-Language_Model_Generalization_ICCV_2023_paper.pdf">Bayesian Prompt Learning for Image-Language Model Generalization</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">18</td>
<td style="text-align: center;">2023-07-28</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Tang_Multiple_Instance_Learning_Framework_with_Masked_Hard_Instance_Mining_for_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Tang_Multiple_Instance_Learning_Framework_with_Masked_Hard_Instance_Mining_for_ICCV_2023_paper.pdf">Multiple Instance Learning Framework with Masked Hard Instance Mining<br />for Whole Slide Image Classification</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">18</td>
<td style="text-align: center;">2023-08-16</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Guo_Membrane_Potential_Batch_Normalization_for_Spiking_Neural_Networks_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Guo_Membrane_Potential_Batch_Normalization_for_Spiking_Neural_Networks_ICCV_2023_paper.pdf">Membrane Potential Batch Normalization for Spiking Neural Networks</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">18</td>
<td style="text-align: center;">2023-07-20</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Bounareli_HyperReenact_One-Shot_Reenactment_via_Jointly_Learning_to_Refine_and_Retarget_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Bounareli_HyperReenact_One-Shot_Reenactment_via_Jointly_Learning_to_Refine_and_Retarget_ICCV_2023_paper.pdf">HyperReenact: One-Shot Reenactment via Jointly Learning to Refine and<br />Retarget Faces</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">18</td>
<td style="text-align: center;">2022-10-10</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Bulat_FS-DETR_Few-Shot_DEtection_TRansformer_with_Prompting_and_without_Re-Training_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Bulat_FS-DETR_Few-Shot_DEtection_TRansformer_with_Prompting_and_without_Re-Training_ICCV_2023_paper.pdf">FS-DETR: Few-Shot DEtection TRansformer with prompting and without re-training</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">18</td>
<td style="text-align: center;">2023-08-05</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Wu_Sketch_and_Text_Guided_Diffusion_Model_for_Colored_Point_Cloud_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Wu_Sketch_and_Text_Guided_Diffusion_Model_for_Colored_Point_Cloud_ICCV_2023_paper.pdf">Sketch and Text Guided Diffusion Model for Colored Point<br />Cloud Generation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">18</td>
<td style="text-align: center;">2023-06-21</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Wu_HSR-Diff_Hyperspectral_Image_Super-Resolution_via_Conditional_Diffusion_Models_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Wu_HSR-Diff_Hyperspectral_Image_Super-Resolution_via_Conditional_Diffusion_Models_ICCV_2023_paper.pdf">HSR-Diff: Hyperspectral Image Super-Resolution via Conditional Diffusion Models</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">18</td>
<td style="text-align: center;">2023-08-09</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Liu_Birds-Eye-View_Scene_Graph_for_Vision-Language_Navigation_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Liu_Birds-Eye-View_Scene_Graph_for_Vision-Language_Navigation_ICCV_2023_paper.pdf">Birds-Eye-View Scene Graph for Vision-Language Navigation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">18</td>
<td style="text-align: center;">2022-11-23</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Li_ClimateNeRF_Extreme_Weather_Synthesis_in_Neural_Radiance_Field_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Li_ClimateNeRF_Extreme_Weather_Synthesis_in_Neural_Radiance_Field_ICCV_2023_paper.pdf">ClimateNeRF: Extreme Weather Synthesis in Neural Radiance Field</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">18</td>
<td style="text-align: center;">2023-06-09</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Ma_DetZero_Rethinking_Offboard_3D_Object_Detection_with_Long-term_Sequential_Point_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Ma_DetZero_Rethinking_Offboard_3D_Object_Detection_with_Long-term_Sequential_Point_ICCV_2023_paper.pdf">DetZero: Rethinking Offboard 3D Object Detection with Long-term Sequential<br />Point Clouds</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">18</td>
<td style="text-align: center;">2023-03-16</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Li_Among_Us_Adversarially_Robust_Collaborative_Perception_by_Consensus_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Li_Among_Us_Adversarially_Robust_Collaborative_Perception_by_Consensus_ICCV_2023_paper.pdf">Among Us: Adversarially Robust Collaborative Perception by Consensus</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">18</td>
<td style="text-align: center;">2023-03-22</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/PNVR_LD-ZNet_A_Latent_Diffusion_Approach_for_Text-Based_Image_Segmentation_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/PNVR_LD-ZNet_A_Latent_Diffusion_Approach_for_Text-Based_Image_Segmentation_ICCV_2023_paper.pdf">LD-ZNet: A Latent Diffusion Approach for Text-Based Image Segmentation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">18</td>
<td style="text-align: center;">2023-07-24</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Ying_CTVIS_Consistent_Training_for_Online_Video_Instance_Segmentation_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Ying_CTVIS_Consistent_Training_for_Online_Video_Instance_Segmentation_ICCV_2023_paper.pdf">CTVIS: Consistent Training for Online Video Instance Segmentation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">18</td>
<td style="text-align: center;">2023-08-28</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Schmied_R3D3_Dense_3D_Reconstruction_of_Dynamic_Scenes_from_Multiple_Cameras_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Schmied_R3D3_Dense_3D_Reconstruction_of_Dynamic_Scenes_from_Multiple_Cameras_ICCV_2023_paper.pdf">R3D3: Dense 3D Reconstruction of Dynamic Scenes from Multiple<br />Cameras</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">18</td>
<td style="text-align: center;">2023-06-15</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Wang_Evaluating_Data_Attribution_for_Text-to-Image_Models_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Wang_Evaluating_Data_Attribution_for_Text-to-Image_Models_ICCV_2023_paper.pdf">Evaluating Data Attribution for Text-to-Image Models</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">18</td>
<td style="text-align: center;">2022-12-09</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Neuhaus_Spurious_Features_Everywhere_-_Large-Scale_Detection_of_Harmful_Spurious_Features_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Neuhaus_Spurious_Features_Everywhere_-_Large-Scale_Detection_of_Harmful_Spurious_Features_ICCV_2023_paper.pdf">Spurious Features Everywhere - Large-Scale Detection of Harmful Spurious<br />Features in ImageNet</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">18</td>
<td style="text-align: center;">2023-06-30</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Seo_FlipNeRF_Flipped_Reflection_Rays_for_Few-shot_Novel_View_Synthesis_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Seo_FlipNeRF_Flipped_Reflection_Rays_for_Few-shot_Novel_View_Synthesis_ICCV_2023_paper.pdf">FlipNeRF: Flipped Reflection Rays for Few-shot Novel View Synthesis</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">18</td>
<td style="text-align: center;">2023-03-23</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Yu_Visually-Prompted_Language_Model_for_Fine-Grained_Scene_Graph_Generation_in_an_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Yu_Visually-Prompted_Language_Model_for_Fine-Grained_Scene_Graph_Generation_in_an_ICCV_2023_paper.pdf">Visually-Prompted Language Model for Fine-Grained Scene Graph Generation in<br />an Open World</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">18</td>
<td style="text-align: center;">2023-07-19</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Hong_Implicit_Identity_Representation_Conditioned_Memory_Compensation_Network_for_Talking_Head_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Hong_Implicit_Identity_Representation_Conditioned_Memory_Compensation_Network_for_Talking_Head_ICCV_2023_paper.pdf">Implicit Identity Representation Conditioned Memory Compensation Network for Talking<br />Head Video Generation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">18</td>
<td style="text-align: center;">2023-04-04</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Su_NPC_Neural_Point_Characters_from_Video_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Su_NPC_Neural_Point_Characters_from_Video_ICCV_2023_paper.pdf">NPC: Neural Point Characters from Video</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">18</td>
<td style="text-align: center;">2023-04-03</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Zhang_NeMF_Inverse_Volume_Rendering_with_Neural_Microflake_Field_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Zhang_NeMF_Inverse_Volume_Rendering_with_Neural_Microflake_Field_ICCV_2023_paper.pdf">NeMF: Inverse Volume Rendering with Neural Microflake Field</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">18</td>
<td style="text-align: center;">2023-02-17</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Chen_Towards_Unifying_Medical_Vision-and-Language_Pre-Training_via_Soft_Prompts_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Chen_Towards_Unifying_Medical_Vision-and-Language_Pre-Training_via_Soft_Prompts_ICCV_2023_paper.pdf">Towards Unifying Medical Vision-and-Language Pre-training via Soft Prompts</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">18</td>
<td style="text-align: center;">2023-03-10</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Wang_Overwriting_Pretrained_Bias_with_Finetuning_Data_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Wang_Overwriting_Pretrained_Bias_with_Finetuning_Data_ICCV_2023_paper.pdf">Overwriting Pretrained Bias with Finetuning Data</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">18</td>
<td style="text-align: center;">2023-09-08</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Jeong_The_Power_of_Sound_TPoS_Audio_Reactive_Video_Generation_with_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Jeong_The_Power_of_Sound_TPoS_Audio_Reactive_Video_Generation_with_ICCV_2023_paper.pdf">The Power of Sound (TPoS): Audio Reactive Video Generation<br />with Stable Diffusion</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">18</td>
<td style="text-align: center;">2023-03-09</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Cheng_MixSpeech_Cross-Modality_Self-Learning_with_Audio-Visual_Stream_Mixup_for_Visual_Speech_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Cheng_MixSpeech_Cross-Modality_Self-Learning_with_Audio-Visual_Stream_Mixup_for_Visual_Speech_ICCV_2023_paper.pdf">MixSpeech: Cross-Modality Self-Learning with Audio-Visual Stream Mixup for Visual<br />Speech Translation and Recognition</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">17</td>
<td style="text-align: center;">2023-04-04</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Ouali_Black_Box_Few-Shot_Adaptation_for_Vision-Language_Models_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Ouali_Black_Box_Few-Shot_Adaptation_for_Vision-Language_Models_ICCV_2023_paper.pdf">Black Box Few-Shot Adaptation for Vision-Language models</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">17</td>
<td style="text-align: center;">2023-01-17</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Lu_A_Large-Scale_Outdoor_Multi-Modal_Dataset_and_Benchmark_for_Novel_View_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Lu_A_Large-Scale_Outdoor_Multi-Modal_Dataset_and_Benchmark_for_Novel_View_ICCV_2023_paper.pdf">A Large-Scale Outdoor Multi-modal Dataset and Benchmark for Novel<br />View Synthesis and Implicit Scene Reconstruction</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">17</td>
<td style="text-align: center;">2023-08-14</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Kang_Exploring_Lightweight_Hierarchical_Vision_Transformers_for_Efficient_Visual_Tracking_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Kang_Exploring_Lightweight_Hierarchical_Vision_Transformers_for_Efficient_Visual_Tracking_ICCV_2023_paper.pdf">Exploring Lightweight Hierarchical Vision Transformers for Efficient Visual Tracking</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">17</td>
<td style="text-align: center;">2023-08-14</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Xie_S3IM_Stochastic_Structural_SIMilarity_and_Its_Unreasonable_Effectiveness_for_Neural_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Xie_S3IM_Stochastic_Structural_SIMilarity_and_Its_Unreasonable_Effectiveness_for_Neural_ICCV_2023_paper.pdf">S3IM: Stochastic Structural SIMilarity and Its Unreasonable Effectiveness for<br />Neural Fields</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">17</td>
<td style="text-align: center;">2023-08-15</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Wang_Memory-and-Anticipation_Transformer_for_Online_Action_Understanding_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Wang_Memory-and-Anticipation_Transformer_for_Online_Action_Understanding_ICCV_2023_paper.pdf">Memory-and-Anticipation Transformer for Online Action Understanding</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">17</td>
<td style="text-align: center;">2023-08-01</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Liu_Improving_Pixel-based_MIM_by_Reducing_Wasted_Modeling_Capability_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Liu_Improving_Pixel-based_MIM_by_Reducing_Wasted_Modeling_Capability_ICCV_2023_paper.pdf">Improving Pixel-based MIM by Reducing Wasted Modeling Capability</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">17</td>
<td style="text-align: center;">2023-08-23</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Song_Blending-NeRF_Text-Driven_Localized_Editing_in_Neural_Radiance_Fields_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Song_Blending-NeRF_Text-Driven_Localized_Editing_in_Neural_Radiance_Fields_ICCV_2023_paper.pdf">Blending-NeRF: Text-Driven Localized Editing in Neural Radiance Fields</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">17</td>
<td style="text-align: center;">2023-08-21</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Han_STEERER_Resolving_Scale_Variations_for_Counting_and_Localization_via_Selective_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Han_STEERER_Resolving_Scale_Variations_for_Counting_and_Localization_via_Selective_ICCV_2023_paper.pdf">STEERER: Resolving Scale Variations for Counting and Localization via<br />Selective Inheritance Learning</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">17</td>
<td style="text-align: center;">2023-08-29</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Liu_Learning_to_Upsample_by_Learning_to_Sample_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Liu_Learning_to_Upsample_by_Learning_to_Sample_ICCV_2023_paper.pdf">Learning to Upsample by Learning to Sample</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">17</td>
<td style="text-align: center;">2023-03-21</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Deuser_Sample4Geo_Hard_Negative_Sampling_For_Cross-View_Geo-Localisation_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Deuser_Sample4Geo_Hard_Negative_Sampling_For_Cross-View_Geo-Localisation_ICCV_2023_paper.pdf">Sample4Geo: Hard Negative Sampling For Cross-View Geo-Localisation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">17</td>
<td style="text-align: center;">2023-08-31</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Kaufmann_EMDB_The_Electromagnetic_Database_of_Global_3D_Human_Pose_and_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Kaufmann_EMDB_The_Electromagnetic_Database_of_Global_3D_Human_Pose_and_ICCV_2023_paper.pdf">EMDB: The Electromagnetic Database of Global 3D Human Pose<br />and Shape in the Wild</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">17</td>
<td style="text-align: center;">2023-04-18</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Li_Fast_Neural_Scene_Flow_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Li_Fast_Neural_Scene_Flow_ICCV_2023_paper.pdf">Fast Neural Scene Flow</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">17</td>
<td style="text-align: center;">2023-08-21</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Tang_When_Prompt-based_Incremental_Learning_Does_Not_Meet_Strong_Pretraining_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Tang_When_Prompt-based_Incremental_Learning_Does_Not_Meet_Strong_Pretraining_ICCV_2023_paper.pdf">When Prompt-based Incremental Learning Does Not Meet Strong Pretraining</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">17</td>
<td style="text-align: center;">2023-08-27</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Li_High-Resolution_Document_Shadow_Removal_via_A_Large-Scale_Real-World_Dataset_and_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Li_High-Resolution_Document_Shadow_Removal_via_A_Large-Scale_Real-World_Dataset_and_ICCV_2023_paper.pdf">High-Resolution Document Shadow Removal via A Large-Scale Real-World Dataset<br />and A Frequency-Aware Shadow Erasing Net</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">17</td>
<td style="text-align: center;">2023-03-08</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Moreau_CROSSFIRE_Camera_Relocalization_On_Self-Supervised_Features_from_an_Implicit_Representation_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Moreau_CROSSFIRE_Camera_Relocalization_On_Self-Supervised_Features_from_an_Implicit_Representation_ICCV_2023_paper.pdf">CROSSFIRE: Camera Relocalization On Self-Supervised Features from an Implicit<br />Representation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">17</td>
<td style="text-align: center;">2023-09-24</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Li_LogicSeg_Parsing_Visual_Semantics_with_Neural_Logic_Learning_and_Reasoning_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Li_LogicSeg_Parsing_Visual_Semantics_with_Neural_Logic_Learning_and_Reasoning_ICCV_2023_paper.pdf">LogicSeg: Parsing Visual Semantics with Neural Logic Learning and<br />Reasoning</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">17</td>
<td style="text-align: center;">2023-01-05</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Yang_Event_Camera_Data_Pre-training_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Yang_Event_Camera_Data_Pre-training_ICCV_2023_paper.pdf">Event Camera Data Pre-training</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">17</td>
<td style="text-align: center;">2023-04-24</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Liu_HOSNeRF_Dynamic_Human-Object-Scene_Neural_Radiance_Fields_from_a_Single_Video_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Liu_HOSNeRF_Dynamic_Human-Object-Scene_Neural_Radiance_Fields_from_a_Single_Video_ICCV_2023_paper.pdf">HOSNeRF: Dynamic Human-Object-Scene Neural Radiance Fields from a Single<br />Video</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">17</td>
<td style="text-align: center;">2023-03-15</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Zhao_Spherical_Space_Feature_Decomposition_for_Guided_Depth_Map_Super-Resolution_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Zhao_Spherical_Space_Feature_Decomposition_for_Guided_Depth_Map_Super-Resolution_ICCV_2023_paper.pdf">Spherical Space Feature Decomposition for Guided Depth Map Super-Resolution</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">17</td>
<td style="text-align: center;">2023-03-09</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Fu_GPGait_Generalized_Pose-based_Gait_Recognition_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Fu_GPGait_Generalized_Pose-based_Gait_Recognition_ICCV_2023_paper.pdf">GPGait: Generalized Pose-based Gait Recognition</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">17</td>
<td style="text-align: center;">2023-08-26</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Hu_Beyond_One-to-One_Rethinking_the_Referring_Image_Segmentation_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Hu_Beyond_One-to-One_Rethinking_the_Referring_Image_Segmentation_ICCV_2023_paper.pdf">Beyond One-to-One: Rethinking the Referring Image Segmentation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">17</td>
<td style="text-align: center;">2023-08-26</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Liu_Point-Query_Quadtree_for_Crowd_Counting_Localization_and_More_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Liu_Point-Query_Quadtree_for_Crowd_Counting_Localization_and_More_ICCV_2023_paper.pdf">Point-Query Quadtree for Crowd Counting, Localization, and More</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">17</td>
<td style="text-align: center;">2023-05-19</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Kim_Chupa_Carving_3D_Clothed_Humans_from_Skinned_Shape_Priors_using_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Kim_Chupa_Carving_3D_Clothed_Humans_from_Skinned_Shape_Priors_using_ICCV_2023_paper.pdf">Chupa: Carving 3D Clothed Humans from Skinned Shape Priors<br />using 2D Diffusion Probabilistic Models</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">17</td>
<td style="text-align: center;">2023-08-27</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Chen_Sparse_Sampling_Transformer_with_Uncertainty-Driven_Ranking_for_Unified_Removal_of_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Chen_Sparse_Sampling_Transformer_with_Uncertainty-Driven_Ranking_for_Unified_Removal_of_ICCV_2023_paper.pdf">Sparse Sampling Transformer with Uncertainty-Driven Ranking for Unified Removal<br />of Raindrops and Rain Streaks</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">17</td>
<td style="text-align: center;">2023-08-15</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Wu_ObjectSDF_Improved_Object-Compositional_Neural_Implicit_Surfaces_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Wu_ObjectSDF_Improved_Object-Compositional_Neural_Implicit_Surfaces_ICCV_2023_paper.pdf">ObjectSDF++: Improved Object-Compositional Neural Implicit Surfaces</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">17</td>
<td style="text-align: center;">2022-10-03</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Wang_Masked_Spiking_Transformer_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Wang_Masked_Spiking_Transformer_ICCV_2023_paper.pdf">Masked Spiking Transformer</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">16</td>
<td style="text-align: center;">2023-07-29</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Xia_CMDA_Cross-Modality_Domain_Adaptation_for_Nighttime_Semantic_Segmentation_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Xia_CMDA_Cross-Modality_Domain_Adaptation_for_Nighttime_Semantic_Segmentation_ICCV_2023_paper.pdf">CMDA: Cross-Modality Domain Adaptation for Nighttime Semantic Segmentation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">16</td>
<td style="text-align: center;">2023-07-26</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Xu_Human-centric_Scene_Understanding_for_3D_Large-scale_Scenarios_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Xu_Human-centric_Scene_Understanding_for_3D_Large-scale_Scenarios_ICCV_2023_paper.pdf">Human-centric Scene Understanding for 3D Large-scale Scenarios</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">16</td>
<td style="text-align: center;">2023-09-26</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Yang_Generating_Visual_Scenes_from_Touch_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Yang_Generating_Visual_Scenes_from_Touch_ICCV_2023_paper.pdf">Generating Visual Scenes from Touch</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">16</td>
<td style="text-align: center;">2023-02-02</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Xiong_Get3DHuman_Lifting_StyleGAN-Human_into_a_3D_Generative_Model_Using_Pixel-Aligned_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Xiong_Get3DHuman_Lifting_StyleGAN-Human_into_a_3D_Generative_Model_Using_Pixel-Aligned_ICCV_2023_paper.pdf">Get3DHuman: Lifting StyleGAN-Human into a 3D Generative Model using<br />Pixel-aligned Reconstruction Priors</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">16</td>
<td style="text-align: center;">2023-08-20</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Huang_ESTextSpotter_Towards_Better_Scene_Text_Spotting_with_Explicit_Synergy_in_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Huang_ESTextSpotter_Towards_Better_Scene_Text_Spotting_with_Explicit_Synergy_in_ICCV_2023_paper.pdf">ESTextSpotter: Towards Better Scene Text Spotting with Explicit Synergy<br />in Transformer</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">16</td>
<td style="text-align: center;">2023-04-05</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Prokudin_Dynamic_Point_Fields_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Prokudin_Dynamic_Point_Fields_ICCV_2023_paper.pdf">Dynamic Point Fields</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">16</td>
<td style="text-align: center;">2022-05-19</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Liu_Integrally_Migrating_Pre-trained_Transformer_Encoder-decoders_for_Visual_Object_Detection_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Liu_Integrally_Migrating_Pre-trained_Transformer_Encoder-decoders_for_Visual_Object_Detection_ICCV_2023_paper.pdf">Integrally Migrating Pre-trained Transformer Encoder-decoders for Visual Object Detection</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">16</td>
<td style="text-align: center;">2022-12-08</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Lin_Graph_Matching_with_Bi-level_Noisy_Correspondence_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Lin_Graph_Matching_with_Bi-level_Noisy_Correspondence_ICCV_2023_paper.pdf">Graph Matching with Bi-level Noisy Correspondence</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">16</td>
<td style="text-align: center;">2023-09-25</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Najibi_Unsupervised_3D_Perception_with_2D_Vision-Language_Distillation_for_Autonomous_Driving_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Najibi_Unsupervised_3D_Perception_with_2D_Vision-Language_Distillation_for_Autonomous_Driving_ICCV_2023_paper.pdf">Unsupervised 3D Perception with 2D Vision-Language Distillation for Autonomous<br />Driving</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">16</td>
<td style="text-align: center;">2023-04-05</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Cheng_ChartReader_A_Unified_Framework_for_Chart_Derendering_and_Comprehension_without_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Cheng_ChartReader_A_Unified_Framework_for_Chart_Derendering_and_Comprehension_without_ICCV_2023_paper.pdf">ChartReader: A Unified Framework for Chart Derendering and Comprehension<br />without Heuristic Rules</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">16</td>
<td style="text-align: center;">2023-08-24</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Liang_Logic-induced_Diagnostic_Reasoning_for_Semi-supervised_Semantic_Segmentation_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Liang_Logic-induced_Diagnostic_Reasoning_for_Semi-supervised_Semantic_Segmentation_ICCV_2023_paper.pdf">Logic-induced Diagnostic Reasoning for Semi-supervised Semantic Segmentation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">16</td>
<td style="text-align: center;">2023-01-06</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Wang_Object_as_Query_Lifting_Any_2D_Object_Detector_to_3D_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Wang_Object_as_Query_Lifting_Any_2D_Object_Detector_to_3D_ICCV_2023_paper.pdf">Object as Query: Lifting any 2D Object Detector to<br />3D Detection</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">16</td>
<td style="text-align: center;">2023-08-01</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Wei_Online_Prototype_Learning_for_Online_Continual_Learning_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Wei_Online_Prototype_Learning_for_Online_Continual_Learning_ICCV_2023_paper.pdf">Online Prototype Learning for Online Continual Learning</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">16</td>
<td style="text-align: center;">2023-03-16</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Tanveer_DS-Fusion_Artistic_Typography_via_Discriminated_and_Stylized_Diffusion_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Tanveer_DS-Fusion_Artistic_Typography_via_Discriminated_and_Stylized_Diffusion_ICCV_2023_paper.pdf">DS-Fusion: Artistic Typography via Discriminated and Stylized Diffusion</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">16</td>
<td style="text-align: center;">2023-07-15</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Wang_ExposureDiffusion_Learning_to_Expose_for_Low-light_Image_Enhancement_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Wang_ExposureDiffusion_Learning_to_Expose_for_Low-light_Image_Enhancement_ICCV_2023_paper.pdf">ExposureDiffusion: Learning to Expose for Low-light Image Enhancement</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">16</td>
<td style="text-align: center;">2022-12-07</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Sanchez_Domain_Generalization_of_3D_Semantic_Segmentation_in_Autonomous_Driving_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Sanchez_Domain_Generalization_of_3D_Semantic_Segmentation_in_Autonomous_Driving_ICCV_2023_paper.pdf">Domain generalization of 3D semantic segmentation in autonomous driving</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">16</td>
<td style="text-align: center;">2023-08-20</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Qiao_March_in_Chat_Interactive_Prompting_for_Remote_Embodied_Referring_Expression_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Qiao_March_in_Chat_Interactive_Prompting_for_Remote_Embodied_Referring_Expression_ICCV_2023_paper.pdf">March in Chat: Interactive Prompting for Remote Embodied Referring<br />Expression</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">16</td>
<td style="text-align: center;">2023-09-02</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Kim_Contrastive_Feature_Masking_Open-Vocabulary_Vision_Transformer_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Kim_Contrastive_Feature_Masking_Open-Vocabulary_Vision_Transformer_ICCV_2023_paper.pdf">Contrastive Feature Masking Open-Vocabulary Vision Transformer</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">16</td>
<td style="text-align: center;">2023-04-17</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Patel_Pretrained_Language_Models_as_Visual_Planners_for_Human_Assistance_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Patel_Pretrained_Language_Models_as_Visual_Planners_for_Human_Assistance_ICCV_2023_paper.pdf">Pretrained Language Models as Visual Planners for Human Assistance</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">16</td>
<td style="text-align: center;">2023-07-14</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Flaborea_Multimodal_Motion_Conditioned_Diffusion_Model_for_Skeleton-based_Video_Anomaly_Detection_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Flaborea_Multimodal_Motion_Conditioned_Diffusion_Model_for_Skeleton-based_Video_Anomaly_Detection_ICCV_2023_paper.pdf">Multimodal Motion Conditioned Diffusion Model for Skeleton-based Video Anomaly<br />Detection</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">16</td>
<td style="text-align: center;">2022-11-17</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Wang_EfficientTrain_Exploring_Generalized_Curriculum_Learning_for_Training_Visual_Backbones_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Wang_EfficientTrain_Exploring_Generalized_Curriculum_Learning_for_Training_Visual_Backbones_ICCV_2023_paper.pdf">EfficientTrain: Exploring Generalized Curriculum Learning for Training Visual Backbones</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">16</td>
<td style="text-align: center;">2023-07-31</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Jie_Revisiting_the_Parameter_Efficiency_of_Adapters_from_the_Perspective_of_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Jie_Revisiting_the_Parameter_Efficiency_of_Adapters_from_the_Perspective_of_ICCV_2023_paper.pdf">Revisiting the Parameter Efficiency of Adapters from the Perspective<br />of Precision Redundancy</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">16</td>
<td style="text-align: center;">2022-05-27</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Jiao_Semi-supervised_Semantics-guided_Adversarial_Training_for_Robust_Trajectory_Prediction_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Jiao_Semi-supervised_Semantics-guided_Adversarial_Training_for_Robust_Trajectory_Prediction_ICCV_2023_paper.pdf">Semi-supervised Semantics-guided Adversarial Training for Robust Trajectory Prediction</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">15</td>
<td style="text-align: center;">2023-08-10</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Zheng_Look_at_the_Neighbor_Distortion-aware_Unsupervised_Domain_Adaptation_for_Panoramic_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Zheng_Look_at_the_Neighbor_Distortion-aware_Unsupervised_Domain_Adaptation_for_Panoramic_ICCV_2023_paper.pdf">Look at the Neighbor: Distortion-aware Unsupervised Domain Adaptation for<br />Panoramic Semantic Segmentation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">15</td>
<td style="text-align: center;">2023-07-20</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Dong_EMQ_Evolving_Training-free_Proxies_for_Automated_Mixed_Precision_Quantization_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Dong_EMQ_Evolving_Training-free_Proxies_for_Automated_Mixed_Precision_Quantization_ICCV_2023_paper.pdf">EMQ: Evolving Training-free Proxies for Automated Mixed Precision Quantization</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">15</td>
<td style="text-align: center;">2023-07-25</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Han_E2VPT_An_Effective_and_Efficient_Approach_for_Visual_Prompt_Tuning_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Han_E2VPT_An_Effective_and_Efficient_Approach_for_Visual_Prompt_Tuning_ICCV_2023_paper.pdf">E2VPT: An Effective and Efficient Approach for Visual Prompt<br />Tuning</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">15</td>
<td style="text-align: center;">2023-08-18</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Cho_Label-Free_Event-based_Object_Recognition_via_Joint_Learning_with_Image_Reconstruction_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Cho_Label-Free_Event-based_Object_Recognition_via_Joint_Learning_with_Image_Reconstruction_ICCV_2023_paper.pdf">Label-Free Event-based Object Recognition via Joint Learning with Image<br />Reconstruction from Events</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">15</td>
<td style="text-align: center;">2023-02-02</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Cao_Multi-Modal_Gated_Mixture_of_Local-to-Global_Experts_for_Dynamic_Image_Fusion_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Cao_Multi-Modal_Gated_Mixture_of_Local-to-Global_Experts_for_Dynamic_Image_Fusion_ICCV_2023_paper.pdf">Multi-modal Gated Mixture of Local-to-Global Experts for Dynamic Image<br />Fusion</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">15</td>
<td style="text-align: center;">2022-08-29</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Wilson_SAFE_Sensitivity-Aware_Features_for_Out-of-Distribution_Object_Detection_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Wilson_SAFE_Sensitivity-Aware_Features_for_Out-of-Distribution_Object_Detection_ICCV_2023_paper.pdf">SAFE: Sensitivity-Aware Features for Out-of-Distribution Object Detection</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">15</td>
<td style="text-align: center;">2023-09-11</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Mo_Class-Incremental_Grouping_Network_for_Continual_Audio-Visual_Learning_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Mo_Class-Incremental_Grouping_Network_for_Continual_Audio-Visual_Learning_ICCV_2023_paper.pdf">Class-Incremental Grouping Network for Continual Audio-Visual Learning</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">15</td>
<td style="text-align: center;">2023-07-29</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Bekuzarov_XMem_Production-level_Video_Segmentation_From_Few_Annotated_Frames_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Bekuzarov_XMem_Production-level_Video_Segmentation_From_Few_Annotated_Frames_ICCV_2023_paper.pdf">XMem++: Production-level Video Segmentation From Few Annotated Frames</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">15</td>
<td style="text-align: center;">2023-08-28</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Liu_Referring_Image_Segmentation_Using_Text_Supervision_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Liu_Referring_Image_Segmentation_Using_Text_Supervision_ICCV_2023_paper.pdf">Referring Image Segmentation Using Text Supervision</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">15</td>
<td style="text-align: center;">2023-07-26</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Huang_Adaptive_Frequency_Filters_As_Efficient_Global_Token_Mixers_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Huang_Adaptive_Frequency_Filters_As_Efficient_Global_Token_Mixers_ICCV_2023_paper.pdf">Adaptive Frequency Filters As Efficient Global Token Mixers</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">15</td>
<td style="text-align: center;">2023-03-21</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Liu_TMA_Temporal_Motion_Aggregation_for_Event-based_Optical_Flow_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Liu_TMA_Temporal_Motion_Aggregation_for_Event-based_Optical_Flow_ICCV_2023_paper.pdf">TMA: Temporal Motion Aggregation for Event-based Optical Flow</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">15</td>
<td style="text-align: center;">2023-07-27</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Yildirim_Diverse_Inpainting_and_Editing_with_GAN_Inversion_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Yildirim_Diverse_Inpainting_and_Editing_with_GAN_Inversion_ICCV_2023_paper.pdf">Diverse Inpainting and Editing with GAN Inversion</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">15</td>
<td style="text-align: center;">2022-07-18</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Qin_UniFusion_Unified_Multi-View_Fusion_Transformer_for_Spatial-Temporal_Representation_in_Birds-Eye-View_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Qin_UniFusion_Unified_Multi-View_Fusion_Transformer_for_Spatial-Temporal_Representation_in_Birds-Eye-View_ICCV_2023_paper.pdf">UniFusion: Unified Multi-view Fusion Transformer for Spatial-Temporal Representation in<br />Birds-Eye-View</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">15</td>
<td style="text-align: center;">2023-09-15</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Low_Robust_e-NeRF_NeRF_from_Sparse__Noisy_Events_under_Non-Uniform_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Low_Robust_e-NeRF_NeRF_from_Sparse__Noisy_Events_under_Non-Uniform_ICCV_2023_paper.pdf">Robust e-NeRF: NeRF from Sparse &amp; Noisy Events under<br />Non-Uniform Motion</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">15</td>
<td style="text-align: center;">2023-03-20</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Guo_Robustifying_Token_Attention_for_Vision_Transformers_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Guo_Robustifying_Token_Attention_for_Vision_Transformers_ICCV_2023_paper.pdf">Robustifying Token Attention for Vision Transformers</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">15</td>
<td style="text-align: center;">2023-07-14</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Wang_RFLA_A_Stealthy_Reflected_Light_Adversarial_Attack_in_the_Physical_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Wang_RFLA_A_Stealthy_Reflected_Light_Adversarial_Attack_in_the_Physical_ICCV_2023_paper.pdf">RFLA: A Stealthy Reflected Light Adversarial Attack in the<br />Physical World</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">15</td>
<td style="text-align: center;">2023-06-15</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Mensink_Encyclopedic_VQA_Visual_Questions_About_Detailed_Properties_of_Fine-Grained_Categories_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Mensink_Encyclopedic_VQA_Visual_Questions_About_Detailed_Properties_of_Fine-Grained_Categories_ICCV_2023_paper.pdf">Encyclopedic VQA: Visual questions about detailed properties of fine-grained<br />categories</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">15</td>
<td style="text-align: center;">2023-03-22</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Wang_UMC_A_Unified_Bandwidth-efficient_and_Multi-resolution_based_Collaborative_Perception_Framework_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Wang_UMC_A_Unified_Bandwidth-efficient_and_Multi-resolution_based_Collaborative_Perception_Framework_ICCV_2023_paper.pdf">UMC: A Unified Bandwidth-efficient and Multi-resolution based Collaborative Perception<br />Framework</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">15</td>
<td style="text-align: center;">2023-06-08</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Cheng_LU-NeRF_Scene_and_Pose_Estimation_by_Synchronizing_Local_Unposed_NeRFs_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Cheng_LU-NeRF_Scene_and_Pose_Estimation_by_Synchronizing_Local_Unposed_NeRFs_ICCV_2023_paper.pdf">LU-NeRF: Scene and Pose Estimation by Synchronizing Local Unposed<br />NeRFs</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">15</td>
<td style="text-align: center;">2022-11-22</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Xia_CASSPR_Cross_Attention_Single_Scan_Place_Recognition_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Xia_CASSPR_Cross_Attention_Single_Scan_Place_Recognition_ICCV_2023_paper.pdf">CASSPR: Cross Attention Single Scan Place Recognition</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">15</td>
<td style="text-align: center;">2023-09-28</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Srivatsan_FLIP_Cross-domain_Face_Anti-spoofing_with_Language_Guidance_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Srivatsan_FLIP_Cross-domain_Face_Anti-spoofing_with_Language_Guidance_ICCV_2023_paper.pdf">FLIP: Cross-domain Face Anti-spoofing with Language Guidance</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">15</td>
<td style="text-align: center;">2023-04-04</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Wang_Towards_Open-Vocabulary_Video_Instance_Segmentation_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Wang_Towards_Open-Vocabulary_Video_Instance_Segmentation_ICCV_2023_paper.pdf">Towards Open-Vocabulary Video Instance Segmentation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">15</td>
<td style="text-align: center;">2023-08-31</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Gustafson_FACET_Fairness_in_Computer_Vision_Evaluation_Benchmark_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Gustafson_FACET_Fairness_in_Computer_Vision_Evaluation_Benchmark_ICCV_2023_paper.pdf">FACET: Fairness in Computer Vision Evaluation Benchmark</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">15</td>
<td style="text-align: center;">2023-09-05</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Zou_RawHDR_High_Dynamic_Range_Image_Reconstruction_from_a_Single_Raw_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Zou_RawHDR_High_Dynamic_Range_Image_Reconstruction_from_a_Single_Raw_ICCV_2023_paper.pdf">RawHDR: High Dynamic Range Image Reconstruction from a Single<br />Raw Image</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">15</td>
<td style="text-align: center;">2023-08-21</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Huang_MGMAE_Motion_Guided_Masking_for_Video_Masked_Autoencoding_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Huang_MGMAE_Motion_Guided_Masking_for_Video_Masked_Autoencoding_ICCV_2023_paper.pdf">MGMAE: Motion Guided Masking for Video Masked Autoencoding</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">15</td>
<td style="text-align: center;">2022-07-22</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Zhao_Divide_and_Conquer_3D_Point_Cloud_Instance_Segmentation_With_Point-Wise_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Zhao_Divide_and_Conquer_3D_Point_Cloud_Instance_Segmentation_With_Point-Wise_ICCV_2023_paper.pdf">Divide and Conquer: 3D Point Cloud Instance Segmentation With<br />Point-Wise Binarization</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">15</td>
<td style="text-align: center;">2023-07-16</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Yang_EmoSet_A_Large-scale_Visual_Emotion_Dataset_with_Rich_Attributes_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Yang_EmoSet_A_Large-scale_Visual_Emotion_Dataset_with_Rich_Attributes_ICCV_2023_paper.pdf">EmoSet: A Large-scale Visual Emotion Dataset with Rich Attributes</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">15</td>
<td style="text-align: center;">2023-10-01</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Liu_ContactGen_Generative_Contact_Modeling_for_Grasp_Generation_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Liu_ContactGen_Generative_Contact_Modeling_for_Grasp_Generation_ICCV_2023_paper.pdf">ContactGen: Generative Contact Modeling for Grasp Generation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">15</td>
<td style="text-align: center;">2023-09-03</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Shi_LoGoPrompt_Synthetic_Text_Images_Can_Be_Good_Visual_Prompts_for_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Shi_LoGoPrompt_Synthetic_Text_Images_Can_Be_Good_Visual_Prompts_for_ICCV_2023_paper.pdf">LoGoPrompt: Synthetic Text Images Can Be Good Visual Prompts<br />for Vision-Language Models</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">14</td>
<td style="text-align: center;">2022-11-14</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/He_BiViT_Extremely_Compressed_Binary_Vision_Transformers_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/He_BiViT_Extremely_Compressed_Binary_Vision_Transformers_ICCV_2023_paper.pdf">BiViT: Extremely Compressed Binary Vision Transformers</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">14</td>
<td style="text-align: center;">2023-04-27</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Mu_ActorsNeRF_Animatable_Few-shot_Human_Rendering_with_Generalizable_NeRFs_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Mu_ActorsNeRF_Animatable_Few-shot_Human_Rendering_with_Generalizable_NeRFs_ICCV_2023_paper.pdf">ActorsNeRF: Animatable Few-shot Human Rendering with Generalizable NeRFs</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">14</td>
<td style="text-align: center;">2023-08-20</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Zhang_DomainAdaptor_A_Novel_Approach_to_Test-time_Adaptation_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Zhang_DomainAdaptor_A_Novel_Approach_to_Test-time_Adaptation_ICCV_2023_paper.pdf">DomainAdaptor: A Novel Approach to Test-time Adaptation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">14</td>
<td style="text-align: center;">2023-08-14</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Mao_Masked_Motion_Predictors_are_Strong_3D_Action_Representation_Learners_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Mao_Masked_Motion_Predictors_are_Strong_3D_Action_Representation_Learners_ICCV_2023_paper.pdf">Masked Motion Predictors are Strong 3D Action Representation Learners</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">14</td>
<td style="text-align: center;">2023-01-11</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Zhu_LinkGAN_Linking_GAN_Latents_to_Pixels_for_Controllable_Image_Synthesis_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Zhu_LinkGAN_Linking_GAN_Latents_to_Pixels_for_Controllable_Image_Synthesis_ICCV_2023_paper.pdf">LinkGAN: Linking GAN Latents to Pixels for Controllable Image<br />Synthesis</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">14</td>
<td style="text-align: center;">2023-03-29</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Chai_Global_Adaptation_Meets_Local_Generalization_Unsupervised_Domain_Adaptation_for_3D_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Chai_Global_Adaptation_Meets_Local_Generalization_Unsupervised_Domain_Adaptation_for_3D_ICCV_2023_paper.pdf">Global Adaptation meets Local Generalization: Unsupervised Domain Adaptation for<br />3D Human Pose Estimation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">14</td>
<td style="text-align: center;">2022-10-11</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Marza_Multi-Object_Navigation_with_Dynamically_Learned_Neural_Implicit_Representations_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Marza_Multi-Object_Navigation_with_Dynamically_Learned_Neural_Implicit_Representations_ICCV_2023_paper.pdf">Multi-Object Navigation with dynamically learned neural implicit representations</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">14</td>
<td style="text-align: center;">2023-08-19</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Zhang_3D-Aware_Neural_Body_Fitting_for_Occlusion_Robust_3D_Human_Pose_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Zhang_3D-Aware_Neural_Body_Fitting_for_Occlusion_Robust_3D_Human_Pose_ICCV_2023_paper.pdf">3D-Aware Neural Body Fitting for Occlusion Robust 3D Human<br />Pose Estimation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">14</td>
<td style="text-align: center;">2022-11-28</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Zhu_H3WB_Human3.6M_3D_WholeBody_Dataset_and_Benchmark_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Zhu_H3WB_Human3.6M_3D_WholeBody_Dataset_and_Benchmark_ICCV_2023_paper.pdf">H3WB: Human3.6M 3D WholeBody Dataset and Benchmark</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">14</td>
<td style="text-align: center;">2023-08-27</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Zuo_Reconstructing_Interacting_Hands_with_Interaction_Prior_from_Monocular_Images_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Zuo_Reconstructing_Interacting_Hands_with_Interaction_Prior_from_Monocular_Images_ICCV_2023_paper.pdf">Reconstructing Interacting Hands with Interaction Prior from Monocular Images</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">14</td>
<td style="text-align: center;">2023-08-13</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Li_Compositional_Feature_Augmentation_for_Unbiased_Scene_Graph_Generation_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Li_Compositional_Feature_Augmentation_for_Unbiased_Scene_Graph_Generation_ICCV_2023_paper.pdf">Compositional Feature Augmentation for Unbiased Scene Graph Generation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">14</td>
<td style="text-align: center;">2023-08-19</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/LI_Calibrating_Uncertainty_for_Semi-Supervised_Crowd_Counting_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/LI_Calibrating_Uncertainty_for_Semi-Supervised_Crowd_Counting_ICCV_2023_paper.pdf">Calibrating Uncertainty for Semi-Supervised Crowd Counting</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">14</td>
<td style="text-align: center;">2023-08-09</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Fang_GIFD_A_Generative_Gradient_Inversion_Method_with_Feature_Domain_Optimization_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Fang_GIFD_A_Generative_Gradient_Inversion_Method_with_Feature_Domain_Optimization_ICCV_2023_paper.pdf">GIFD: A Generative Gradient Inversion Method with Feature Domain<br />Optimization</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">14</td>
<td style="text-align: center;">2023-09-08</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Qiao_Dynamic_Mesh-Aware_Radiance_Fields_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Qiao_Dynamic_Mesh-Aware_Radiance_Fields_ICCV_2023_paper.pdf">Dynamic Mesh-Aware Radiance Fields</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">14</td>
<td style="text-align: center;">2023-04-03</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Han_Open-Vocabulary_Semantic_Segmentation_with_Decoupled_One-Pass_Network_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Han_Open-Vocabulary_Semantic_Segmentation_with_Decoupled_One-Pass_Network_ICCV_2023_paper.pdf">Open-Vocabulary Semantic Segmentation with Decoupled One-Pass Network</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">14</td>
<td style="text-align: center;">2023-03-28</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Chen_CuNeRF_Cube-Based_Neural_Radiance_Field_for_Zero-Shot_Medical_Image_Arbitrary-Scale_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Chen_CuNeRF_Cube-Based_Neural_Radiance_Field_for_Zero-Shot_Medical_Image_Arbitrary-Scale_ICCV_2023_paper.pdf">CuNeRF: Cube-Based Neural Radiance Field for Zero-Shot Medical Image<br />Arbitrary-Scale Super Resolution</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">14</td>
<td style="text-align: center;">2023-08-22</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Wang_ReFit_Recurrent_Fitting_Network_for_3D_Human_Recovery_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Wang_ReFit_Recurrent_Fitting_Network_for_3D_Human_Recovery_ICCV_2023_paper.pdf">ReFit: Recurrent Fitting Network for 3D Human Recovery</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">14</td>
<td style="text-align: center;">2023-07-21</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Wang_CORE_Cooperative_Reconstruction_for_Multi-Agent_Perception_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Wang_CORE_Cooperative_Reconstruction_for_Multi-Agent_Perception_ICCV_2023_paper.pdf">Core: Cooperative Reconstruction for Multi-Agent Perception</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">14</td>
<td style="text-align: center;">2023-09-04</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Lai_Mask-Attention-Free_Transformer_for_3D_Instance_Segmentation_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Lai_Mask-Attention-Free_Transformer_for_3D_Instance_Segmentation_ICCV_2023_paper.pdf">Mask-Attention-Free Transformer for 3D Instance Segmentation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">14</td>
<td style="text-align: center;">2022-11-04</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Struppek_Rickrolling_the_Artist_Injecting_Backdoors_into_Text_Encoders_for_Text-to-Image_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Struppek_Rickrolling_the_Artist_Injecting_Backdoors_into_Text_Encoders_for_Text-to-Image_ICCV_2023_paper.pdf">Rickrolling the Artist: Injecting Backdoors into Text Encoders for<br />Text-to-Image Synthesis</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">14</td>
<td style="text-align: center;">2023-03-24</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Yang_UrbanGIRAFFE_Representing_Urban_Scenes_as_Compositional_Generative_Neural_Feature_Fields_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Yang_UrbanGIRAFFE_Representing_Urban_Scenes_as_Compositional_Generative_Neural_Feature_Fields_ICCV_2023_paper.pdf">UrbanGIRAFFE: Representing Urban Scenes as Compositional Generative Neural Feature<br />Fields</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">14</td>
<td style="text-align: center;">2023-07-20</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Ye_Cascade-DETR_Delving_into_High-Quality_Universal_Object_Detection_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Ye_Cascade-DETR_Delving_into_High-Quality_Universal_Object_Detection_ICCV_2023_paper.pdf">Cascade-DETR: Delving into High-Quality Universal Object Detection</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">14</td>
<td style="text-align: center;">2023-04-12</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Bokhovkin_Mesh2Tex_Generating_Mesh_Textures_from_Image_Queries_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Bokhovkin_Mesh2Tex_Generating_Mesh_Textures_from_Image_Queries_ICCV_2023_paper.pdf">Mesh2Tex: Generating Mesh Textures from Image Queries</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">14</td>
<td style="text-align: center;">2023-07-21</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Zhang_SA-BEV_Generating_Semantic-Aware_Birds-Eye-View_Feature_for_Multi-view_3D_Object_Detection_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Zhang_SA-BEV_Generating_Semantic-Aware_Birds-Eye-View_Feature_for_Multi-view_3D_Object_Detection_ICCV_2023_paper.pdf">SA-BEV: Generating Semantic-Aware Birds-Eye-View Feature for Multi-view 3D Object<br />Detection</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">14</td>
<td style="text-align: center;">2023-08-10</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Zhou_Deep_Fusion_Transformer_Network_with_Weighted_Vector-Wise_Keypoints_Voting_for_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Zhou_Deep_Fusion_Transformer_Network_with_Weighted_Vector-Wise_Keypoints_Voting_for_ICCV_2023_paper.pdf">Deep Fusion Transformer Network with Weighted Vector-Wise Keypoints Voting<br />for Robust 6D Object Pose Estimation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">14</td>
<td style="text-align: center;">2023-08-21</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Li_Pixel_Adaptive_Deep_Unfolding_Transformer_for_Hyperspectral_Image_Reconstruction_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Li_Pixel_Adaptive_Deep_Unfolding_Transformer_for_Hyperspectral_Image_Reconstruction_ICCV_2023_paper.pdf">Pixel Adaptive Deep Unfolding Transformer for Hyperspectral Image Reconstruction</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">14</td>
<td style="text-align: center;">2023-07-18</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Bae_EigenTrajectory_Low-Rank_Descriptors_for_Multi-Modal_Trajectory_Forecasting_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Bae_EigenTrajectory_Low-Rank_Descriptors_for_Multi-Modal_Trajectory_Forecasting_ICCV_2023_paper.pdf">EigenTrajectory: Low-Rank Descriptors for Multi-Modal Trajectory Forecasting</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">14</td>
<td style="text-align: center;">2023-02-01</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Ji_Continual_Segment_Towards_a_Single_Unified_and_Non-forgetting_Continual_Segmentation_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Ji_Continual_Segment_Towards_a_Single_Unified_and_Non-forgetting_Continual_Segmentation_ICCV_2023_paper.pdf">Continual Segment: Towards a Single, Unified and Non-forgetting Continual<br />Segmentation Model of 143 Whole-body Organs in CT Scans</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">14</td>
<td style="text-align: center;">2023-04-21</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Chen_Deep_Multiview_Clustering_by_Contrasting_Cluster_Assignments_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Chen_Deep_Multiview_Clustering_by_Contrasting_Cluster_Assignments_ICCV_2023_paper.pdf">Deep Multiview Clustering by Contrasting Cluster Assignments</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">13</td>
<td style="text-align: center;">2023-09-12</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Le_Quality-Agnostic_Deepfake_Detection_with_Intra-model_Collaborative_Learning_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Le_Quality-Agnostic_Deepfake_Detection_with_Intra-model_Collaborative_Learning_ICCV_2023_paper.pdf">Quality-Agnostic Deepfake Detection with Intra-model Collaborative Learning</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">13</td>
<td style="text-align: center;">2023-09-26</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Wang_DistillBEV_Boosting_Multi-Camera_3D_Object_Detection_with_Cross-Modal_Knowledge_Distillation_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Wang_DistillBEV_Boosting_Multi-Camera_3D_Object_Detection_with_Cross-Modal_Knowledge_Distillation_ICCV_2023_paper.pdf">DistillBEV: Boosting Multi-Camera 3D Object Detection with Cross-Modal Knowledge<br />Distillation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">13</td>
<td style="text-align: center;">2023-03-28</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Zhou_HiLo_Exploiting_High_Low_Frequency_Relations_for_Unbiased_Panoptic_Scene_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Zhou_HiLo_Exploiting_High_Low_Frequency_Relations_for_Unbiased_Panoptic_Scene_ICCV_2023_paper.pdf">HiLo: Exploiting High Low Frequency Relations for Unbiased Panoptic<br />Scene Graph Generation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">13</td>
<td style="text-align: center;">2023-08-14</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Kim_Context-Aware_Planning_and_Environment-Aware_Memory_for_Instruction_Following_Embodied_Agents_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Kim_Context-Aware_Planning_and_Environment-Aware_Memory_for_Instruction_Following_Embodied_Agents_ICCV_2023_paper.pdf">Context-Aware Planning and Environment-Aware Memory for Instruction Following Embodied<br />Agents</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">13</td>
<td style="text-align: center;">2023-08-11</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Zhang_Exploring_Predicate_Visual_Context_in_Detecting_of_Human-Object_Interactions_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Zhang_Exploring_Predicate_Visual_Context_in_Detecting_of_Human-Object_Interactions_ICCV_2023_paper.pdf">Exploring Predicate Visual Context in Detecting of HumanObject Interactions</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">13</td>
<td style="text-align: center;">2023-09-21</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Huang_A_Sentence_Speaks_a_Thousand_Images_Domain_Generalization_through_Distilling_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Huang_A_Sentence_Speaks_a_Thousand_Images_Domain_Generalization_through_Distilling_ICCV_2023_paper.pdf">A Sentence Speaks a Thousand Images: Domain Generalization through<br />Distilling CLIP with Language Guidance</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">13</td>
<td style="text-align: center;">2023-08-28</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Cai_CLNeRF_Continual_Learning_Meets_NeRF_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Cai_CLNeRF_Continual_Learning_Meets_NeRF_ICCV_2023_paper.pdf">CLNeRF: Continual Learning Meets NeRF</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">13</td>
<td style="text-align: center;">2023-10-01</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Tanke_Social_Diffusion_Long-term_Multiple_Human_Motion_Anticipation_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Tanke_Social_Diffusion_Long-term_Multiple_Human_Motion_Anticipation_ICCV_2023_paper.pdf">Social Diffusion: Long-term Multiple Human Motion Anticipation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">13</td>
<td style="text-align: center;">2023-09-27</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Girish_SHACIRA_Scalable_HAsh-grid_Compression_for_Implicit_Neural_Representations_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Girish_SHACIRA_Scalable_HAsh-grid_Compression_for_Implicit_Neural_Representations_ICCV_2023_paper.pdf">SHACIRA: Scalable HAsh-grid Compression for Implicit Neural Representations</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">13</td>
<td style="text-align: center;">2023-02-28</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Meng_Towards_Memory-_and_Time-Efficient_Backpropagation_for_Training_Spiking_Neural_Networks_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Meng_Towards_Memory-_and_Time-Efficient_Backpropagation_for_Training_Spiking_Neural_Networks_ICCV_2023_paper.pdf">Towards Memory- and Time-Efficient Backpropagation for Training Spiking Neural<br />Networks</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">13</td>
<td style="text-align: center;">2023-09-15</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Delmas_PoseFix_Correcting_3D_Human_Poses_with_Natural_Language_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Delmas_PoseFix_Correcting_3D_Human_Poses_with_Natural_Language_ICCV_2023_paper.pdf">PoseFix: Correcting 3D Human Poses with Natural Language</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">13</td>
<td style="text-align: center;">2023-07-20</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Wang_Lighting_up_NeRF_via_Unsupervised_Decomposition_and_Enhancement_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Wang_Lighting_up_NeRF_via_Unsupervised_Decomposition_and_Enhancement_ICCV_2023_paper.pdf">Lighting up NeRF via Unsupervised Decomposition and Enhancement</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">13</td>
<td style="text-align: center;">2023-08-07</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Guo_From_Sky_to_the_Ground_A_Large-scale_Benchmark_and_Simple_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Guo_From_Sky_to_the_Ground_A_Large-scale_Benchmark_and_Simple_ICCV_2023_paper.pdf">From Sky to the Ground: A Large-scale Benchmark and<br />Simple Baseline Towards Real Rain Removal</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">13</td>
<td style="text-align: center;">2023-04-24</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Zhu_Universal_Domain_Adaptation_via_Compressive_Attention_Matching_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Zhu_Universal_Domain_Adaptation_via_Compressive_Attention_Matching_ICCV_2023_paper.pdf">Universal Domain Adaptation via Compressive Attention Matching</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">13</td>
<td style="text-align: center;">2023-06-15</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Dravid_Rosetta_Neurons_Mining_the_Common_Units_in_a_Model_Zoo_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Dravid_Rosetta_Neurons_Mining_the_Common_Units_in_a_Model_Zoo_ICCV_2023_paper.pdf">Rosetta Neurons: Mining the Common Units in a Model<br />Zoo</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">13</td>
<td style="text-align: center;">2023-08-05</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Chen_An_Adaptive_Model_Ensemble_Adversarial_Attack_for_Boosting_Adversarial_Transferability_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Chen_An_Adaptive_Model_Ensemble_Adversarial_Attack_for_Boosting_Adversarial_Transferability_ICCV_2023_paper.pdf">An Adaptive Model Ensemble Adversarial Attack for Boosting Adversarial<br />Transferability</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">13</td>
<td style="text-align: center;">2022-11-21</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Lin_SMAUG_Sparse_Masked_Autoencoder_for_Efficient_Video-Language_Pre-Training_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Lin_SMAUG_Sparse_Masked_Autoencoder_for_Efficient_Video-Language_Pre-Training_ICCV_2023_paper.pdf">SMAUG: Sparse Masked Autoencoder for Efficient Video-Language Pre-training</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">13</td>
<td style="text-align: center;">2023-08-28</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Liu_Multi-Modal_Neural_Radiance_Field_for_Monocular_Dense_SLAM_with_a_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Liu_Multi-Modal_Neural_Radiance_Field_for_Monocular_Dense_SLAM_with_a_ICCV_2023_paper.pdf">Multi-Modal Neural Radiance Field for Monocular Dense SLAM with<br />a Light-Weight ToF Sensor</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">13</td>
<td style="text-align: center;">2023-09-28</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Buhler_Preface_A_Data-driven_Volumetric_Prior_for_Few-shot_Ultra_High-resolution_Face_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Buhler_Preface_A_Data-driven_Volumetric_Prior_for_Few-shot_Ultra_High-resolution_Face_ICCV_2023_paper.pdf">Preface: A Data-driven Volumetric Prior for Few-shot Ultra High-resolution<br />Face Synthesis</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">13</td>
<td style="text-align: center;">2023-09-05</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Zheng_Empowering_Low-Light_Image_Enhancer_through_Customized_Learnable_Priors_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Zheng_Empowering_Low-Light_Image_Enhancer_through_Customized_Learnable_Priors_ICCV_2023_paper.pdf">Empowering Low-Light Image Enhancer through Customized Learnable Priors</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">13</td>
<td style="text-align: center;">2023-09-16</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Lee_ExBluRF_Efficient_Radiance_Fields_for_Extreme_Motion_Blurred_Images_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Lee_ExBluRF_Efficient_Radiance_Fields_for_Extreme_Motion_Blurred_Images_ICCV_2023_paper.pdf">ExBluRF: Efficient Radiance Fields for Extreme Motion Blurred Images</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">13</td>
<td style="text-align: center;">2023-08-22</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Kan_Knowledge-Aware_Prompt_Tuning_for_Generalizable_Vision-Language_Models_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Kan_Knowledge-Aware_Prompt_Tuning_for_Generalizable_Vision-Language_Models_ICCV_2023_paper.pdf">Knowledge-Aware Prompt Tuning for Generalizable Vision-Language Models</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">13</td>
<td style="text-align: center;">2023-03-18</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Yang_Grounding_3D_Object_Affordance_from_2D_Interactions_in_Images_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Yang_Grounding_3D_Object_Affordance_from_2D_Interactions_in_Images_ICCV_2023_paper.pdf">Grounding 3D Object Affordance from 2D Interactions in Images</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">13</td>
<td style="text-align: center;">2023-05-25</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Shao_Action_Sensitivity_Learning_for_Temporal_Action_Localization_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Shao_Action_Sensitivity_Learning_for_Temporal_Action_Localization_ICCV_2023_paper.pdf">Action Sensitivity Learning for Temporal Action Localization</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">13</td>
<td style="text-align: center;">2023-03-20</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Li_Pluralistic_Aging_Diffusion_Autoencoder_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Li_Pluralistic_Aging_Diffusion_Autoencoder_ICCV_2023_paper.pdf">Pluralistic Aging Diffusion Autoencoder</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">13</td>
<td style="text-align: center;">2023-09-15</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Ma_Deformable_Neural_Radiance_Fields_using_RGB_and_Event_Cameras_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Ma_Deformable_Neural_Radiance_Fields_using_RGB_and_Event_Cameras_ICCV_2023_paper.pdf">Deformable Neural Radiance Fields using RGB and Event Cameras</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">13</td>
<td style="text-align: center;">2022-11-17</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Chang_DETRDistill_A_Universal_Knowledge_Distillation_Framework_for_DETR-families_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Chang_DETRDistill_A_Universal_Knowledge_Distillation_Framework_for_DETR-families_ICCV_2023_paper.pdf">DETRDistill: A Universal Knowledge Distillation Framework for DETR-families</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">13</td>
<td style="text-align: center;">2023-07-19</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Wang_What_do_neural_networks_learn_in_image_classification_A_frequency_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Wang_What_do_neural_networks_learn_in_image_classification_A_frequency_ICCV_2023_paper.pdf">What do neural networks learn in image classification? A<br />frequency shortcut perspective</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">13</td>
<td style="text-align: center;">2023-08-11</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Liu_Enhancing_Generalization_of_Universal_Adversarial_Perturbation_through_Gradient_Aggregation_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Liu_Enhancing_Generalization_of_Universal_Adversarial_Perturbation_through_Gradient_Aggregation_ICCV_2023_paper.pdf">Enhancing Generalization of Universal Adversarial Perturbation through Gradient Aggregation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">13</td>
<td style="text-align: center;">2022-11-21</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Zhuo_Video_Background_Music_Generation_Dataset_Method_and_Evaluation_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Zhuo_Video_Background_Music_Generation_Dataset_Method_and_Evaluation_ICCV_2023_paper.pdf">Video Background Music Generation: Dataset, Method and Evaluation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">13</td>
<td style="text-align: center;">2023-08-13</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Guo_RMP-Loss_Regularizing_Membrane_Potential_Distribution_for_Spiking_Neural_Networks_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Guo_RMP-Loss_Regularizing_Membrane_Potential_Distribution_for_Spiking_Neural_Networks_ICCV_2023_paper.pdf">RMP-Loss: Regularizing Membrane Potential Distribution for Spiking Neural Networks</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">13</td>
<td style="text-align: center;">2023-08-14</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Wang_DREAMWALKER_Mental_Planning_for_Continuous_Vision-Language_Navigation_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Wang_DREAMWALKER_Mental_Planning_for_Continuous_Vision-Language_Navigation_ICCV_2023_paper.pdf">Dreamwalker: Mental Planning for Continuous Vision-Language Navigation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">13</td>
<td style="text-align: center;">2023-08-18</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Yuan_RLIPv2_Fast_Scaling_of_Relational_Language-Image_Pre-Training_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Yuan_RLIPv2_Fast_Scaling_of_Relational_Language-Image_Pre-Training_ICCV_2023_paper.pdf">RLIPv2: Fast Scaling of Relational Language-Image Pre-training</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">12</td>
<td style="text-align: center;">2023-09-18</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Yang_GEDepth_Ground_Embedding_for_Monocular_Depth_Estimation_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Yang_GEDepth_Ground_Embedding_for_Monocular_Depth_Estimation_ICCV_2023_paper.pdf">GEDepth: Ground Embedding for Monocular Depth Estimation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">12</td>
<td style="text-align: center;">2022-05-03</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Roessle_End2End_Multi-View_Feature_Matching_with_Differentiable_Pose_Optimization_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Roessle_End2End_Multi-View_Feature_Matching_with_Differentiable_Pose_Optimization_ICCV_2023_paper.pdf">End2End Multi-View Feature Matching with Differentiable Pose Optimization</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">12</td>
<td style="text-align: center;">2022-06-01</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Jung_CAFA_Class-Aware_Feature_Alignment_for_Test-Time_Adaptation_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Jung_CAFA_Class-Aware_Feature_Alignment_for_Test-Time_Adaptation_ICCV_2023_paper.pdf">CAFA: Class-Aware Feature Alignment for Test-Time Adaptation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">12</td>
<td style="text-align: center;">2023-03-21</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Dawidowicz_LIMITR_Leveraging_Local_Information_for_Medical_Image-Text_Representation_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Dawidowicz_LIMITR_Leveraging_Local_Information_for_Medical_Image-Text_Representation_ICCV_2023_paper.pdf">LIMITR: Leveraging Local Information for Medical Image-Text Representation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">12</td>
<td style="text-align: center;">2023-08-17</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Sun_Spatially_and_Spectrally_Consistent_Deep_Functional_Maps_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Sun_Spatially_and_Spectrally_Consistent_Deep_Functional_Maps_ICCV_2023_paper.pdf">Spatially and Spectrally Consistent Deep Functional Maps</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">12</td>
<td style="text-align: center;">2022-10-16</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Hyeon-Woo_Scratching_Visual_Transformers_Back_with_Uniform_Attention_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Hyeon-Woo_Scratching_Visual_Transformers_Back_with_Uniform_Attention_ICCV_2023_paper.pdf">Scratching Visual Transformer's Back with Uniform Attention</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">12</td>
<td style="text-align: center;">2023-07-19</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Liu_CPCM_Contextual_Point_Cloud_Modeling_for_Weakly-supervised_Point_Cloud_Semantic_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Liu_CPCM_Contextual_Point_Cloud_Modeling_for_Weakly-supervised_Point_Cloud_Semantic_ICCV_2023_paper.pdf">CPCM: Contextual Point Cloud Modeling for Weakly-supervised Point Cloud<br />Semantic Segmentation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">12</td>
<td style="text-align: center;">2023-07-27</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Wang_Seal-3D_Interactive_Pixel-Level_Editing_for_Neural_Radiance_Fields_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Wang_Seal-3D_Interactive_Pixel-Level_Editing_for_Neural_Radiance_Fields_ICCV_2023_paper.pdf">Seal-3D: Interactive Pixel-Level Editing for Neural Radiance Fields</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">12</td>
<td style="text-align: center;">2023-08-02</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Liu_Improving_Generalization_in_Visual_Reinforcement_Learning_via_Conflict-aware_Gradient_Agreement_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Liu_Improving_Generalization_in_Visual_Reinforcement_Learning_via_Conflict-aware_Gradient_Agreement_ICCV_2023_paper.pdf">Improving Generalization in Visual Reinforcement Learning via Conflict-aware Gradient<br />Agreement Augmentation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">12</td>
<td style="text-align: center;">2022-12-26</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Wei_Generalized_Differentiable_RANSAC_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Wei_Generalized_Differentiable_RANSAC_ICCV_2023_paper.pdf">Generalized Differentiable RANSAC</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">12</td>
<td style="text-align: center;">2023-08-16</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Wang_Low-Light_Image_Enhancement_with_Illumination-Aware_Gamma_Correction_and_Complete_Image_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Wang_Low-Light_Image_Enhancement_with_Illumination-Aware_Gamma_Correction_and_Complete_Image_ICCV_2023_paper.pdf">Low-Light Image Enhancement with Illumination-Aware Gamma Correction and Complete<br />Image Modelling Network</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">12</td>
<td style="text-align: center;">2023-07-31</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Pan_Random_Sub-Samples_Generation_for_Self-Supervised_Real_Image_Denoising_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Pan_Random_Sub-Samples_Generation_for_Self-Supervised_Real_Image_Denoising_ICCV_2023_paper.pdf">Random Sub-Samples Generation for Self-Supervised Real Image Denoising</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">12</td>
<td style="text-align: center;">2022-12-01</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Takmaz_3D_Segmentation_of_Humans_in_Point_Clouds_with_Synthetic_Data_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Takmaz_3D_Segmentation_of_Humans_in_Point_Clouds_with_Synthetic_Data_ICCV_2023_paper.pdf">3D Segmentation of Humans in Point Clouds with Synthetic<br />Data</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">12</td>
<td style="text-align: center;">2023-03-15</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Rojas_Re-ReND_Real-Time_Rendering_of_NeRFs_across_Devices_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Rojas_Re-ReND_Real-Time_Rendering_of_NeRFs_across_Devices_ICCV_2023_paper.pdf">Re-ReND: Real-time Rendering of NeRFs across Devices</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">12</td>
<td style="text-align: center;">2023-08-21</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Wei_Improving_Continuous_Sign_Language_Recognition_with_Cross-Lingual_Signs_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Wei_Improving_Continuous_Sign_Language_Recognition_with_Cross-Lingual_Signs_ICCV_2023_paper.pdf">Improving Continuous Sign Language Recognition with Cross-Lingual Signs</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">12</td>
<td style="text-align: center;">2023-07-20</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Li_AlignDet_Aligning_Pre-training_and_Fine-tuning_in_Object_Detection_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Li_AlignDet_Aligning_Pre-training_and_Fine-tuning_in_Object_Detection_ICCV_2023_paper.pdf">AlignDet: Aligning Pre-training and Fine-tuning in Object Detection</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">12</td>
<td style="text-align: center;">2023-07-16</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Chen_Revisiting_Domain-Adaptive_3D_Object_Detection_by_Reliable_Diverse_and_Class-balanced_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Chen_Revisiting_Domain-Adaptive_3D_Object_Detection_by_Reliable_Diverse_and_Class-balanced_ICCV_2023_paper.pdf">Revisiting Domain-Adaptive 3D Object Detection by Reliable, Diverse and<br />Class-balanced Pseudo-Labeling</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">12</td>
<td style="text-align: center;">2023-08-25</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Li_IOMatch_Simplifying_Open-Set_Semi-Supervised_Learning_with_Joint_Inliers_and_Outliers_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Li_IOMatch_Simplifying_Open-Set_Semi-Supervised_Learning_with_Joint_Inliers_and_Outliers_ICCV_2023_paper.pdf">IOMatch: Simplifying Open-Set Semi-Supervised Learning with Joint Inliers and<br />Outliers Utilization</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">12</td>
<td style="text-align: center;">2023-08-22</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Salehi_Time_Does_Tell_Self-Supervised_Time-Tuning_of_Dense_Image_Representations_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Salehi_Time_Does_Tell_Self-Supervised_Time-Tuning_of_Dense_Image_Representations_ICCV_2023_paper.pdf">Time Does Tell: Self-Supervised Time-Tuning of Dense Image Representations</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">12</td>
<td style="text-align: center;">2023-03-17</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Han_CHAMPAGNE_Learning_Real-world_Conversation_from_Large-Scale_Web_Videos_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Han_CHAMPAGNE_Learning_Real-world_Conversation_from_Large-Scale_Web_Videos_ICCV_2023_paper.pdf">Champagne: Learning Real-world Conversation from Large-Scale Web Videos</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">12</td>
<td style="text-align: center;">2023-08-19</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Tang_SwinLSTM_Improving_Spatiotemporal_Prediction_Accuracy_using_Swin_Transformer_and_LSTM_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Tang_SwinLSTM_Improving_Spatiotemporal_Prediction_Accuracy_using_Swin_Transformer_and_LSTM_ICCV_2023_paper.pdf">SwinLSTM: Improving Spatiotemporal Prediction Accuracy using Swin Transformer and<br />LSTM</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">12</td>
<td style="text-align: center;">2023-08-22</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Cong_Enhancing_NeRF_akin_to_Enhancing_LLMs_Generalizable_NeRF_Transformer_with_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Cong_Enhancing_NeRF_akin_to_Enhancing_LLMs_Generalizable_NeRF_Transformer_with_ICCV_2023_paper.pdf">Enhancing NeRF akin to Enhancing LLMs: Generalizable NeRF Transformer<br />with Mixture-of-View-Experts</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">12</td>
<td style="text-align: center;">2022-08-01</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Dang_Search_for_or_Navigate_to_Dual_Adaptive_Thinking_for_Object_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Dang_Search_for_or_Navigate_to_Dual_Adaptive_Thinking_for_Object_ICCV_2023_paper.pdf">Search for or Navigate to? Dual Adaptive Thinking for<br />Object Navigation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">12</td>
<td style="text-align: center;">2023-09-26</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Tripathi_DECO_Dense_Estimation_of_3D_Human-Scene_Contact_In_The_Wild_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Tripathi_DECO_Dense_Estimation_of_3D_Human-Scene_Contact_In_The_Wild_ICCV_2023_paper.pdf">DECO: Dense Estimation of 3D Human-Scene Contact In The<br />Wild</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">12</td>
<td style="text-align: center;">2023-09-17</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Zhi_LivelySpeaker_Towards_Semantic-Aware_Co-Speech_Gesture_Generation_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Zhi_LivelySpeaker_Towards_Semantic-Aware_Co-Speech_Gesture_Generation_ICCV_2023_paper.pdf">LivelySpeaker: Towards Semantic-Aware Co-Speech Gesture Generation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">12</td>
<td style="text-align: center;">2023-08-11</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Hesse_FunnyBirds_A_Synthetic_Vision_Dataset_for_a_Part-Based_Analysis_of_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Hesse_FunnyBirds_A_Synthetic_Vision_Dataset_for_a_Part-Based_Analysis_of_ICCV_2023_paper.pdf">FunnyBirds: A Synthetic Vision Dataset for a Part-Based Analysis<br />of Explainable AI Methods</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">12</td>
<td style="text-align: center;">2023-09-12</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Yu_Modality_Unifying_Network_for_Visible-Infrared_Person_Re-Identification_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Yu_Modality_Unifying_Network_for_Visible-Infrared_Person_Re-Identification_ICCV_2023_paper.pdf">Modality Unifying Network for Visible-Infrared Person Re-Identification</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">12</td>
<td style="text-align: center;">2023-03-27</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Nag_DiffTAD_Temporal_Action_Detection_with_Proposal_Denoising_Diffusion_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Nag_DiffTAD_Temporal_Action_Detection_with_Proposal_Denoising_Diffusion_ICCV_2023_paper.pdf">DiffTAD: Temporal Action Detection with Proposal Denoising Diffusion</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">12</td>
<td style="text-align: center;">2023-08-12</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Ding_3DMOTFormer_Graph_Transformer_for_Online_3D_Multi-Object_Tracking_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Ding_3DMOTFormer_Graph_Transformer_for_Online_3D_Multi-Object_Tracking_ICCV_2023_paper.pdf">3DMOTFormer: Graph Transformer for Online 3D Multi-Object Tracking</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">12</td>
<td style="text-align: center;">2023-08-18</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Moon_Online_Class_Incremental_Learning_on_Stochastic_Blurry_Task_Boundary_via_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Moon_Online_Class_Incremental_Learning_on_Stochastic_Blurry_Task_Boundary_via_ICCV_2023_paper.pdf">Online Class Incremental Learning on Stochastic Blurry Task Boundary<br />via Mask and Visual Prompt Tuning</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">12</td>
<td style="text-align: center;">2023-08-06</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Yao_Focus_the_Discrepancy_Intra-_and_Inter-Correlation_Learning_for_Image_Anomaly_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Yao_Focus_the_Discrepancy_Intra-_and_Inter-Correlation_Learning_for_Image_Anomaly_ICCV_2023_paper.pdf">Focus the Discrepancy: Intra- and Inter-Correlation Learning for Image<br />Anomaly Detection</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">12</td>
<td style="text-align: center;">2023-04-26</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Zubic_From_Chaos_Comes_Order_Ordering_Event_Representations_for_Object_Recognition_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Zubic_From_Chaos_Comes_Order_Ordering_Event_Representations_for_Object_Recognition_ICCV_2023_paper.pdf">From Chaos Comes Order: Ordering Event Representations for Object<br />Recognition and Detection</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">12</td>
<td style="text-align: center;">2023-04-24</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Song_Total-Recon_Deformable_Scene_Reconstruction_for_Embodied_View_Synthesis_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Song_Total-Recon_Deformable_Scene_Reconstruction_for_Embodied_View_Synthesis_ICCV_2023_paper.pdf">Total-Recon: Deformable Scene Reconstruction for Embodied View Synthesis</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">12</td>
<td style="text-align: center;">2023-07-20</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Lu_See_More_and_Know_More_Zero-shot_Point_Cloud_Segmentation_via_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Lu_See_More_and_Know_More_Zero-shot_Point_Cloud_Segmentation_via_ICCV_2023_paper.pdf">See More and Know More: Zero-shot Point Cloud Segmentation<br />via Multi-modal Visual Data</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">12</td>
<td style="text-align: center;">2023-07-16</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Shi_Boosting_3-DoF_Ground-to-Satellite_Camera_Localization_Accuracy_via_Geometry-Guided_Cross-View_Transformer_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Shi_Boosting_3-DoF_Ground-to-Satellite_Camera_Localization_Accuracy_via_Geometry-Guided_Cross-View_Transformer_ICCV_2023_paper.pdf">Boosting 3-DoF Ground-to-Satellite Camera Localization Accuracy via Geometry-Guided Cross-View<br />Transformer</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">12</td>
<td style="text-align: center;">2023-07-31</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Feng_DiffPose_SpatioTemporal_Diffusion_Model_for_Video-Based_Human_Pose_Estimation_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Feng_DiffPose_SpatioTemporal_Diffusion_Model_for_Video-Based_Human_Pose_Estimation_ICCV_2023_paper.pdf">DiffPose: SpatioTemporal Diffusion Model for Video-Based Human Pose Estimation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">12</td>
<td style="text-align: center;">2023-04-19</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Ponglertnapakorn_DiFaReli_Diffusion_Face_Relighting_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Ponglertnapakorn_DiFaReli_Diffusion_Face_Relighting_ICCV_2023_paper.pdf">DiFaReli: Diffusion Face Relighting</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">12</td>
<td style="text-align: center;">2023-07-19</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Liu_MODA_Mapping-Once_Audio-driven_Portrait_Animation_with_Dual_Attentions_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Liu_MODA_Mapping-Once_Audio-driven_Portrait_Animation_with_Dual_Attentions_ICCV_2023_paper.pdf">MODA: Mapping-Once Audio-driven Portrait Animation with Dual Attentions</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">12</td>
<td style="text-align: center;">2023-08-03</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Li_Point2Mask_Point-supervised_Panoptic_Segmentation_via_Optimal_Transport_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Li_Point2Mask_Point-supervised_Panoptic_Segmentation_via_Optimal_Transport_ICCV_2023_paper.pdf">Point2Mask: Point-supervised Panoptic Segmentation via Optimal Transport</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">12</td>
<td style="text-align: center;">2023-08-31</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Wang_StyleInV_A_Temporal_Style_Modulated_Inversion_Network_for_Unconditional_Video_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Wang_StyleInV_A_Temporal_Style_Modulated_Inversion_Network_for_Unconditional_Video_ICCV_2023_paper.pdf">StyleInV: A Temporal Style Modulated Inversion Network for Unconditional<br />Video Generation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">12</td>
<td style="text-align: center;">2023-03-09</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Xu_MBPTrack_Improving_3D_Point_Cloud_Tracking_with_Memory_Networks_and_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Xu_MBPTrack_Improving_3D_Point_Cloud_Tracking_with_Memory_Networks_and_ICCV_2023_paper.pdf">MBPTrack: Improving 3D Point Cloud Tracking with Memory networks<br />and Box Priors</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">12</td>
<td style="text-align: center;">2023-07-16</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Liu_Tangent_Model_Composition_for_Ensembling_and_Continual_Fine-tuning_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Liu_Tangent_Model_Composition_for_Ensembling_and_Continual_Fine-tuning_ICCV_2023_paper.pdf">Tangent Model Composition for Ensembling and Continual Fine-tuning</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">12</td>
<td style="text-align: center;">2023-08-31</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Yu_Towards_High-Fidelity_Text-Guided_3D_Face_Generation_and_Manipulation_Using_only_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Yu_Towards_High-Fidelity_Text-Guided_3D_Face_Generation_and_Manipulation_Using_only_ICCV_2023_paper.pdf">Towards High-Fidelity Text-Guided 3D Face Generation and Manipulation Using<br />only Images</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">12</td>
<td style="text-align: center;">2023-09-26</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Park_Nearest_Neighbor_Guidance_for_Out-of-Distribution_Detection_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Park_Nearest_Neighbor_Guidance_for_Out-of-Distribution_Detection_ICCV_2023_paper.pdf">Nearest Neighbor Guidance for Out-of-Distribution Detection</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">12</td>
<td style="text-align: center;">2023-07-18</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Tonini_Object-aware_Gaze_Target_Detection_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Tonini_Object-aware_Gaze_Target_Detection_ICCV_2023_paper.pdf">Object-aware Gaze Target Detection</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">11</td>
<td style="text-align: center;">2023-04-12</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Zhang_Probabilistic_Human_Mesh_Recovery_in_3D_Scenes_from_Egocentric_Views_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Zhang_Probabilistic_Human_Mesh_Recovery_in_3D_Scenes_from_Egocentric_Views_ICCV_2023_paper.pdf">Probabilistic Human Mesh Recovery in 3D Scenes from Egocentric<br />Views</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">11</td>
<td style="text-align: center;">2022-12-12</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Hingun_REAP_A_Large-Scale_Realistic_Adversarial_Patch_Benchmark_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Hingun_REAP_A_Large-Scale_Realistic_Adversarial_Patch_Benchmark_ICCV_2023_paper.pdf">REAP: A Large-Scale Realistic Adversarial Patch Benchmark</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">11</td>
<td style="text-align: center;">2023-05-18</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Zhao_MVPSNet_Fast_Generalizable_Multi-view_Photometric_Stereo_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Zhao_MVPSNet_Fast_Generalizable_Multi-view_Photometric_Stereo_ICCV_2023_paper.pdf">MVPSNet: Fast Generalizable Multi-view Photometric Stereo</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">11</td>
<td style="text-align: center;">2023-07-21</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Lee_Latent-OFER_Detect_Mask_and_Reconstruct_with_Latent_Vectors_for_Occluded_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Lee_Latent-OFER_Detect_Mask_and_Reconstruct_with_Latent_Vectors_for_Occluded_ICCV_2023_paper.pdf">Latent-OFER: Detect, Mask, and Reconstruct with Latent Vectors for<br />Occluded Facial Expression Recognition</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">11</td>
<td style="text-align: center;">2023-06-28</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Hou_Subclass-balancing_Contrastive_Learning_for_Long-tailed_Recognition_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Hou_Subclass-balancing_Contrastive_Learning_for_Long-tailed_Recognition_ICCV_2023_paper.pdf">Subclass-balancing Contrastive Learning for Long-tailed Recognition</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">11</td>
<td style="text-align: center;">2023-09-03</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Tang_CoTDet_Affordance_Knowledge_Prompting_for_Task_Driven_Object_Detection_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Tang_CoTDet_Affordance_Knowledge_Prompting_for_Task_Driven_Object_Detection_ICCV_2023_paper.pdf">CoTDet: Affordance Knowledge Prompting for Task Driven Object Detection</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">11</td>
<td style="text-align: center;">2023-08-08</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Ding_Prune_Spatio-temporal_Tokens_by_Semantic-aware_Temporal_Accumulation_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Ding_Prune_Spatio-temporal_Tokens_by_Semantic-aware_Temporal_Accumulation_ICCV_2023_paper.pdf">Prune Spatio-temporal Tokens by Semantic-aware Temporal Accumulation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">11</td>
<td style="text-align: center;">2023-04-24</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Fan_Once_Detected_Never_Lost_Surpassing_Human_Performance_in_Offline_LiDAR_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Fan_Once_Detected_Never_Lost_Surpassing_Human_Performance_in_Offline_LiDAR_ICCV_2023_paper.pdf">Once Detected, Never Lost: Surpassing Human Performance in Offline<br />LiDAR based 3D Object Detection</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">11</td>
<td style="text-align: center;">2023-08-01</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Lou_ELFNet_Evidential_Local-global_Fusion_for_Stereo_Matching_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Lou_ELFNet_Evidential_Local-global_Fusion_for_Stereo_Matching_ICCV_2023_paper.pdf">ELFNet: Evidential Local-global Fusion for Stereo Matching</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">11</td>
<td style="text-align: center;">2023-08-11</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Zhang_Generalizing_Event-Based_Motion_Deblurring_in_Real-World_Scenarios_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Zhang_Generalizing_Event-Based_Motion_Deblurring_in_Real-World_Scenarios_ICCV_2023_paper.pdf">Generalizing Event-Based Motion Deblurring in Real-World Scenarios</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">11</td>
<td style="text-align: center;">2023-03-24</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Cao_Anomaly_Detection_Under_Distribution_Shift_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Cao_Anomaly_Detection_Under_Distribution_Shift_ICCV_2023_paper.pdf">Anomaly Detection under Distribution Shift</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">11</td>
<td style="text-align: center;">2023-03-25</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Wu_LPFF_A_Portrait_Dataset_for_Face_Generators_Across_Large_Poses_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Wu_LPFF_A_Portrait_Dataset_for_Face_Generators_Across_Large_Poses_ICCV_2023_paper.pdf">LPFF: A Portrait Dataset for Face Generators Across Large<br />Poses</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">11</td>
<td style="text-align: center;">2023-03-16</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Chen_Mimic3D_Thriving_3D-Aware_GANs_via_3D-to-2D_Imitation_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Chen_Mimic3D_Thriving_3D-Aware_GANs_via_3D-to-2D_Imitation_ICCV_2023_paper.pdf">Mimic3D: Thriving 3D-Aware GANs via 3D-to-2D Imitation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">11</td>
<td style="text-align: center;">2023-08-14</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Suryanto_ACTIVE_Towards_Highly_Transferable_3D_Physical_Camouflage_for_Universal_and_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Suryanto_ACTIVE_Towards_Highly_Transferable_3D_Physical_Camouflage_for_Universal_and_ICCV_2023_paper.pdf">ACTIVE: Towards Highly Transferable 3D Physical Camouflage for Universal<br />and Robust Vehicle Evasion</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">11</td>
<td style="text-align: center;">2023-03-17</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Li_Unleashing_the_Potential_of_Spiking_Neural_Networks_with_Dynamic_Confidence_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Li_Unleashing_the_Potential_of_Spiking_Neural_Networks_with_Dynamic_Confidence_ICCV_2023_paper.pdf">Unleashing the Potential of Spiking Neural Networks with Dynamic<br />Confidence</a></td>
<td style="text-align: left;"></td>
</tr>
</tbody>
</table>
              
            </div>
          </div><footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="Footer Navigation">
        <a href="../../EMNLP/EMNLP_2023/" class="btn btn-neutral float-left" title="EMNLP 2023"><span class="icon icon-circle-arrow-left"></span> Previous</a>
        <a href="../../ICLR/ICLR_2023/" class="btn btn-neutral float-right" title="ICLR 2023">Next <span class="icon icon-circle-arrow-right"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
          
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="Versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    
    
      <span><a href="../../EMNLP/EMNLP_2023/" style="color: #fcfcfc">&laquo; Previous</a></span>
    
    
      <span><a href="../../ICLR/ICLR_2023/" style="color: #fcfcfc">Next &raquo;</a></span>
    
  </span>
</div>
    <script src="../../js/jquery-3.6.0.min.js"></script>
    <script>var base_url = "../..";</script>
    <script src="../../js/theme_extra.js"></script>
    <script src="../../js/theme.js"></script>
      <script src="../../search/main.js"></script>
    <script>
        jQuery(function () {
            SphinxRtdTheme.Navigation.enable(true);
        });
    </script>

</body>
</html>
