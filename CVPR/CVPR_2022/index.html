<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
      <link rel="shortcut icon" href="../../img/favicon.ico" />
    <title>CVPR 2022 - AI Papers</title>
    <link rel="stylesheet" href="../../css/theme.css" />
    <link rel="stylesheet" href="../../css/theme_extra.css" />
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/github.min.css" />
        <link href="../../mytheme.css" rel="stylesheet" />
    
      <script>
        // Current page data
        var mkdocs_page_name = "CVPR 2022";
        var mkdocs_page_input_path = "CVPR/CVPR_2022.md";
        var mkdocs_page_url = null;
      </script>
    
    <!--[if lt IE 9]>
      <script src="../../js/html5shiv.min.js"></script>
    <![endif]-->
      <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
      <script>hljs.highlightAll();</script> 
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
          <a href="../.." class="icon icon-home"> AI Papers
        </a><div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../../search.html" method="get">
      <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../..">AI Papers</a>
                </li>
              </ul>
              <p class="caption"><span class="caption-text">ACL</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../ACL/ACL_2023/">ACL 2023</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">CVPR</span></p>
              <ul class="current">
                  <li class="toctree-l1 current"><a class="reference internal current" href="./">CVPR 2022</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../CVPR_2023/">CVPR 2023</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">EMNLP</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../EMNLP/EMNLP_2022/">EMNLP 2022</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">ICCV</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../ICCV/ICCV_2023/">ICCV 2023</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">ICLR</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../ICLR/ICLR_2023/">ICLR 2023</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../ICLR/ICLR_2024_Reviewing/">ICLR 2024 Reviewing</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">ICML</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../ICML/ICML_2022/">ICML 2022</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../ICML/ICML_2023/">ICML 2023</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">NeurIPS</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../NeurIPS/NeurIPS_2022/">NeurIPS 2022</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../NeurIPS/NeurIPS_2023/">NeurIPS 2023</a>
                  </li>
              </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      <nav class="wy-nav-top" role="navigation" aria-label="Mobile navigation menu">
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../..">AI Papers</a>
        
      </nav>
      <div class="wy-nav-content">
        <div class="rst-content"><div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../.." class="icon icon-home" aria-label="Docs"></a></li>
          <li class="breadcrumb-item">CVPR</li>
      <li class="breadcrumb-item active">CVPR 2022</li>
    <li class="wy-breadcrumbs-aside">
    </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
            <div class="section" itemprop="articleBody">
              
                <p>Last updated: 2023-12-18 19:16:44. Maintained by <a href="https://wayson.tech/">Weisen Jiang</a>.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">citation</th>
<th style="text-align: center;">date</th>
<th style="text-align: center;">review</th>
<th style="text-align: left;">title (pdf)</th>
<th style="text-align: left;">authors</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">4366</td>
<td style="text-align: center;">2021-12-20</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Rombach_High-Resolution_Image_Synthesis_With_Latent_Diffusion_Models_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Rombach_High-Resolution_Image_Synthesis_With_Latent_Diffusion_Models_CVPR_2022_paper.pdf">High-Resolution Image Synthesis with Latent Diffusion Models</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">3376</td>
<td style="text-align: center;">2021-11-11</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/He_Masked_Autoencoders_Are_Scalable_Vision_Learners_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/He_Masked_Autoencoders_Are_Scalable_Vision_Learners_CVPR_2022_paper.pdf">Masked Autoencoders Are Scalable Vision Learners</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">2087</td>
<td style="text-align: center;">2022-01-10</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Liu_A_ConvNet_for_the_2020s_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Liu_A_ConvNet_for_the_2020s_CVPR_2022_paper.pdf">A ConvNet for the 2020s</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">823</td>
<td style="text-align: center;">2021-12-02</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Cheng_Masked-Attention_Mask_Transformer_for_Universal_Image_Segmentation_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Cheng_Masked-Attention_Mask_Transformer_for_Universal_Image_Segmentation_CVPR_2022_paper.pdf">Masked-attention Mask Transformer for Universal Image Segmentation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">811</td>
<td style="text-align: center;">2021-06-24</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Liu_Video_Swin_Transformer_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Liu_Video_Swin_Transformer_CVPR_2022_paper.pdf">Video Swin Transformer</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">753</td>
<td style="text-align: center;">2021-11-18</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Liu_Swin_Transformer_V2_Scaling_Up_Capacity_and_Resolution_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Liu_Swin_Transformer_V2_Scaling_Up_Capacity_and_Resolution_CVPR_2022_paper.pdf">Swin Transformer V2: Scaling Up Capacity and Resolution</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">721</td>
<td style="text-align: center;">2021-11-18</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Zamir_Restormer_Efficient_Transformer_for_High-Resolution_Image_Restoration_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Zamir_Restormer_Efficient_Transformer_for_High-Resolution_Image_Restoration_CVPR_2022_paper.pdf">Restormer: Efficient Transformer for High-Resolution Image Restoration</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">701</td>
<td style="text-align: center;">2021-12-09</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Fridovich-Keil_Plenoxels_Radiance_Fields_Without_Neural_Networks_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Fridovich-Keil_Plenoxels_Radiance_Fields_Without_Neural_Networks_CVPR_2022_paper.pdf">Plenoxels: Radiance Fields without Neural Networks</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">673</td>
<td style="text-align: center;">2021-11-18</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Xie_SimMIM_A_Simple_Framework_for_Masked_Image_Modeling_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Xie_SimMIM_A_Simple_Framework_for_Masked_Image_Modeling_CVPR_2022_paper.pdf">SimMIM: a Simple Framework for Masked Image Modeling</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">645</td>
<td style="text-align: center;">2021-06-08</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Zhai_Scaling_Vision_Transformers_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Zhai_Scaling_Vision_Transformers_CVPR_2022_paper.pdf">Scaling Vision Transformers</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">604</td>
<td style="text-align: center;">2021-06-06</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Wang_Uformer_A_General_U-Shaped_Transformer_for_Image_Restoration_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Wang_Uformer_A_General_U-Shaped_Transformer_for_Image_Restoration_CVPR_2022_paper.pdf">Uformer: A General U-Shaped Transformer for Image Restoration</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">601</td>
<td style="text-align: center;">2021-12-15</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Chan_Efficient_Geometry-Aware_3D_Generative_Adversarial_Networks_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Chan_Efficient_Geometry-Aware_3D_Generative_Adversarial_Networks_CVPR_2022_paper.pdf">Efficient Geometry-aware 3D Generative Adversarial Networks</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">541</td>
<td style="text-align: center;">2022-01-24</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Lugmayr_RePaint_Inpainting_Using_Denoising_Diffusion_Probabilistic_Models_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Lugmayr_RePaint_Inpainting_Using_Denoising_Diffusion_Probabilistic_Models_CVPR_2022_paper.pdf">RePaint: Inpainting using Denoising Diffusion Probabilistic Models</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">519</td>
<td style="text-align: center;">2021-11-23</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Barron_Mip-NeRF_360_Unbounded_Anti-Aliased_Neural_Radiance_Fields_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Barron_Mip-NeRF_360_Unbounded_Anti-Aliased_Neural_Radiance_Fields_CVPR_2022_paper.pdf">Mip-NeRF 360: Unbounded Anti-Aliased Neural Radiance Fields</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">505</td>
<td style="text-align: center;">2021-07-01</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Dong_CSWin_Transformer_A_General_Vision_Transformer_Backbone_With_Cross-Shaped_Windows_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Dong_CSWin_Transformer_A_General_Vision_Transformer_Backbone_With_Cross-Shaped_Windows_CVPR_2022_paper.pdf">CSWin Transformer: A General Vision Transformer Backbone with Cross-Shaped<br />Windows</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">477</td>
<td style="text-align: center;">2022-03-10</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Zhou_Conditional_Prompt_Learning_for_Vision-Language_Models_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Zhou_Conditional_Prompt_Learning_for_Vision-Language_Models_CVPR_2022_paper.pdf">Conditional Prompt Learning for Vision-Language Models</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">468</td>
<td style="text-align: center;">2021-11-22</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Sun_Direct_Voxel_Grid_Optimization_Super-Fast_Convergence_for_Radiance_Fields_Reconstruction_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Sun_Direct_Voxel_Grid_Optimization_Super-Fast_Convergence_for_Radiance_Fields_Reconstruction_CVPR_2022_paper.pdf">Direct Voxel Grid Optimization: Super-fast Convergence for Radiance Fields<br />Reconstruction</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">428</td>
<td style="text-align: center;">2021-12-07</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Li_Grounded_Language-Image_Pre-Training_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Li_Grounded_Language-Image_Pre-Training_CVPR_2022_paper.pdf">Grounded Language-Image Pre-training</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">418</td>
<td style="text-align: center;">2021-01-07</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Meinhardt_TrackFormer_Multi-Object_Tracking_With_Transformers_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Meinhardt_TrackFormer_Multi-Object_Tracking_With_Transformers_CVPR_2022_paper.pdf">TrackFormer: Multi-Object Tracking with Transformers</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">409</td>
<td style="text-align: center;">2021-07-06</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Deng_Depth-Supervised_NeRF_Fewer_Views_and_Faster_Training_for_Free_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Deng_Depth-Supervised_NeRF_Fewer_Views_and_Faster_Training_for_Free_CVPR_2022_paper.pdf">Depth-supervised NeRF: Fewer Views and Faster Training for Free</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">400</td>
<td style="text-align: center;">2021-12-16</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Wei_Masked_Feature_Prediction_for_Self-Supervised_Visual_Pre-Training_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Wei_Masked_Feature_Prediction_for_Self-Supervised_Visual_Pre-Training_CVPR_2022_paper.pdf">Masked Feature Prediction for Self-Supervised Visual Pre-Training</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">379</td>
<td style="text-align: center;">2021-11-22</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Yu_MetaFormer_Is_Actually_What_You_Need_for_Vision_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Yu_MetaFormer_Is_Actually_What_You_Need_for_Vision_CVPR_2022_paper.pdf">MetaFormer is Actually What You Need for Vision</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">365</td>
<td style="text-align: center;">2021-11-29</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Avrahami_Blended_Diffusion_for_Text-Driven_Editing_of_Natural_Images_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Avrahami_Blended_Diffusion_for_Text-Driven_Editing_of_Natural_Images_CVPR_2022_paper.pdf">Blended Diffusion for Text-driven Editing of Natural Images</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">361</td>
<td style="text-align: center;">2022-03-13</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Ding_Scaling_Up_Your_Kernels_to_31x31_Revisiting_Large_Kernel_Design_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Ding_Scaling_Up_Your_Kernels_to_31x31_Revisiting_Large_Kernel_Design_CVPR_2022_paper.pdf">Scaling Up Your Kernels to 31Ã—31: Revisiting Large Kernel<br />Design in CNNs</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">344</td>
<td style="text-align: center;">2022-02-10</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Tancik_Block-NeRF_Scalable_Large_Scene_Neural_View_Synthesis_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Tancik_Block-NeRF_Scalable_Large_Scene_Neural_View_Synthesis_CVPR_2022_paper.pdf">Block-NeRF: Scalable Large Scene Neural View Synthesis</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">341</td>
<td style="text-align: center;">2021-12-02</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Li_MViTv2_Improved_Multiscale_Vision_Transformers_for_Classification_and_Detection_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Li_MViTv2_Improved_Multiscale_Vision_Transformers_for_Classification_and_Detection_CVPR_2022_paper.pdf">MViTv2: Improved Multiscale Vision Transformers for Classification and Detection</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">336</td>
<td style="text-align: center;">2021-12-08</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Singh_FLAVA_A_Foundational_Language_and_Vision_Alignment_Model_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Singh_FLAVA_A_Foundational_Language_and_Vision_Alignment_Model_CVPR_2022_paper.pdf">FLAVA: A Foundational Language And Vision Alignment Model</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">326</td>
<td style="text-align: center;">2021-09-04</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Wortsman_Robust_Fine-Tuning_of_Zero-Shot_Models_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Wortsman_Robust_Fine-Tuning_of_Zero-Shot_Models_CVPR_2022_paper.pdf">Robust fine-tuning of zero-shot models</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">310</td>
<td style="text-align: center;">2021-07-13</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Guo_CMT_Convolutional_Neural_Networks_Meet_Vision_Transformers_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Guo_CMT_Convolutional_Neural_Networks_Meet_Vision_Transformers_CVPR_2022_paper.pdf">CMT: Convolutional Neural Networks Meet Vision Transformers</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">298</td>
<td style="text-align: center;">2021-12-02</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Jain_Zero-Shot_Text-Guided_Object_Generation_With_Dream_Fields_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Jain_Zero-Shot_Text-Guided_Object_Generation_With_Dream_Fields_CVPR_2022_paper.pdf">Zero-Shot Text-Guided Object Generation with Dream Fields</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">296</td>
<td style="text-align: center;">2021-11-15</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Zhai_LiT_Zero-Shot_Transfer_With_Locked-Image_Text_Tuning_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Zhai_LiT_Zero-Shot_Transfer_With_Locked-Image_Text_Tuning_CVPR_2022_paper.pdf">LiT: Zero-Shot Transfer with Locked-image text Tuning</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">287</td>
<td style="text-align: center;">2021-06-15</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Roth_Towards_Total_Recall_in_Industrial_Anomaly_Detection_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Roth_Towards_Total_Recall_in_Industrial_Anomaly_Detection_CVPR_2022_paper.pdf">Towards Total Recall in Industrial Anomaly Detection</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">283</td>
<td style="text-align: center;">2021-12-07</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Verbin_Ref-NeRF_Structured_View-Dependent_Appearance_for_Neural_Radiance_Fields_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Verbin_Ref-NeRF_Structured_View-Dependent_Appearance_for_Neural_Radiance_Fields_CVPR_2022_paper.pdf">Ref-NeRF: Structured View-Dependent Appearance for Neural Radiance Fields</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">272</td>
<td style="text-align: center;">2021-10-06</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Kim_DiffusionCLIP_Text-Guided_Diffusion_Models_for_Robust_Image_Manipulation_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Kim_DiffusionCLIP_Text-Guided_Diffusion_Models_for_Robust_Image_Manipulation_CVPR_2022_paper.pdf">DiffusionCLIP: Text-Guided Diffusion Models for Robust Image Manipulation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">271</td>
<td style="text-align: center;">2021-11-29</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Yu_Point-BERT_Pre-Training_3D_Point_Cloud_Transformers_With_Masked_Point_Modeling_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Yu_Point-BERT_Pre-Training_3D_Point_Cloud_Transformers_With_Masked_Point_Modeling_CVPR_2022_paper.pdf">Point-BERT: Pre-training 3D Point Cloud Transformers with Masked Point<br />Modeling</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">257</td>
<td style="text-align: center;">2021-12-02</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Rao_DenseCLIP_Language-Guided_Dense_Prediction_With_Context-Aware_Prompting_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Rao_DenseCLIP_Language-Guided_Dense_Prediction_With_Context-Aware_Prompting_CVPR_2022_paper.pdf">DenseCLIP: Language-Guided Dense Prediction with Context-Aware Prompting</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">253</td>
<td style="text-align: center;">2021-12-01</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Niemeyer_RegNeRF_Regularizing_Neural_Radiance_Fields_for_View_Synthesis_From_Sparse_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Niemeyer_RegNeRF_Regularizing_Neural_Radiance_Fields_for_View_Synthesis_From_Sparse_CVPR_2022_paper.pdf">RegNeRF: Regularizing Neural Radiance Fields for View Synthesis from<br />Sparse Inputs</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">250</td>
<td style="text-align: center;">2022-03-22</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Bai_TransFusion_Robust_LiDAR-Camera_Fusion_for_3D_Object_Detection_With_Transformers_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Bai_TransFusion_Robust_LiDAR-Camera_Fusion_for_3D_Object_Detection_With_Transformers_CVPR_2022_paper.pdf">TransFusion: Robust LiDAR-Camera Fusion for 3D Object Detection with<br />Transformers</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">246</td>
<td style="text-align: center;">2021-12-22</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Zhu_NICE-SLAM_Neural_Implicit_Scalable_Encoding_for_SLAM_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Zhu_NICE-SLAM_Neural_Implicit_Scalable_Encoding_for_SLAM_CVPR_2022_paper.pdf">NICE-SLAM: Neural Implicit Scalable Encoding for SLAM</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">238</td>
<td style="text-align: center;">2022-03-02</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Li_DN-DETR_Accelerate_DETR_Training_by_Introducing_Query_DeNoising_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Li_DN-DETR_Accelerate_DETR_Training_by_Introducing_Query_DeNoising_CVPR_2022_paper.pdf">DN-DETR: Accelerate DETR Training by Introducing Query DeNoising</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">234</td>
<td style="text-align: center;">2021-08-12</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Chen_Mobile-Former_Bridging_MobileNet_and_Transformer_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Chen_Mobile-Former_Bridging_MobileNet_and_Transformer_CVPR_2022_paper.pdf">Mobile-Former: Bridging MobileNet and Transformer</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">233</td>
<td style="text-align: center;">2021-12-16</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Zhong_RegionCLIP_Region-Based_Language-Image_Pretraining_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Zhong_RegionCLIP_Region-Based_Language-Image_Pretraining_CVPR_2022_paper.pdf">RegionCLIP: Region-based Language-Image Pretraining</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">228</td>
<td style="text-align: center;">2021-12-21</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Or-El_StyleSDF_High-Resolution_3D-Consistent_Image_and_Geometry_Generation_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Or-El_StyleSDF_High-Resolution_3D-Consistent_Image_and_Geometry_Generation_CVPR_2022_paper.pdf">StyleSDF: High-Resolution 3D-Consistent Image and Geometry Generation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">226</td>
<td style="text-align: center;">2022-02-08</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Chang_MaskGIT_Masked_Generative_Image_Transformer_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Chang_MaskGIT_Masked_Generative_Image_Transformer_CVPR_2022_paper.pdf">MaskGIT: Masked Generative Image Transformer</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">226</td>
<td style="text-align: center;">2021-11-03</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Dou_An_Empirical_Study_of_Training_End-to-End_Vision-and-Language_Transformers_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Dou_An_Empirical_Study_of_Training_End-to-End_Vision-and-Language_Transformers_CVPR_2022_paper.pdf">An Empirical Study of Training End-to-End Vision-and-Language Transformers</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">224</td>
<td style="text-align: center;">2022-02-22</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Xu_GroupViT_Semantic_Segmentation_Emerges_From_Text_Supervision_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Xu_GroupViT_Semantic_Segmentation_Emerges_From_Text_Supervision_CVPR_2022_paper.pdf">GroupViT: Semantic Segmentation Emerges from Text Supervision</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">224</td>
<td style="text-align: center;">2021-11-29</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Tang_Self-Supervised_Pre-Training_of_Swin_Transformers_for_3D_Medical_Image_Analysis_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Tang_Self-Supervised_Pre-Training_of_Swin_Transformers_for_3D_Medical_Image_Analysis_CVPR_2022_paper.pdf">Self-Supervised Pre-Training of Swin Transformers for 3D Medical Image<br />Analysis</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">221</td>
<td style="text-align: center;">2021-04-28</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Duan_Revisiting_Skeleton-Based_Action_Recognition_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Duan_Revisiting_Skeleton-Based_Action_Recognition_CVPR_2022_paper.pdf">Revisiting Skeleton-based Action Recognition</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">219</td>
<td style="text-align: center;">2022-01-21</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Xu_Point-NeRF_Point-Based_Neural_Radiance_Fields_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Xu_Point-NeRF_Point-Based_Neural_Radiance_Fields_CVPR_2022_paper.pdf">Point-NeRF: Point-based Neural Radiance Fields</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">216</td>
<td style="text-align: center;">2021-04-27</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Chan_BasicVSR_Improving_Video_Super-Resolution_With_Enhanced_Propagation_and_Alignment_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Chan_BasicVSR_Improving_Video_Super-Resolution_With_Enhanced_Propagation_and_Alignment_CVPR_2022_paper.pdf">BasicVSR++: Improving Video Super-Resolution with Enhanced Propagation and Alignment</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">214</td>
<td style="text-align: center;">2022-01-11</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Weng_HumanNeRF_Free-Viewpoint_Rendering_of_Moving_People_From_Monocular_Video_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Weng_HumanNeRF_Free-Viewpoint_Rendering_of_Moving_People_From_Monocular_Video_CVPR_2022_paper.pdf">HumanNeRF: Free-viewpoint Rendering of Moving People from Monocular Video</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">213</td>
<td style="text-align: center;">2021-12-09</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Wang_CLIP-NeRF_Text-and-Image_Driven_Manipulation_of_Neural_Radiance_Fields_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Wang_CLIP-NeRF_Text-and-Image_Driven_Manipulation_of_Neural_Radiance_Fields_CVPR_2022_paper.pdf">CLIP-NeRF: Text-and-Image Driven Manipulation of Neural Radiance Fields</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">212</td>
<td style="text-align: center;">2021-11-29</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Hoyer_DAFormer_Improving_Network_Architectures_and_Training_Strategies_for_Domain-Adaptive_Semantic_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Hoyer_DAFormer_Improving_Network_Architectures_and_Training_Strategies_for_Domain-Adaptive_Semantic_CVPR_2022_paper.pdf">DAFormer: Improving Network Architectures and Training Strategies for Domain-Adaptive<br />Semantic Segmentation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">195</td>
<td style="text-align: center;">2021-12-06</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Michel_Text2Mesh_Text-Driven_Neural_Stylization_for_Meshes_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Michel_Text2Mesh_Text-Driven_Neural_Stylization_for_Meshes_CVPR_2022_paper.pdf">Text2Mesh: Text-Driven Neural Stylization for Meshes</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">193</td>
<td style="text-align: center;">2022-03-16</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Zhao_Decoupled_Knowledge_Distillation_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Zhao_Decoupled_Knowledge_Distillation_CVPR_2022_paper.pdf">Decoupled Knowledge Distillation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">191</td>
<td style="text-align: center;">2022-01-09</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Tu_MAXIM_Multi-Axis_MLP_for_Image_Processing_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Tu_MAXIM_Multi-Axis_MLP_for_Image_Processing_CVPR_2022_paper.pdf">MAXIM: Multi-Axis MLP for Image Processing</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">186</td>
<td style="text-align: center;">2021-11-26</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Mildenhall_NeRF_in_the_Dark_High_Dynamic_Range_View_Synthesis_From_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Mildenhall_NeRF_in_the_Dark_High_Dynamic_Range_View_Synthesis_From_CVPR_2022_paper.pdf">NeRF in the Dark: High Dynamic Range View Synthesis<br />from Noisy Raw Images</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">183</td>
<td style="text-align: center;">2021-11-30</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Preechakul_Diffusion_Autoencoders_Toward_a_Meaningful_and_Decodable_Representation_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Preechakul_Diffusion_Autoencoders_Toward_a_Meaningful_and_Decodable_Representation_CVPR_2022_paper.pdf">Diffusion Autoencoders: Toward a Meaningful and Decodable Representation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">180</td>
<td style="text-align: center;">2021-11-24</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Munkberg_Extracting_Triangular_3D_Models_Materials_and_Lighting_From_Images_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Munkberg_Extracting_Triangular_3D_Models_Materials_and_Lighting_From_Images_CVPR_2022_paper.pdf">Extracting Triangular 3D Models, Materials, and Lighting From Images</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">178</td>
<td style="text-align: center;">2021-12-04</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Zhang_PointCLIP_Point_Cloud_Understanding_by_CLIP_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Zhang_PointCLIP_Point_Cloud_Understanding_by_CLIP_CVPR_2022_paper.pdf">PointCLIP: Point Cloud Understanding by CLIP</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">176</td>
<td style="text-align: center;">2021-10-06</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Sanghi_CLIP-Forge_Towards_Zero-Shot_Text-To-Shape_Generation_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Sanghi_CLIP-Forge_Towards_Zero-Shot_Text-To-Shape_Generation_CVPR_2022_paper.pdf">CLIP-Forge: Towards Zero-Shot Text-to-Shape Generation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">168</td>
<td style="text-align: center;">2022-04-07</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Thrush_Winoground_Probing_Vision_and_Language_Models_for_Visio-Linguistic_Compositionality_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Thrush_Winoground_Probing_Vision_and_Language_Models_for_Visio-Linguistic_Compositionality_CVPR_2022_paper.pdf">Winoground: Probing Vision and Language Models for Visio-Linguistic Compositionality</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">166</td>
<td style="text-align: center;">2021-12-06</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Roessle_Dense_Depth_Priors_for_Neural_Radiance_Fields_From_Sparse_Input_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Roessle_Dense_Depth_Priors_for_Neural_Radiance_Fields_From_Sparse_Input_CVPR_2022_paper.pdf">Dense Depth Priors for Neural Radiance Fields from Sparse<br />Input Views</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">163</td>
<td style="text-align: center;">2022-06-01</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Chen_Scaling_Vision_Transformers_to_Gigapixel_Images_via_Hierarchical_Self-Supervised_Learning_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Chen_Scaling_Vision_Transformers_to_Gigapixel_Images_via_Hierarchical_Self-Supervised_Learning_CVPR_2022_paper.pdf">Scaling Vision Transformers to Gigapixel Images via Hierarchical Self-Supervised<br />Learning</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">162</td>
<td style="text-align: center;">2021-12-13</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Sung_VL-Adapter_Parameter-Efficient_Transfer_Learning_for_Vision-and-Language_Tasks_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Sung_VL-Adapter_Parameter-Efficient_Transfer_Learning_for_Vision-and-Language_Tasks_CVPR_2022_paper.pdf">VL-ADAPTER: Parameter-Efficient Transfer Learning for Vision-and-Language Tasks</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">162</td>
<td style="text-align: center;">2022-01-03</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Xia_Vision_Transformer_With_Deformable_Attention_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Xia_Vision_Transformer_With_Deformable_Attention_CVPR_2022_paper.pdf">Vision Transformer with Deformable Attention</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">161</td>
<td style="text-align: center;">2021-11-30</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Alaluf_HyperStyle_StyleGAN_Inversion_With_HyperNetworks_for_Real_Image_Editing_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Alaluf_HyperStyle_StyleGAN_Inversion_With_HyperNetworks_for_Real_Image_Editing_CVPR_2022_paper.pdf">HyperStyle: StyleGAN Inversion with HyperNetworks for Real Image Editing</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">161</td>
<td style="text-align: center;">2021-06-09</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Beyer_Knowledge_Distillation_A_Good_Teacher_Is_Patient_and_Consistent_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Beyer_Knowledge_Distillation_A_Good_Teacher_Is_Patient_and_Consistent_CVPR_2022_paper.pdf">Knowledge distillation: A good teacher is patient and consistent</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">159</td>
<td style="text-align: center;">2022-03-25</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Wang_Continual_Test-Time_Domain_Adaptation_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Wang_Continual_Test-Time_Domain_Adaptation_CVPR_2022_paper.pdf">Continual Test-Time Domain Adaptation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">158</td>
<td style="text-align: center;">2022-03-21</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Cui_MixFormer_End-to-End_Tracking_With_Iterative_Mixed_Attention_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Cui_MixFormer_End-to-End_Tracking_With_Iterative_Mixed_Attention_CVPR_2022_paper.pdf">MixFormer: End-to-End Tracking with Iterative Mixed Attention</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">157</td>
<td style="text-align: center;">2022-06-01</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Parmar_On_Aliased_Resizing_and_Surprising_Subtleties_in_GAN_Evaluation_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Parmar_On_Aliased_Resizing_and_Surprising_Subtleties_in_GAN_Evaluation_CVPR_2022_paper.pdf">On Aliased Resizing and Surprising Subtleties in GAN Evaluation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">156</td>
<td style="text-align: center;">2021-11-30</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Wang_CRIS_CLIP-Driven_Referring_Image_Segmentation_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Wang_CRIS_CLIP-Driven_Referring_Image_Segmentation_CVPR_2022_paper.pdf">CRIS: CLIP-Driven Referring Image Segmentation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">155</td>
<td style="text-align: center;">2022-04-21</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Ma_Toward_Fast_Flexible_and_Robust_Low-Light_Image_Enhancement_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Ma_Toward_Fast_Flexible_and_Robust_Low-Light_Image_Enhancement_CVPR_2022_paper.pdf">Toward Fast, Flexible, and Robust Low-Light Image Enhancement</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">153</td>
<td style="text-align: center;">2021-09-14</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Wang_High-Fidelity_GAN_Inversion_for_Image_Attribute_Editing_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Wang_High-Fidelity_GAN_Inversion_for_Image_Attribute_Editing_CVPR_2022_paper.pdf">High-Fidelity GAN Inversion for Image Attribute Editing</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">152</td>
<td style="text-align: center;">2021-12-16</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Deng_GRAM_Generative_Radiance_Manifolds_for_3D-Aware_Image_Generation_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Deng_GRAM_Generative_Radiance_Manifolds_for_3D-Aware_Image_Generation_CVPR_2022_paper.pdf">GRAM: Generative Radiance Manifolds for 3D-Aware Image Generation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">151</td>
<td style="text-align: center;">2022-03-22</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Cazenavette_Dataset_Distillation_by_Matching_Training_Trajectories_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Cazenavette_Dataset_Distillation_by_Matching_Training_Trajectories_CVPR_2022_paper.pdf">Dataset Distillation by Matching Training Trajectories</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">150</td>
<td style="text-align: center;">2021-12-09</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Chung_Come-Closer-Diffuse-Faster_Accelerating_Conditional_Diffusion_Models_for_Inverse_Problems_Through_Stochastic_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Chung_Come-Closer-Diffuse-Faster_Accelerating_Conditional_Diffusion_Models_for_Inverse_Problems_Through_Stochastic_CVPR_2022_paper.pdf">Come-Closer-Diffuse-Faster: Accelerating Conditional Diffusion Models for Inverse Problems through<br />Stochastic Contraction</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">148</td>
<td style="text-align: center;">2022-02-21</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Yang_Vision-Language_Pre-Training_With_Triple_Contrastive_Learning_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Yang_Vision-Language_Pre-Training_With_Triple_Contrastive_Learning_CVPR_2022_paper.pdf">Vision-Language Pre-Training with Triple Contrastive Learning</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">147</td>
<td style="text-align: center;">2021-04-09</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Azinovic_Neural_RGB-D_Surface_Reconstruction_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Azinovic_Neural_RGB-D_Surface_Reconstruction_CVPR_2022_paper.pdf">Neural RGB-D Surface Reconstruction</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">145</td>
<td style="text-align: center;">2022-03-08</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Wang_Semi-Supervised_Semantic_Segmentation_Using_Unreliable_Pseudo-Labels_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Wang_Semi-Supervised_Semantic_Segmentation_Using_Unreliable_Pseudo-Labels_CVPR_2022_paper.pdf">Semi-Supervised Semantic Segmentation Using Unreliable Pseudo-Labels</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">142</td>
<td style="text-align: center;">2022-04-03</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Kim_AdaFace_Quality_Adaptive_Margin_for_Face_Recognition_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Kim_AdaFace_Quality_Adaptive_Margin_for_Face_Recognition_CVPR_2022_paper.pdf">AdaFace: Quality Adaptive Margin for Face Recognition</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">141</td>
<td style="text-align: center;">2021-12-18</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Luddecke_Image_Segmentation_Using_Text_and_Image_Prompts_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Luddecke_Image_Segmentation_Using_Text_and_Image_Prompts_CVPR_2022_paper.pdf">Image Segmentation Using Text and Image Prompts</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">138</td>
<td style="text-align: center;">2021-12-10</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Hong_HeadNeRF_A_Real-Time_NeRF-Based_Parametric_Head_Model_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Hong_HeadNeRF_A_Real-Time_NeRF-Based_Parametric_Head_Model_CVPR_2022_paper.pdf">HeadNeRF: A Realtime NeRF-based Parametric Head Model</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">138</td>
<td style="text-align: center;">2021-12-29</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Skorokhodov_StyleGAN-V_A_Continuous_Video_Generator_With_the_Price_Image_Quality_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Skorokhodov_StyleGAN-V_A_Continuous_Video_Generator_With_the_Price_Image_Quality_CVPR_2022_paper.pdf">StyleGAN-V: A Continuous Video Generator with the Price, Image<br />Quality and Perks of StyleGAN2</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">137</td>
<td style="text-align: center;">2022-05-09</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Kundu_Panoptic_Neural_Fields_A_Semantic_Object-Aware_Neural_Scene_Representation_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Kundu_Panoptic_Neural_Fields_A_Semantic_Object-Aware_Neural_Scene_Representation_CVPR_2022_paper.pdf">Panoptic Neural Fields: A Semantic Object-Aware Neural Scene Representation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">137</td>
<td style="text-align: center;">2021-11-18</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Khandelwal_Simple_but_Effective_CLIP_Embeddings_for_Embodied_AI_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Khandelwal_Simple_but_Effective_CLIP_Embeddings_for_Embodied_AI_CVPR_2022_paper.pdf">Simple but Effective: CLIP Embeddings for Embodied AI</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">136</td>
<td style="text-align: center;">2022-01-26</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Deng_Anomaly_Detection_via_Reverse_Distillation_From_One-Class_Embedding_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Deng_Anomaly_Detection_via_Reverse_Distillation_From_One-Class_Embedding_CVPR_2022_paper.pdf">Anomaly Detection via Reverse Distillation from One-Class Embedding</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">136</td>
<td style="text-align: center;">2022-03-15</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Li_DeepFusion_Lidar-Camera_Deep_Fusion_for_Multi-Modal_3D_Object_Detection_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Li_DeepFusion_Lidar-Camera_Deep_Fusion_for_Multi-Modal_3D_Object_Detection_CVPR_2022_paper.pdf">DeepFusion: Lidar-Camera Deep Fusion for Multi-Modal 3D Object Detection</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">136</td>
<td style="text-align: center;">2021-12-16</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Xiu_ICON_Implicit_Clothed_Humans_Obtained_From_Normals_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Xiu_ICON_Implicit_Clothed_Humans_Obtained_From_Normals_CVPR_2022_paper.pdf">ICON: Implicit Clothed humans Obtained from Normals</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">135</td>
<td style="text-align: center;">2022-02-14</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Qin_Geometric_Transformer_for_Fast_and_Robust_Point_Cloud_Registration_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Qin_Geometric_Transformer_for_Fast_and_Robust_Point_Cloud_Registration_CVPR_2022_paper.pdf">Geometric Transformer for Fast and Robust Point Cloud Registration</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">135</td>
<td style="text-align: center;">2021-12-01</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Kwon_CLIPstyler_Image_Style_Transfer_With_a_Single_Text_Condition_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Kwon_CLIPstyler_Image_Style_Transfer_With_a_Single_Text_Condition_CVPR_2022_paper.pdf">CLIPstyler: Image Style Transfer with a Single Text Condition</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">133</td>
<td style="text-align: center;">2022-03-21</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Zhang_Not_All_Points_Are_Equal_Learning_Highly_Efficient_Point-Based_Detectors_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Zhang_Not_All_Points_Are_Equal_Learning_Highly_Efficient_Point-Based_Detectors_CVPR_2022_paper.pdf">Not All Points Are Equal: Learning Highly Efficient Point-based<br />Detectors for 3D LiDAR Point Clouds</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">132</td>
<td style="text-align: center;">2021-12-20</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Turki_Mega-NERF_Scalable_Construction_of_Large-Scale_NeRFs_for_Virtual_Fly-Throughs_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Turki_Mega-NERF_Scalable_Construction_of_Large-Scale_NeRFs_for_Virtual_Fly-Throughs_CVPR_2022_paper.pdf">Mega-NeRF: Scalable Construction of Large-Scale NeRFs for Virtual Fly-<br />Throughs</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">132</td>
<td style="text-align: center;">2022-01-12</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Yan_Multiview_Transformers_for_Video_Recognition_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Yan_Multiview_Transformers_for_Video_Recognition_CVPR_2022_paper.pdf">Multiview Transformers for Video Recognition</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">130</td>
<td style="text-align: center;">2022-03-28</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Lai_Stratified_Transformer_for_3D_Point_Cloud_Segmentation_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Lai_Stratified_Transformer_for_3D_Point_Cloud_Segmentation_CVPR_2022_paper.pdf">Stratified Transformer for 3D Point Cloud Segmentation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">129</td>
<td style="text-align: center;">2021-11-26</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Xu_GMFlow_Learning_Optical_Flow_via_Global_Matching_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Xu_GMFlow_Learning_Optical_Flow_via_Global_Matching_CVPR_2022_paper.pdf">GMFlow: Learning Optical Flow via Global Matching</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">129</td>
<td style="text-align: center;">2022-01-07</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Zellers_MERLOT_Reserve_Neural_Script_Knowledge_Through_Vision_and_Language_and_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Zellers_MERLOT_Reserve_Neural_Script_Knowledge_Through_Vision_and_Language_and_CVPR_2022_paper.pdf">MERLOT RESERVE: Neural Script Knowledge through Vision and Language<br />and Sound</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">129</td>
<td style="text-align: center;">2022-04-07</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Yang_Unified_Contrastive_Learning_in_Image-Text-Label_Space_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Yang_Unified_Contrastive_Learning_in_Image-Text-Label_Space_CVPR_2022_paper.pdf">Unified Contrastive Learning in Image-Text-Label Space</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">128</td>
<td style="text-align: center;">2022-03-01</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Afham_CrossPoint_Self-Supervised_Cross-Modal_Contrastive_Learning_for_3D_Point_Cloud_Understanding_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Afham_CrossPoint_Self-Supervised_Cross-Modal_Contrastive_Learning_for_3D_Point_Cloud_Understanding_CVPR_2022_paper.pdf">CrossPoint: Self-Supervised Cross-Modal Contrastive Learning for 3D Point Cloud<br />Understanding</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">127</td>
<td style="text-align: center;">2022-05-05</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Zhou_Cross-View_Transformers_for_Real-Time_Map-View_Semantic_Segmentation_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Zhou_Cross-View_Transformers_for_Real-Time_Map-View_Semantic_Segmentation_CVPR_2022_paper.pdf">Cross-view Transformers for real-time Map-view Semantic Segmentation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">127</td>
<td style="text-align: center;">2021-12-13</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Fan_Embracing_Single_Stride_3D_Object_Detector_With_Sparse_Transformer_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Fan_Embracing_Single_Stride_3D_Object_Detector_With_Sparse_Transformer_CVPR_2022_paper.pdf">Embracing Single Stride 3D Object Detector with Sparse Transformer</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">127</td>
<td style="text-align: center;">2021-12-14</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Zheng_I_M_Avatar_Implicit_Morphable_Head_Avatars_From_Videos_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Zheng_I_M_Avatar_Implicit_Morphable_Head_Avatars_From_Videos_CVPR_2022_paper.pdf">I M Avatar: Implicit Morphable Head Avatars from Videos</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">127</td>
<td style="text-align: center;">2021-11-29</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Rematas_Urban_Radiance_Fields_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Rematas_Urban_Radiance_Fields_CVPR_2022_paper.pdf">Urban Radiance Fields</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">126</td>
<td style="text-align: center;">2022-03-29</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Li_MAT_Mask-Aware_Transformer_for_Large_Hole_Image_Inpainting_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Li_MAT_Mask-Aware_Transformer_for_Large_Hole_Image_Inpainting_CVPR_2022_paper.pdf">MAT: Mask-Aware Transformer for Large Hole Image Inpainting</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">125</td>
<td style="text-align: center;">2021-12-14</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Yin_A-ViT_Adaptive_Tokens_for_Efficient_Vision_Transformer_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Yin_A-ViT_Adaptive_Tokens_for_Efficient_Vision_Transformer_CVPR_2022_paper.pdf">A-ViT: Adaptive Tokens for Efficient Vision Transformer</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">125</td>
<td style="text-align: center;">2022-06-01</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Guo_Generating_Diverse_and_Natural_3D_Human_Motions_From_Text_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Guo_Generating_Diverse_and_Natural_3D_Human_Motions_From_Text_CVPR_2022_paper.pdf">Generating Diverse and Natural 3D Human Motions from Text</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">123</td>
<td style="text-align: center;">2021-12-02</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Wang_BEVT_BERT_Pretraining_of_Video_Transformers_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Wang_BEVT_BERT_Pretraining_of_Video_Transformers_CVPR_2022_paper.pdf">BEVT: BERT Pretraining of Video Transformers</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">123</td>
<td style="text-align: center;">2021-11-22</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Douillard_DyTox_Transformers_for_Continual_Learning_With_DYnamic_TOken_eXpansion_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Douillard_DyTox_Transformers_for_Continual_Learning_With_DYnamic_TOken_eXpansion_CVPR_2022_paper.pdf">DyTox: Transformers for Continual Learning with DYnamic TOken eXpansion</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">122</td>
<td style="text-align: center;">2021-12-05</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Whang_Deblurring_via_Stochastic_Refinement_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Whang_Deblurring_via_Stochastic_Refinement_CVPR_2022_paper.pdf">Deblurring via Stochastic Refinement</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">118</td>
<td style="text-align: center;">2022-03-07</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Greff_Kubric_A_Scalable_Dataset_Generator_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Greff_Kubric_A_Scalable_Dataset_Generator_CVPR_2022_paper.pdf">Kubric: A scalable dataset generator</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">118</td>
<td style="text-align: center;">2021-11-29</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Pan_On_the_Integration_of_Self-Attention_and_Convolution_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Pan_On_the_Integration_of_Self-Attention_and_Convolution_CVPR_2022_paper.pdf">On the Integration of Self-Attention and Convolution</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">115</td>
<td style="text-align: center;">2021-12-17</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Li_Align_and_Prompt_Video-and-Language_Pre-Training_With_Entity_Prompts_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Li_Align_and_Prompt_Video-and-Language_Pre-Training_With_Entity_Prompts_CVPR_2022_paper.pdf">Align and Prompt: Video-and-Language Pre-training with Entity Prompts</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">114</td>
<td style="text-align: center;">2021-11-24</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Li_MHFormer_Multi-Hypothesis_Transformer_for_3D_Human_Pose_Estimation_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Li_MHFormer_Multi-Hypothesis_Transformer_for_3D_Human_Pose_Estimation_CVPR_2022_paper.pdf">MHFormer: Multi-Hypothesis Transformer for 3D Human Pose Estimation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">114</td>
<td style="text-align: center;">2022-01-20</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Girdhar_Omnivore_A_Single_Model_for_Many_Visual_Modalities_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Girdhar_Omnivore_A_Single_Model_for_Many_Visual_Modalities_CVPR_2022_paper.pdf">Omnivore: A Single Model for Many Visual Modalities</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">113</td>
<td style="text-align: center;">2022-05-10</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Yuan_NeRF-Editing_Geometry_Editing_of_Neural_Radiance_Fields_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Yuan_NeRF-Editing_Geometry_Editing_of_Neural_Radiance_Fields_CVPR_2022_paper.pdf">NeRF-Editing: Geometry Editing of Neural Radiance Fields</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">113</td>
<td style="text-align: center;">2022-03-21</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Wang_ViM_Out-of-Distribution_With_Virtual-Logit_Matching_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Wang_ViM_Out-of-Distribution_With_Virtual-Logit_Matching_CVPR_2022_paper.pdf">ViM: Out-Of-Distribution with Virtual-logit Matching</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">111</td>
<td style="text-align: center;">2022-04-21</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Chen_Contrastive_Test-Time_Adaptation_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Chen_Contrastive_Test-Time_Adaptation_CVPR_2022_paper.pdf">Contrastive Test-Time Adaptation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">111</td>
<td style="text-align: center;">2021-05-24</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Li_Oriented_RepPoints_for_Aerial_Object_Detection_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Li_Oriented_RepPoints_for_Aerial_Object_Detection_CVPR_2022_paper.pdf">Oriented RepPoints for Aerial Object Detection</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">109</td>
<td style="text-align: center;">2021-12-20</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Zhang_StyleSwin_Transformer-Based_GAN_for_High-Resolution_Image_Generation_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Zhang_StyleSwin_Transformer-Based_GAN_for_High-Resolution_Image_Generation_CVPR_2022_paper.pdf">StyleSwin: Transformer-based GAN for High-resolution Image Generation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">109</td>
<td style="text-align: center;">2021-12-02</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Grassal_Neural_Head_Avatars_From_Monocular_RGB_Videos_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Grassal_Neural_Head_Avatars_From_Monocular_RGB_Videos_CVPR_2022_paper.pdf">Neural Head Avatars from Monocular RGB Videos</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">108</td>
<td style="text-align: center;">2022-03-30</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Liu_Target-Aware_Dual_Adversarial_Learning_and_a_Multi-Scenario_Multi-Modality_Benchmark_To_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Liu_Target-Aware_Dual_Adversarial_Learning_and_a_Multi-Scenario_Multi-Modality_Benchmark_To_CVPR_2022_paper.pdf">Target-aware Dual Adversarial Learning and a Multi-scenario Multi-Modality Benchmark<br />to Fuse Infrared and Visible for Object Detection</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">108</td>
<td style="text-align: center;">2021-12-21</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Lee_MPViT_Multi-Path_Vision_Transformer_for_Dense_Prediction_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Lee_MPViT_Multi-Path_Vision_Transformer_for_Dense_Prediction_CVPR_2022_paper.pdf">MPViT: Multi-Path Vision Transformer for Dense Prediction</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">107</td>
<td style="text-align: center;">2021-05-17</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Mao_Towards_Robust_Vision_Transformer_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Mao_Towards_Robust_Vision_Transformer_CVPR_2022_paper.pdf">Towards Robust Vision Transformer</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">106</td>
<td style="text-align: center;">2022-03-28</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Zhou_Rethinking_Semantic_Segmentation_A_Prototype_View_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Zhou_Rethinking_Semantic_Segmentation_A_Prototype_View_CVPR_2022_paper.pdf">Rethinking Semantic Segmentation: A Prototype View</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">105</td>
<td style="text-align: center;">2021-07-28</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Liu_Neural_Rays_for_Occlusion-Aware_Image-Based_Rendering_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Liu_Neural_Rays_for_Occlusion-Aware_Image-Based_Rendering_CVPR_2022_paper.pdf">Neural Rays for Occlusion-aware Image-based Rendering</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">105</td>
<td style="text-align: center;">2022-03-17</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Mittal_AutoSDF_Shape_Priors_for_3D_Completion_Reconstruction_and_Generation_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Mittal_AutoSDF_Shape_Priors_for_3D_Completion_Reconstruction_and_Generation_CVPR_2022_paper.pdf">AutoSDF: Shape Priors for 3D Completion, Reconstruction and Generation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">105</td>
<td style="text-align: center;">2021-11-25</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Lin_SwinBERT_End-to-End_Transformers_With_Sparse_Attention_for_Video_Captioning_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Lin_SwinBERT_End-to-End_Transformers_With_Sparse_Attention_for_Video_Captioning_CVPR_2022_paper.pdf">SwinBERT: End-to-End Transformers with Sparse Attention for Video Captioning</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">104</td>
<td style="text-align: center;">2022-04-12</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Yu_DAIR-V2X_A_Large-Scale_Dataset_for_Vehicle-Infrastructure_Cooperative_3D_Object_Detection_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Yu_DAIR-V2X_A_Large-Scale_Dataset_for_Vehicle-Infrastructure_Cooperative_3D_Object_Detection_CVPR_2022_paper.pdf">DAIR-V2X: A Large-Scale Dataset for Vehicle-Infrastructure Cooperative 3D Object<br />Detection</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">104</td>
<td style="text-align: center;">2021-11-23</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Yang_Focal_and_Global_Knowledge_Distillation_for_Detectors_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Yang_Focal_and_Global_Knowledge_Distillation_for_Detectors_CVPR_2022_paper.pdf">Focal and Global Knowledge Distillation for Detectors</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">103</td>
<td style="text-align: center;">2021-11-15</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Cai_Mask-Guided_Spectral-Wise_Transformer_for_Efficient_Hyperspectral_Image_Reconstruction_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Cai_Mask-Guided_Spectral-Wise_Transformer_for_Efficient_Hyperspectral_Image_Reconstruction_CVPR_2022_paper.pdf">Mask-guided Spectral-wise Transformer for Efficient Hyperspectral Image Reconstruction</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">102</td>
<td style="text-align: center;">2021-11-29</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Valanarasu_TransWeather_Transformer-Based_Restoration_of_Images_Degraded_by_Adverse_Weather_Conditions_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Valanarasu_TransWeather_Transformer-Based_Restoration_of_Images_Degraded_by_Adverse_Weather_Conditions_CVPR_2022_paper.pdf">TransWeather: Transformer-based Restoration of Images Degraded by Adverse Weather<br />Conditions</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">101</td>
<td style="text-align: center;">2021-11-25</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Sajjadi_Scene_Representation_Transformer_Geometry-Free_Novel_View_Synthesis_Through_Set-Latent_Scene_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Sajjadi_Scene_Representation_Transformer_Geometry-Free_Novel_View_Synthesis_Through_Set-Latent_Scene_CVPR_2022_paper.pdf">Scene Representation Transformer: Geometry-Free Novel View Synthesis Through Set-Latent<br />Scene Representations</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">101</td>
<td style="text-align: center;">2022-06-01</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Wu_URetinex-Net_Retinex-Based_Deep_Unfolding_Network_for_Low-Light_Image_Enhancement_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Wu_URetinex-Net_Retinex-Based_Deep_Unfolding_Network_for_Low-Light_Image_Enhancement_CVPR_2022_paper.pdf">URetinex-Net: Retinex-based Deep Unfolding Network for Low-light Image Enhancement</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">99</td>
<td style="text-align: center;">2021-02-20</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Chen_VisualGPT_Data-Efficient_Adaptation_of_Pretrained_Language_Models_for_Image_Captioning_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Chen_VisualGPT_Data-Efficient_Adaptation_of_Pretrained_Language_Models_for_Image_Captioning_CVPR_2022_paper.pdf">VisualGPT: Data-efficient Adaptation of Pretrained Language Models for Image<br />Captioning</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">99</td>
<td style="text-align: center;">2020-08-13</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Tao_DF-GAN_A_Simple_and_Effective_Baseline_for_Text-to-Image_Synthesis_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Tao_DF-GAN_A_Simple_and_Effective_Baseline_for_Text-to-Image_Synthesis_CVPR_2022_paper.pdf">DF-GAN: A Simple and Effective Baseline for Text-to-Image Synthesis</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">99</td>
<td style="text-align: center;">2021-11-27</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Zhou_Towards_Language-Free_Training_for_Text-to-Image_Generation_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Zhou_Towards_Language-Free_Training_for_Text-to-Image_Generation_CVPR_2022_paper.pdf">Towards Language-Free Training for Text-to-Image Generation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">98</td>
<td style="text-align: center;">2021-11-29</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Tewel_ZeroCap_Zero-Shot_Image-to-Text_Generation_for_Visual-Semantic_Arithmetic_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Tewel_ZeroCap_Zero-Shot_Image-to-Text_Generation_for_Visual-Semantic_Arithmetic_CVPR_2022_paper.pdf">ZeroCap: Zero-Shot Image-to-Text Generation for Visual-Semantic Arithmetic</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">98</td>
<td style="text-align: center;">2022-04-26</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Chen_Focal_Sparse_Convolutional_Networks_for_3D_Object_Detection_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Chen_Focal_Sparse_Convolutional_Networks_for_3D_Object_Detection_CVPR_2022_paper.pdf">Focal Sparse Convolutional Networks for 3D Object Detection</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">97</td>
<td style="text-align: center;">2021-11-30</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Ren_Shunted_Self-Attention_via_Multi-Scale_Token_Aggregation_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Ren_Shunted_Self-Attention_via_Multi-Scale_Token_Aggregation_CVPR_2022_paper.pdf">Shunted Self-Attention via Multi-Scale Token Aggregation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">96</td>
<td style="text-align: center;">2022-02-17</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Wang_Fourier_PlenOctrees_for_Dynamic_Radiance_Field_Rendering_in_Real-Time_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Wang_Fourier_PlenOctrees_for_Dynamic_Radiance_Field_Rendering_in_Real-Time_CVPR_2022_paper.pdf">Fourier PlenOctrees for Dynamic Radiance Field Rendering in Real-time</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">96</td>
<td style="text-align: center;">2022-03-22</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Zhang_DTFD-MIL_Double-Tier_Feature_Distillation_Multiple_Instance_Learning_for_Histopathology_Whole_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Zhang_DTFD-MIL_Double-Tier_Feature_Distillation_Multiple_Instance_Learning_for_Histopathology_Whole_CVPR_2022_paper.pdf">DTFD-MIL: Double-Tier Feature Distillation Multiple Instance Learning for Histopathology<br />Whole Slide Image Classification</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">95</td>
<td style="text-align: center;">2022-05-06</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Lu_Prompt_Distribution_Learning_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Lu_Prompt_Distribution_Learning_CVPR_2022_paper.pdf">Prompt Distribution Learning</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">95</td>
<td style="text-align: center;">2021-06-05</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Tang_Patch_Slimming_for_Efficient_Vision_Transformers_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Tang_Patch_Slimming_for_Efficient_Vision_Transformers_CVPR_2022_paper.pdf">Patch Slimming for Efficient Vision Transformers</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">95</td>
<td style="text-align: center;">2021-05-30</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Deng_StyTr2_Image_Style_Transfer_With_Transformers_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Deng_StyTr2_Image_Style_Transfer_With_Transformers_CVPR_2022_paper.pdf">StyTr2: Image Style Transfer with Transformers</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">94</td>
<td style="text-align: center;">2022-03-21</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Mayer_Transforming_Model_Prediction_for_Tracking_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Mayer_Transforming_Model_Prediction_for_Tracking_CVPR_2022_paper.pdf">Transforming Model Prediction for Tracking</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">94</td>
<td style="text-align: center;">2021-11-01</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Gu_Multi-Scale_High-Resolution_Vision_Transformer_for_Semantic_Segmentation_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Gu_Multi-Scale_High-Resolution_Vision_Transformer_for_Semantic_Segmentation_CVPR_2022_paper.pdf">Multi-Scale High-Resolution Vision Transformer for Semantic Segmentation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">94</td>
<td style="text-align: center;">2021-11-26</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Johari_GeoNeRF_Generalizing_NeRF_With_Geometry_Priors_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Johari_GeoNeRF_Generalizing_NeRF_With_Geometry_Priors_CVPR_2022_paper.pdf">GeoNeRF: Generalizing NeRF with Geometry Priors</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">94</td>
<td style="text-align: center;">2022-02-23</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Wang_Self-Supervised_Transformers_for_Unsupervised_Object_Discovery_Using_Normalized_Cut_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Wang_Self-Supervised_Transformers_for_Unsupervised_Object_Discovery_Using_Normalized_Cut_CVPR_2022_paper.pdf">Self-Supervised Transformers for Unsupervised Object Discovery using Normalized Cut</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">92</td>
<td style="text-align: center;">2021-12-20</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Xu_3D-Aware_Image_Synthesis_via_Learning_Structural_and_Textural_Representations_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Xu_3D-Aware_Image_Synthesis_via_Learning_Structural_and_Textural_Representations_CVPR_2022_paper.pdf">3D-aware Image Synthesis via Learning Structural and Textural Representations</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">91</td>
<td style="text-align: center;">2022-05-16</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Melas-Kyriazi_Deep_Spectral_Methods_A_Surprisingly_Strong_Baseline_for_Unsupervised_Semantic_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Melas-Kyriazi_Deep_Spectral_Methods_A_Surprisingly_Strong_Baseline_for_Unsupervised_Semantic_CVPR_2022_paper.pdf">Deep Spectral Methods: A Surprisingly Strong Baseline for Unsupervised<br />Semantic Segmentation and Localization</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">91</td>
<td style="text-align: center;">2022-01-20</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Seo_End-to-End_Generative_Pretraining_for_Multimodal_Video_Captioning_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Seo_End-to-End_Generative_Pretraining_for_Multimodal_Video_Captioning_CVPR_2022_paper.pdf">End-to-end Generative Pretraining for Multimodal Video Captioning</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">90</td>
<td style="text-align: center;">None</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Wang_CAFE_Learning_To_Condense_Dataset_by_Aligning_Features_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Wang_CAFE_Learning_To_Condense_Dataset_by_Aligning_Features_CVPR_2022_paper.pdf">CAFE Learning to Condense Dataset by Aligning Features</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">88</td>
<td style="text-align: center;">2021-05-12</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Cole_When_Does_Contrastive_Visual_Representation_Learning_Work_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Cole_When_Does_Contrastive_Visual_Representation_Learning_Work_CVPR_2022_paper.pdf">When Does Contrastive Visual Representation Learning Work?</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">88</td>
<td style="text-align: center;">2022-04-18</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Shiohara_Detecting_Deepfakes_With_Self-Blended_Images_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Shiohara_Detecting_Deepfakes_With_Self-Blended_Images_CVPR_2022_paper.pdf">Detecting Deepfakes with Self-Blended Images</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">88</td>
<td style="text-align: center;">2022-04-01</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Choi_Perception_Prioritized_Training_of_Diffusion_Models_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Choi_Perception_Prioritized_Training_of_Diffusion_Models_CVPR_2022_paper.pdf">Perception Prioritized Training of Diffusion Models</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">88</td>
<td style="text-align: center;">2022-03-21</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/He_ELIC_Efficient_Learned_Image_Compression_With_Unevenly_Grouped_Space-Channel_Contextual_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/He_ELIC_Efficient_Learned_Image_Compression_With_Unevenly_Grouped_Space-Channel_Contextual_CVPR_2022_paper.pdf">ELIC: Efficient Learned Image Compression with Unevenly Grouped Space-Channel<br />Contextual Adaptive Coding</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">87</td>
<td style="text-align: center;">2021-12-23</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Yang_BANMo_Building_Animatable_3D_Neural_Models_From_Many_Casual_Videos_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Yang_BANMo_Building_Animatable_3D_Neural_Models_From_Many_Casual_Videos_CVPR_2022_paper.pdf">BANMo: Building Animatable 3D Neural Models from Many Casual<br />Videos</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">87</td>
<td style="text-align: center;">2021-11-30</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Sun_FENeRF_Face_Editing_in_Neural_Radiance_Fields_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Sun_FENeRF_Face_Editing_in_Neural_Radiance_Fields_CVPR_2022_paper.pdf">FENeRF: Face Editing in Neural Radiance Fields</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">87</td>
<td style="text-align: center;">2021-11-30</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Meng_AdaViT_Adaptive_Vision_Transformers_for_Efficient_Image_Recognition_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Meng_AdaViT_Adaptive_Vision_Transformers_for_Efficient_Image_Recognition_CVPR_2022_paper.pdf">AdaViT: Adaptive Vision Transformers for Efficient Image Recognition</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">87</td>
<td style="text-align: center;">2022-01-20</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Wu_MeMViT_Memory-Augmented_Multiscale_Vision_Transformer_for_Efficient_Long-Term_Video_Recognition_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Wu_MeMViT_Memory-Augmented_Multiscale_Vision_Transformer_for_Efficient_Long-Term_Video_Recognition_CVPR_2022_paper.pdf">MeMViT: Memory-Augmented Multiscale Vision Transformer for Efficient Long-Term Video<br />Recognition</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">86</td>
<td style="text-align: center;">2021-10-12</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Collins_ABO_Dataset_and_Benchmarks_for_Real-World_3D_Object_Understanding_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Collins_ABO_Dataset_and_Benchmarks_for_Real-World_3D_Object_Understanding_CVPR_2022_paper.pdf">ABO: Dataset and Benchmarks for Real-World 3D Object Understanding</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">86</td>
<td style="text-align: center;">2021-12-15</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Ding_Decoupling_Zero-Shot_Semantic_Segmentation_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Ding_Decoupling_Zero-Shot_Semantic_Segmentation_CVPR_2022_paper.pdf">Decoupling Zero-Shot Semantic Segmentation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">85</td>
<td style="text-align: center;">2021-11-27</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Li_Targeted_Supervised_Contrastive_Learning_for_Long-Tailed_Recognition_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Li_Targeted_Supervised_Contrastive_Learning_for_Long-Tailed_Recognition_CVPR_2022_paper.pdf">Targeted Supervised Contrastive Learning for Long-Tailed Recognition</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">84</td>
<td style="text-align: center;">2022-03-03</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Lee_Autoregressive_Image_Generation_Using_Residual_Quantization_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Lee_Autoregressive_Image_Generation_Using_Residual_Quantization_CVPR_2022_paper.pdf">Autoregressive Image Generation using Residual Quantization</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">84</td>
<td style="text-align: center;">2022-06-01</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Guo_Image_Dehazing_Transformer_With_Transmission-Aware_3D_Position_Embedding_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Guo_Image_Dehazing_Transformer_With_Transmission-Aware_3D_Position_Embedding_CVPR_2022_paper.pdf">Image Dehazing Transformer with Transmission-Aware 3D Position Embedding</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">83</td>
<td style="text-align: center;">2022-04-15</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Hu_Pushing_the_Limits_of_Simple_Pipelines_for_Few-Shot_Learning_External_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Hu_Pushing_the_Limits_of_Simple_Pipelines_for_Few-Shot_Learning_External_CVPR_2022_paper.pdf">Pushing the Limits of Simple Pipelines for Few-Shot Learning:<br />External Data and Fine-Tuning Make a Difference</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">83</td>
<td style="text-align: center;">2022-03-19</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/He_Voxel_Set_Transformer_A_Set-to-Set_Approach_to_3D_Object_Detection_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/He_Voxel_Set_Transformer_A_Set-to-Set_Approach_to_3D_Object_Detection_CVPR_2022_paper.pdf">Voxel Set Transformer: A Set-to-Set Approach to 3D Object<br />Detection from Point Clouds</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">83</td>
<td style="text-align: center;">2022-03-06</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Xu_Multi-Class_Token_Transformer_for_Weakly_Supervised_Semantic_Segmentation_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Xu_Multi-Class_Token_Transformer_for_Weakly_Supervised_Semantic_Segmentation_CVPR_2022_paper.pdf">Multi-class Token Transformer for Weakly Supervised Semantic Segmentation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">82</td>
<td style="text-align: center;">2021-11-29</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Sun_DanceTrack_Multi-Object_Tracking_in_Uniform_Appearance_and_Diverse_Motion_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Sun_DanceTrack_Multi-Object_Tracking_in_Uniform_Appearance_and_Diverse_Motion_CVPR_2022_paper.pdf">DanceTrack: Multi-Object Tracking in Uniform Appearance and Diverse Motion</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">82</td>
<td style="text-align: center;">2021-03-16</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Yang_QueryDet_Cascaded_Sparse_Query_for_Accelerating_High-Resolution_Small_Object_Detection_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Yang_QueryDet_Cascaded_Sparse_Query_for_Accelerating_High-Resolution_Small_Object_Detection_CVPR_2022_paper.pdf">QueryDet: Cascaded Sparse Query for Accelerating High-Resolution Small Object<br />Detection</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">82</td>
<td style="text-align: center;">2022-03-03</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Vu_SoftGroup_for_3D_Instance_Segmentation_on_Point_Clouds_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Vu_SoftGroup_for_3D_Instance_Segmentation_on_Point_Clouds_CVPR_2022_paper.pdf">SoftGroup for 3D Instance Segmentation on Point Clouds</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">82</td>
<td style="text-align: center;">2022-06-01</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Li_All-in-One_Image_Restoration_for_Unknown_Corruption_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Li_All-in-One_Image_Restoration_for_Unknown_Corruption_CVPR_2022_paper.pdf">All-In-One Image Restoration for Unknown Corruption</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">81</td>
<td style="text-align: center;">2022-04-12</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Zhang_TopFormer_Token_Pyramid_Transformer_for_Mobile_Semantic_Segmentation_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Zhang_TopFormer_Token_Pyramid_Transformer_for_Mobile_Semantic_Segmentation_CVPR_2022_paper.pdf">TopFormer: Token Pyramid Transformer for Mobile Semantic Segmentation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">81</td>
<td style="text-align: center;">2022-03-15</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Lang_Learning_What_Not_To_Segment_A_New_Perspective_on_Few-Shot_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Lang_Learning_What_Not_To_Segment_A_New_Perspective_on_Few-Shot_CVPR_2022_paper.pdf">Learning What Not to Segment: A New Perspective on<br />Few-Shot Segmentation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">81</td>
<td style="text-align: center;">2021-11-25</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Liu_Perturbed_and_Strict_Mean_Teachers_for_Semi-Supervised_Semantic_Segmentation_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Liu_Perturbed_and_Strict_Mean_Teachers_for_Semi-Supervised_Semantic_Segmentation_CVPR_2022_paper.pdf">Perturbed and Strict Mean Teachers for Semi-supervised Semantic Segmentation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">81</td>
<td style="text-align: center;">2021-12-17</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Suhail_Light_Field_Neural_Rendering_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Suhail_Light_Field_Neural_Rendering_CVPR_2022_paper.pdf">Light Field Neural Rendering</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">81</td>
<td style="text-align: center;">2022-03-05</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Ru_Learning_Affinity_From_Attention_End-to-End_Weakly-Supervised_Semantic_Segmentation_With_Transformers_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Ru_Learning_Affinity_From_Attention_End-to-End_Weakly-Supervised_Semantic_Segmentation_With_Transformers_CVPR_2022_paper.pdf">Learning Affinity from Attention: End-to-End Weakly-Supervised Semantic Segmentation with<br />Transformers</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">80</td>
<td style="text-align: center;">2021-12-02</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Zhu_Uni-Perceiver_Pre-Training_Unified_Architecture_for_Generic_Perception_for_Zero-Shot_and_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Zhu_Uni-Perceiver_Pre-Training_Unified_Architecture_for_Generic_Perception_for_Zero-Shot_and_CVPR_2022_paper.pdf">Uni-Perceiver: Pre-training Unified Architecture for Generic Perception for Zero-shot<br />and Few-shot Tasks</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">80</td>
<td style="text-align: center;">2021-12-01</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Dinh_HyperInverter_Improving_StyleGAN_Inversion_via_Hypernetwork_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Dinh_HyperInverter_Improving_StyleGAN_Inversion_via_Hypernetwork_CVPR_2022_paper.pdf">HyperInverter: Improving StyleGAN Inversion via Hypernetwork</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">80</td>
<td style="text-align: center;">2022-05-05</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Guo_Neural_3D_Scene_Reconstruction_With_the_Manhattan-World_Assumption_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Guo_Neural_3D_Scene_Reconstruction_With_the_Manhattan-World_Assumption_CVPR_2022_paper.pdf">Neural 3D Scene Reconstruction with the Manhattan-world Assumption</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">80</td>
<td style="text-align: center;">2021-11-27</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Shi_Video_Frame_Interpolation_Transformer_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Shi_Video_Frame_Interpolation_Transformer_CVPR_2022_paper.pdf">Video Frame Interpolation Transformer</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">80</td>
<td style="text-align: center;">2022-01-07</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Vaze_Generalized_Category_Discovery_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Vaze_Generalized_Category_Discovery_CVPR_2022_paper.pdf">Generalized Category Discovery</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">79</td>
<td style="text-align: center;">2022-01-02</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Tumanyan_Splicing_ViT_Features_for_Semantic_Appearance_Transfer_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Tumanyan_Splicing_ViT_Features_for_Semantic_Appearance_Transfer_CVPR_2022_paper.pdf">Splicing ViT Features for Semantic Appearance Transfer</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">79</td>
<td style="text-align: center;">2022-03-22</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Li_Practical_Stereo_Matching_via_Cascaded_Recurrent_Network_With_Adaptive_Correlation_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Li_Practical_Stereo_Matching_via_Cascaded_Recurrent_Network_With_Adaptive_Correlation_CVPR_2022_paper.pdf">Practical Stereo Matching via Cascaded Recurrent Network with Adaptive<br />Correlation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">78</td>
<td style="text-align: center;">2021-11-17</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Ristea_Self-Supervised_Predictive_Convolutional_Attentive_Block_for_Anomaly_Detection_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Ristea_Self-Supervised_Predictive_Convolutional_Attentive_Block_for_Anomaly_Detection_CVPR_2022_paper.pdf">Self-Supervised Predictive Convolutional Attentive Block for Anomaly Detection</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">77</td>
<td style="text-align: center;">2022-04-14</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Bhatnagar_BEHAVE_Dataset_and_Method_for_Tracking_Human_Object_Interactions_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Bhatnagar_BEHAVE_Dataset_and_Method_for_Tracking_Human_Object_Interactions_CVPR_2022_paper.pdf">BEHAVE: Dataset and Method for Tracking Human Object Interactions</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">77</td>
<td style="text-align: center;">2022-03-18</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Wu_Sparse_Fuse_Dense_Towards_High_Quality_3D_Detection_With_Depth_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Wu_Sparse_Fuse_Dense_Towards_High_Quality_3D_Detection_With_Depth_CVPR_2022_paper.pdf">Sparse Fuse Dense: Towards High Quality 3D Detection with<br />Depth Completion</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">76</td>
<td style="text-align: center;">2021-12-09</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Lindell_BACON_Band-Limited_Coordinate_Networks_for_Multiscale_Scene_Representation_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Lindell_BACON_Band-Limited_Coordinate_Networks_for_Multiscale_Scene_Representation_CVPR_2022_paper.pdf">Bacon: Band-limited Coordinate Networks for Multiscale Scene Representation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">76</td>
<td style="text-align: center;">2021-12-15</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Sun_Putting_People_in_Their_Place_Monocular_Regression_of_3D_People_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Sun_Putting_People_in_Their_Place_Monocular_Regression_of_3D_People_CVPR_2022_paper.pdf">Putting People in their Place: Monocular Regression of 3D<br />People in Depth</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">76</td>
<td style="text-align: center;">2021-12-08</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Fini_Self-Supervised_Models_Are_Continual_Learners_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Fini_Self-Supervised_Models_Are_Continual_Learners_CVPR_2022_paper.pdf">Self-Supervised Models are Continual Learners</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">76</td>
<td style="text-align: center;">2021-12-09</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Park_Fast_Point_Transformer_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Park_Fast_Point_Transformer_CVPR_2022_paper.pdf">Fast Point Transformer</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">75</td>
<td style="text-align: center;">2022-03-02</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Zhang_MixSTE_Seq2seq_Mixed_Spatio-Temporal_Encoder_for_3D_Human_Pose_Estimation_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Zhang_MixSTE_Seq2seq_Mixed_Spatio-Temporal_Encoder_for_3D_Human_Pose_Estimation_CVPR_2022_paper.pdf">MixSTE: Seq2seq Mixed Spatio-Temporal Encoder for 3D Human Pose<br />Estimation in Video</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">75</td>
<td style="text-align: center;">2021-12-10</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Fan_FaceFormer_Speech-Driven_3D_Facial_Animation_With_Transformers_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Fan_FaceFormer_Speech-Driven_3D_Facial_Animation_With_Transformers_CVPR_2022_paper.pdf">FaceFormer: Speech-Driven 3D Facial Animation with Transformers</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">75</td>
<td style="text-align: center;">2021-11-19</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Rebain_LOLNerf_Learn_From_One_Look_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Rebain_LOLNerf_Learn_From_One_Look_CVPR_2022_paper.pdf">LOLNeRF: Learn from One Look</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">75</td>
<td style="text-align: center;">2021-12-31</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Kim_InfoNeRF_Ray_Entropy_Minimization_for_Few-Shot_Neural_Volume_Rendering_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Kim_InfoNeRF_Ray_Entropy_Minimization_for_Few-Shot_Neural_Volume_Rendering_CVPR_2022_paper.pdf">InfoNeRF: Ray Entropy Minimization for Few-Shot Neural Volume Rendering</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">75</td>
<td style="text-align: center;">2022-04-02</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Kang_Class-Incremental_Learning_by_Knowledge_Distillation_With_Adaptive_Feature_Consolidation_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Kang_Class-Incremental_Learning_by_Knowledge_Distillation_With_Adaptive_Feature_Consolidation_CVPR_2022_paper.pdf">Class-Incremental Learning by Knowledge Distillation with Adaptive Feature Consolidation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">74</td>
<td style="text-align: center;">2022-03-14</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Zheng_SimMatch_Semi-Supervised_Learning_With_Similarity_Matching_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Zheng_SimMatch_Semi-Supervised_Learning_With_Similarity_Matching_CVPR_2022_paper.pdf">SimMatch: Semi-supervised Learning with Similarity Matching</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">74</td>
<td style="text-align: center;">2022-03-14</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Zhou_Forward_Compatible_Few-Shot_Class-Incremental_Learning_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Zhou_Forward_Compatible_Few-Shot_Class-Incremental_Learning_CVPR_2022_paper.pdf">Forward Compatible Few-Shot Class-Incremental Learning</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">74</td>
<td style="text-align: center;">2022-03-10</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Hu_Point_Density-Aware_Voxels_for_LiDAR_3D_Object_Detection_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Hu_Point_Density-Aware_Voxels_for_LiDAR_3D_Object_Detection_CVPR_2022_paper.pdf">Point Density-Aware Voxels for LiDAR 3D Object Detection</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">73</td>
<td style="text-align: center;">2022-03-22</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Gao_FedDC_Federated_Learning_With_Non-IID_Data_via_Local_Drift_Decoupling_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Gao_FedDC_Federated_Learning_With_Non-IID_Data_via_Local_Drift_Decoupling_CVPR_2022_paper.pdf">FedDC: Federated Learning with Non-IID Data via Local Drift<br />Decoupling and Correction</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">73</td>
<td style="text-align: center;">2021-06-10</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Qu_Rethinking_Architecture_Design_for_Tackling_Data_Heterogeneity_in_Federated_Learning_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Qu_Rethinking_Architecture_Design_for_Tackling_Data_Heterogeneity_in_Federated_Learning_CVPR_2022_paper.pdf">Rethinking Architecture Design for Tackling Data Heterogeneity in Federated<br />Learning</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">73</td>
<td style="text-align: center;">2022-01-13</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Li_CLIP-Event_Connecting_Text_and_Images_With_Event_Structures_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Li_CLIP-Event_Connecting_Text_and_Images_With_Event_Structures_CVPR_2022_paper.pdf">CLIP-Event: Connecting Text and Images with Event Structures</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">73</td>
<td style="text-align: center;">2021-09-08</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Li_Panoptic_SegFormer_Delving_Deeper_Into_Panoptic_Segmentation_With_Transformers_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Li_Panoptic_SegFormer_Delving_Deeper_Into_Panoptic_Segmentation_With_Transformers_CVPR_2022_paper.pdf">Panoptic SegFormer: Delving Deeper into Panoptic Segmentation with Transformers</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">73</td>
<td style="text-align: center;">2022-04-28</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Mou_Deep_Generalized_Unfolding_Networks_for_Image_Restoration_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Mou_Deep_Generalized_Unfolding_Networks_for_Image_Restoration_CVPR_2022_paper.pdf">Deep Generalized Unfolding Networks for Image Restoration</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">72</td>
<td style="text-align: center;">2021-11-19</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Xue_Advancing_High-Resolution_Video-Language_Representation_With_Large-Scale_Video_Transcriptions_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Xue_Advancing_High-Resolution_Video-Language_Representation_With_Large-Scale_Video_Transcriptions_CVPR_2022_paper.pdf">Advancing High-Resolution Video-Language Representation with Large-Scale Video Transcriptions</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">72</td>
<td style="text-align: center;">2022-06-01</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Xu_SNR-Aware_Low-Light_Image_Enhancement_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Xu_SNR-Aware_Low-Light_Image_Enhancement_CVPR_2022_paper.pdf">SNR-Aware Low-light Image Enhancement</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">72</td>
<td style="text-align: center;">2022-03-08</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Li_Selective-Supervised_Contrastive_Learning_With_Noisy_Labels_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Li_Selective-Supervised_Contrastive_Learning_With_Noisy_Labels_CVPR_2022_paper.pdf">Selective-Supervised Contrastive Learning with Noisy Labels</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">72</td>
<td style="text-align: center;">2021-12-08</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Shvetsova_Everything_at_Once_-_Multi-Modal_Fusion_Transformer_for_Video_Retrieval_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Shvetsova_Everything_at_Once_-_Multi-Modal_Fusion_Transformer_for_Video_Retrieval_CVPR_2022_paper.pdf">Everything at Once â€“ Multi-modal Fusion Transformer for Video<br />Retrieval</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">72</td>
<td style="text-align: center;">2022-06-01</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Zhou_HiVT_Hierarchical_Vector_Transformer_for_Multi-Agent_Motion_Prediction_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Zhou_HiVT_Hierarchical_Vector_Transformer_for_Multi-Agent_Motion_Prediction_CVPR_2022_paper.pdf">HiVT: Hierarchical Vector Transformer for Multi-Agent Motion Prediction</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">71</td>
<td style="text-align: center;">2022-03-24</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Siyao_Bailando_3D_Dance_Generation_by_Actor-Critic_GPT_With_Choreographic_Memory_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Siyao_Bailando_3D_Dance_Generation_by_Actor-Critic_GPT_With_Choreographic_Memory_CVPR_2022_paper.pdf">Bailando: 3D Dance Generation by Actor-Critic GPT with Choreographic<br />Memory</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">71</td>
<td style="text-align: center;">2022-03-04</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Xu_Attention_Concatenation_Volume_for_Accurate_and_Efficient_Stereo_Matching_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Xu_Attention_Concatenation_Volume_for_Accurate_and_Efficient_Stereo_Matching_CVPR_2022_paper.pdf">Attention Concatenation Volume for Accurate and Efficient Stereo Matching</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">71</td>
<td style="text-align: center;">2022-04-24</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Danecek_EMOCA_Emotion_Driven_Monocular_Face_Capture_and_Animation_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Danecek_EMOCA_Emotion_Driven_Monocular_Face_Capture_and_Animation_CVPR_2022_paper.pdf">EMOCA: Emotion Driven Monocular Face Capture and Animation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">70</td>
<td style="text-align: center;">2022-01-25</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Ramakrishnan_PONI_Potential_Functions_for_ObjectGoal_Navigation_With_Interaction-Free_Learning_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Ramakrishnan_PONI_Potential_Functions_for_ObjectGoal_Navigation_With_Interaction-Free_Learning_CVPR_2022_paper.pdf">PONI: Potential Functions for ObjectGoal Navigation with Interaction-free Learning</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">70</td>
<td style="text-align: center;">2022-03-28</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Yew_REGTR_End-to-End_Point_Cloud_Correspondences_With_Transformers_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Yew_REGTR_End-to-End_Point_Cloud_Correspondences_With_Transformers_CVPR_2022_paper.pdf">REGTR: End-to-end Point Cloud Correspondences with Transformers</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">70</td>
<td style="text-align: center;">2022-03-13</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Hong_Depth-Aware_Generative_Adversarial_Network_for_Talking_Head_Video_Generation_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Hong_Depth-Aware_Generative_Adversarial_Network_for_Talking_Head_Video_Generation_CVPR_2022_paper.pdf">Depth-Aware Generative Adversarial Network for Talking Head Video Generation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">69</td>
<td style="text-align: center;">2021-11-24</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Tang_An_Image_Patch_Is_a_Wave_Phase-Aware_Vision_MLP_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Tang_An_Image_Patch_Is_a_Wave_Phase-Aware_Vision_MLP_CVPR_2022_paper.pdf">An Image Patch is a Wave: Phase-Aware Vision MLP</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">69</td>
<td style="text-align: center;">2022-03-16</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Zou_The_Devil_Is_in_the_Details_Window-Based_Attention_for_Image_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Zou_The_Devil_Is_in_the_Details_Window-Based_Attention_for_Image_CVPR_2022_paper.pdf">The Devil Is in the Details: Window-based Attention for<br />Image Compression</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">69</td>
<td style="text-align: center;">2021-08-30</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Guo_Hire-MLP_Vision_MLP_via_Hierarchical_Rearrangement_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Guo_Hire-MLP_Vision_MLP_via_Hierarchical_Rearrangement_CVPR_2022_paper.pdf">Hire-MLP: Vision MLP via Hierarchical Rearrangement</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">69</td>
<td style="text-align: center;">2022-05-11</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Ran_Surface_Representation_for_Point_Clouds_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Ran_Surface_Representation_for_Point_Clouds_CVPR_2022_paper.pdf">Surface Representation for Point Clouds</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">69</td>
<td style="text-align: center;">2021-12-02</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Gupta_OW-DETR_Open-World_Detection_Transformer_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Gupta_OW-DETR_Open-World_Detection_Transformer_CVPR_2022_paper.pdf">OW-DETR: Open-world Detection Transformer</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">68</td>
<td style="text-align: center;">2022-05-24</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Huang_StylizedNeRF_Consistent_3D_Scene_Stylization_As_Stylized_NeRF_via_2D-3D_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Huang_StylizedNeRF_Consistent_3D_Scene_Stylization_As_Stylized_NeRF_via_2D-3D_CVPR_2022_paper.pdf">StylizedNeRF: Consistent 3D Scene Stylization as Stylized NeRF via<br />2D-3D Mutual Learning</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">68</td>
<td style="text-align: center;">2022-01-25</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Yan_ShapeFormer_Transformer-Based_Shape_Completion_via_Sparse_Representation_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Yan_ShapeFormer_Transformer-Based_Shape_Completion_via_Sparse_Representation_CVPR_2022_paper.pdf">ShapeFormer: Transformer-based Shape Completion via Sparse Representation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">68</td>
<td style="text-align: center;">2022-03-16</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Yi_Physical_Inertial_Poser_PIP_Physics-Aware_Real-Time_Human_Motion_Tracking_From_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Yi_Physical_Inertial_Poser_PIP_Physics-Aware_Real-Time_Human_Motion_Tracking_From_CVPR_2022_paper.pdf">Physical Inertial Poser (PIP): Physics-aware Real-time Human Motion Tracking<br />from Sparse Inertial Sensors</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">68</td>
<td style="text-align: center;">2021-12-09</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Hendrycks_PixMix_Dreamlike_Pictures_Comprehensively_Improve_Safety_Measures_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Hendrycks_PixMix_Dreamlike_Pictures_Comprehensively_Improve_Safety_Measures_CVPR_2022_paper.pdf">PixMix: Dreamlike Pictures Comprehensively Improve Safety Measures</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">68</td>
<td style="text-align: center;">2022-06-01</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Chi_InfoGCN_Representation_Learning_for_Human_Skeleton-Based_Action_Recognition_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Chi_InfoGCN_Representation_Learning_for_Human_Skeleton-Based_Action_Recognition_CVPR_2022_paper.pdf">InfoGCN: Representation Learning for Human Skeleton-based Action Recognition</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">68</td>
<td style="text-align: center;">2022-01-30</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Jiang_SelfRecon_Self_Reconstruction_Your_Digital_Avatar_From_Monocular_Video_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Jiang_SelfRecon_Self_Reconstruction_Your_Digital_Avatar_From_Monocular_Video_CVPR_2022_paper.pdf">SelfRecon: Self Reconstruction Your Digital Avatar from Monocular Video</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">68</td>
<td style="text-align: center;">2022-01-13</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Ge_Bridging_Video-Text_Retrieval_With_Multiple_Choice_Questions_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Ge_Bridging_Video-Text_Retrieval_With_Multiple_Choice_Questions_CVPR_2022_paper.pdf">Bridging Video-text Retrieval with Multiple Choice Questions</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">67</td>
<td style="text-align: center;">2021-12-06</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Zhao_HumanNeRF_Efficiently_Generated_Human_Radiance_Field_From_Sparse_Inputs_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Zhao_HumanNeRF_Efficiently_Generated_Human_Radiance_Field_From_Sparse_Inputs_CVPR_2022_paper.pdf">HumanNeRF: Efficiently Generated Human Radiance Field from Sparse Inputs</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">67</td>
<td style="text-align: center;">2021-12-01</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Park_The_Majority_Can_Help_the_Minority_Context-Rich_Minority_Oversampling_for_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Park_The_Majority_Can_Help_the_Minority_Context-Rich_Minority_Oversampling_for_CVPR_2022_paper.pdf">The Majority Can Help the Minority: Context-rich Minority Oversampling<br />for Long-tailed Classification</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">67</td>
<td style="text-align: center;">2022-06-01</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Zhu_Balanced_Contrastive_Learning_for_Long-Tailed_Visual_Recognition_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Zhu_Balanced_Contrastive_Learning_for_Long-Tailed_Visual_Recognition_CVPR_2022_paper.pdf">Balanced Contrastive Learning for Long-Tailed Visual Recognition</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">67</td>
<td style="text-align: center;">2022-03-23</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Chen_Self-Supervised_Learning_of_Adversarial_Example_Towards_Good_Generalizations_for_Deepfake_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Chen_Self-Supervised_Learning_of_Adversarial_Example_Towards_Good_Generalizations_for_Deepfake_CVPR_2022_paper.pdf">Self-supervised Learning of Adversarial Example: Towards Good Generalizations for<br />Deepfake Detection</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">67</td>
<td style="text-align: center;">2021-04-13</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Cheng_Pointly-Supervised_Instance_Segmentation_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Cheng_Pointly-Supervised_Instance_Segmentation_CVPR_2022_paper.pdf">Pointly-Supervised Instance Segmentation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">67</td>
<td style="text-align: center;">2022-03-02</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Dong_Incremental_Transformer_Structure_Enhanced_Image_Inpainting_With_Masking_Positional_Encoding_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Dong_Incremental_Transformer_Structure_Enhanced_Image_Inpainting_With_Masking_Positional_Encoding_CVPR_2022_paper.pdf">Incremental Transformer Structure Enhanced Image Inpainting with Masking Positional<br />Encoding</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">67</td>
<td style="text-align: center;">2021-12-17</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Darmon_Improving_Neural_Implicit_Surfaces_Geometry_With_Patch_Warping_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Darmon_Improving_Neural_Implicit_Surfaces_Geometry_With_Patch_Warping_CVPR_2022_paper.pdf">Improving neural implicit surfaces geometry with patch warping</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">66</td>
<td style="text-align: center;">2021-12-01</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Cao_MonoScene_Monocular_3D_Semantic_Scene_Completion_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Cao_MonoScene_Monocular_3D_Semantic_Scene_Completion_CVPR_2022_paper.pdf">MonoScene: Monocular 3D Semantic Scene Completion</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">66</td>
<td style="text-align: center;">2022-03-26</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Chen_Knowledge_Distillation_With_the_Reused_Teacher_Classifier_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Chen_Knowledge_Distillation_With_the_Reused_Teacher_Classifier_CVPR_2022_paper.pdf">Knowledge Distillation with the Reused Teacher Classifier</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">66</td>
<td style="text-align: center;">2022-03-28</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Gorti_X-Pool_Cross-Modal_Language-Video_Attention_for_Text-Video_Retrieval_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Gorti_X-Pool_Cross-Modal_Language-Video_Attention_for_Text-Video_Retrieval_CVPR_2022_paper.pdf">X-Pool: Cross-Modal Language-Video Attention for Text-Video Retrieval</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">66</td>
<td style="text-align: center;">2022-06-01</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Huang_Learn_From_Others_and_Be_Yourself_in_Heterogeneous_Federated_Learning_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Huang_Learn_From_Others_and_Be_Yourself_in_Heterogeneous_Federated_Learning_CVPR_2022_paper.pdf">Learn from Others and Be Yourself in Heterogeneous Federated<br />Learning</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">66</td>
<td style="text-align: center;">2022-05-29</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Kong_IFRNet_Intermediate_Feature_Refine_Network_for_Efficient_Frame_Interpolation_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Kong_IFRNet_Intermediate_Feature_Refine_Network_for_Efficient_Frame_Interpolation_CVPR_2022_paper.pdf">IFRNet: Intermediate Feature Refine Network for Efficient Frame Interpolation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">66</td>
<td style="text-align: center;">2022-06-01</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Buch_Revisiting_the_Video_in_Video-Language_Understanding_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Buch_Revisiting_the_Video_in_Video-Language_Understanding_CVPR_2022_paper.pdf">Revisiting the â€œVideoâ€ in Video-Language Understanding</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">66</td>
<td style="text-align: center;">2021-11-29</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Ding_TransMVSNet_Global_Context-Aware_Multi-View_Stereo_Network_With_Transformers_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Ding_TransMVSNet_Global_Context-Aware_Multi-View_Stereo_Network_With_Transformers_CVPR_2022_paper.pdf">TransMVSNet: Global Context-aware Multi-view Stereo Network with Transformers</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">66</td>
<td style="text-align: center;">2021-11-30</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Guo_NeRFReN_Neural_Radiance_Fields_With_Reflections_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Guo_NeRFReN_Neural_Radiance_Fields_With_Reflections_CVPR_2022_paper.pdf">NeRFReN: Neural Radiance Fields with Reflections</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">65</td>
<td style="text-align: center;">2021-12-09</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Wei_HairCLIP_Design_Your_Hair_by_Text_and_Reference_Image_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Wei_HairCLIP_Design_Your_Hair_by_Text_and_Reference_Image_CVPR_2022_paper.pdf">HairCLIP: Design Your Hair by Text and Reference Image</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">65</td>
<td style="text-align: center;">2022-06-01</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Cao_End-to-End_Reconstruction-Classification_Learning_for_Face_Forgery_Detection_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Cao_End-to-End_Reconstruction-Classification_Learning_for_Face_Forgery_Detection_CVPR_2022_paper.pdf">End-to-End Reconstruction-Classification Learning for Face Forgery Detection</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">65</td>
<td style="text-align: center;">2022-03-24</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Zhou_Global_Tracking_Transformers_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Zhou_Global_Tracking_Transformers_CVPR_2022_paper.pdf">Global Tracking Transformers</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">65</td>
<td style="text-align: center;">2022-03-31</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Cai_MeMOT_Multi-Object_Tracking_With_Memory_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Cai_MeMOT_Multi-Object_Tracking_With_Memory_CVPR_2022_paper.pdf">MeMOT: Multi-Object Tracking with Memory</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">65</td>
<td style="text-align: center;">2022-01-20</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Singh_Revisiting_Weakly_Supervised_Pre-Training_of_Visual_Perception_Models_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Singh_Revisiting_Weakly_Supervised_Pre-Training_of_Visual_Perception_Models_CVPR_2022_paper.pdf">Revisiting Weakly Supervised Pre-Training of Visual Perception Models</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">65</td>
<td style="text-align: center;">2022-03-28</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Zheng_Structured_Local_Radiance_Fields_for_Human_Avatar_Modeling_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Zheng_Structured_Local_Radiance_Fields_for_Human_Avatar_Modeling_CVPR_2022_paper.pdf">Structured Local Radiance Fields for Human Avatar Modeling</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">65</td>
<td style="text-align: center;">2022-03-31</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Kim_ReSTR_Convolution-Free_Referring_Image_Segmentation_Using_Transformers_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Kim_ReSTR_Convolution-Free_Referring_Image_Segmentation_Using_Transformers_CVPR_2022_paper.pdf">ReSTR: Convolution-free Referring Image Segmentation Using Transformers</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">65</td>
<td style="text-align: center;">2021-06-08</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Bar_DETReg_Unsupervised_Pretraining_With_Region_Priors_for_Object_Detection_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Bar_DETReg_Unsupervised_Pretraining_With_Region_Priors_for_Object_Detection_CVPR_2022_paper.pdf">DETReg: Unsupervised Pretraining with Region Priors for Object Detection</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">64</td>
<td style="text-align: center;">2022-06-01</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Hou_Point-to-Voxel_Knowledge_Distillation_for_LiDAR_Semantic_Segmentation_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Hou_Point-to-Voxel_Knowledge_Distillation_for_LiDAR_Semantic_Segmentation_CVPR_2022_paper.pdf">Point-to-Voxel Knowledge Distillation for LiDAR Semantic Segmentation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">63</td>
<td style="text-align: center;">2022-06-01</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Zhang_Spiking_Transformers_for_Event-Based_Single_Object_Tracking_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Zhang_Spiking_Transformers_for_Event-Based_Single_Object_Tracking_CVPR_2022_paper.pdf">Spiking Transformers for Event-based Single Object Tracking</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">63</td>
<td style="text-align: center;">2022-01-13</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Zhang_On_Adversarial_Robustness_of_Trajectory_Prediction_for_Autonomous_Vehicles_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Zhang_On_Adversarial_Robustness_of_Trajectory_Prediction_for_Autonomous_Vehicles_CVPR_2022_paper.pdf">On Adversarial Robustness of Trajectory Prediction for Autonomous Vehicles</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">63</td>
<td style="text-align: center;">2022-03-28</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Cho_Part-Based_Pseudo_Label_Refinement_for_Unsupervised_Person_Re-Identification_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Cho_Part-Based_Pseudo_Label_Refinement_for_Unsupervised_Person_Re-Identification_CVPR_2022_paper.pdf">Part-based Pseudo Label Refinement for Unsupervised Person Re-identification</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">63</td>
<td style="text-align: center;">2022-04-19</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Alldieck_Photorealistic_Monocular_3D_Reconstruction_of_Humans_Wearing_Clothing_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Alldieck_Photorealistic_Monocular_3D_Reconstruction_of_Humans_Wearing_Clothing_CVPR_2022_paper.pdf">Photorealistic Monocular 3D Reconstruction of Humans Wearing Clothing</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">63</td>
<td style="text-align: center;">2022-03-19</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Zheng_CLRNet_Cross_Layer_Refinement_Network_for_Lane_Detection_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Zheng_CLRNet_Cross_Layer_Refinement_Network_for_Lane_Detection_CVPR_2022_paper.pdf">CLRNet: Cross Layer Refinement Network for Lane Detection</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">63</td>
<td style="text-align: center;">2021-02-25</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Zhou_Simple_Multi-Dataset_Detection_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Zhou_Simple_Multi-Dataset_Detection_CVPR_2022_paper.pdf">Simple Multi-dataset Detection</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">63</td>
<td style="text-align: center;">2021-11-29</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Botach_End-to-End_Referring_Video_Object_Segmentation_With_Multimodal_Transformers_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Botach_End-to-End_Referring_Video_Object_Segmentation_With_Multimodal_Transformers_CVPR_2022_paper.pdf">End-to-End Referring Video Object Segmentation with Multimodal Transformers</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">63</td>
<td style="text-align: center;">2022-05-24</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Sun_OnePose_One-Shot_Object_Pose_Estimation_Without_CAD_Models_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Sun_OnePose_One-Shot_Object_Pose_Estimation_Without_CAD_Models_CVPR_2022_paper.pdf">OnePose: One-Shot Object Pose Estimation without CAD Models</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">63</td>
<td style="text-align: center;">2022-03-28</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Xue_GIRAFFE_HD_A_High-Resolution_3D-Aware_Generative_Model_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Xue_GIRAFFE_HD_A_High-Resolution_3D-Aware_Generative_Model_CVPR_2022_paper.pdf">GIRAFFE HD: A High-Resolution 3D-aware Generative Model</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">63</td>
<td style="text-align: center;">2021-12-23</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Biten_LaTr_Layout-Aware_Transformer_for_Scene-Text_VQA_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Biten_LaTr_Layout-Aware_Transformer_for_Scene-Text_VQA_CVPR_2022_paper.pdf">LaTr: Layout-Aware Transformer for Scene-Text VQA</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">62</td>
<td style="text-align: center;">2022-03-08</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Zhong_Shadows_Can_Be_Dangerous_Stealthy_and_Effective_Physical-World_Adversarial_Attack_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Zhong_Shadows_Can_Be_Dangerous_Stealthy_and_Effective_Physical-World_Adversarial_Attack_CVPR_2022_paper.pdf">Shadows can be Dangerous: Stealthy and Effective Physical-world Adversarial<br />Attack by Natural Phenomenon</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">62</td>
<td style="text-align: center;">2022-03-05</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Liu_Towards_Efficient_and_Scalable_Sharpness-Aware_Minimization_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Liu_Towards_Efficient_and_Scalable_Sharpness-Aware_Minimization_CVPR_2022_paper.pdf">Towards Efficient and Scalable Sharpness-Aware Minimization</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">62</td>
<td style="text-align: center;">2022-06-01</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Athar_RigNeRF_Fully_Controllable_Neural_3D_Portraits_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Athar_RigNeRF_Fully_Controllable_Neural_3D_Portraits_CVPR_2022_paper.pdf">RigNeRF: Fully Controllable Neural 3D Portraits</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">62</td>
<td style="text-align: center;">2022-04-06</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Li_Towards_an_End-to-End_Framework_for_Flow-Guided_Video_Inpainting_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Li_Towards_an_End-to-End_Framework_for_Flow-Guided_Video_Inpainting_CVPR_2022_paper.pdf">Towards An End-to-End Framework for Flow-Guided Video Inpainting</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">62</td>
<td style="text-align: center;">2022-02-23</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Chen_Think_Global_Act_Local_Dual-Scale_Graph_Transformer_for_Vision-and-Language_Navigation_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Chen_Think_Global_Act_Local_Dual-Scale_Graph_Transformer_for_Vision-and-Language_Navigation_CVPR_2022_paper.pdf">Think Global, Act Local: Dual-scale Graph Transformer for Vision-and-Language<br />Navigation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">62</td>
<td style="text-align: center;">2022-04-06</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Zheng_Gait_Recognition_in_the_Wild_With_Dense_3D_Representations_and_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Zheng_Gait_Recognition_in_the_Wild_With_Dense_3D_Representations_and_CVPR_2022_paper.pdf">Gait Recognition in the Wild with Dense 3D Representations<br />and A Benchmark</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">62</td>
<td style="text-align: center;">2021-11-29</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Huang_HDR-NeRF_High_Dynamic_Range_Neural_Radiance_Fields_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Huang_HDR-NeRF_High_Dynamic_Range_Neural_Radiance_Fields_CVPR_2022_paper.pdf">HDR-NeRF: High Dynamic Range Neural Radiance Fields</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">61</td>
<td style="text-align: center;">2022-02-07</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Peng_Crafting_Better_Contrastive_Views_for_Siamese_Representation_Learning_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Peng_Crafting_Better_Contrastive_Views_for_Siamese_Representation_Learning_CVPR_2022_paper.pdf">Crafting Better Contrastive Views for Siamese Representation Learning</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">61</td>
<td style="text-align: center;">2022-03-27</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Li_Deep_Hierarchical_Semantic_Segmentation_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Li_Deep_Hierarchical_Semantic_Segmentation_CVPR_2022_paper.pdf">Deep Hierarchical Semantic Segmentation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">61</td>
<td style="text-align: center;">2022-03-30</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Hersche_Constrained_Few-Shot_Class-Incremental_Learning_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Hersche_Constrained_Few-Shot_Class-Incremental_Learning_CVPR_2022_paper.pdf">Constrained Few-shot Class-incremental Learning</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">61</td>
<td style="text-align: center;">2021-11-27</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Zhang_Towards_Principled_Disentanglement_for_Domain_Generalization_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Zhang_Towards_Principled_Disentanglement_for_Domain_Generalization_CVPR_2022_paper.pdf">Towards Principled Disentanglement for Domain Generalization</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">60</td>
<td style="text-align: center;">2021-12-20</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Yang_Lite_Vision_Transformer_With_Enhanced_Self-Attention_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Yang_Lite_Vision_Transformer_With_Enhanced_Self-Attention_CVPR_2022_paper.pdf">Lite Vision Transformer with Enhanced Self-Attention</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">60</td>
<td style="text-align: center;">2021-09-07</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Chen_Unpaired_Deep_Image_Deraining_Using_Dual_Contrastive_Learning_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Chen_Unpaired_Deep_Image_Deraining_Using_Dual_Contrastive_Learning_CVPR_2022_paper.pdf">Unpaired Deep Image Deraining Using Dual Contrastive Learning</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">60</td>
<td style="text-align: center;">2020-08-26</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Agarwal_Estimating_Example_Difficulty_Using_Variance_of_Gradients_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Agarwal_Estimating_Example_Difficulty_Using_Variance_of_Gradients_CVPR_2022_paper.pdf">Estimating Example Difficulty using Variance of Gradients</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">60</td>
<td style="text-align: center;">2021-11-25</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Li_Cross-Domain_Adaptive_Teacher_for_Object_Detection_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Li_Cross-Domain_Adaptive_Teacher_for_Object_Detection_CVPR_2022_paper.pdf">Cross-Domain Adaptive Teacher for Object Detection</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">60</td>
<td style="text-align: center;">2022-03-13</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Jia_LAS-AT_Adversarial_Training_With_Learnable_Attack_Strategy_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Jia_LAS-AT_Adversarial_Training_With_Learnable_Attack_Strategy_CVPR_2022_paper.pdf">LAS-AT: Adversarial Training with Learnable Attack Strategy</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">60</td>
<td style="text-align: center;">2022-04-09</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Xie_Joint_Distribution_Matters_Deep_Brownian_Distance_Covariance_for_Few-Shot_Classification_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Xie_Joint_Distribution_Matters_Deep_Brownian_Distance_Covariance_for_Few-Shot_Classification_CVPR_2022_paper.pdf">Joint Distribution Matters: Deep Brownian Distance Covariance for Few-Shot<br />Classification</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">60</td>
<td style="text-align: center;">2022-04-14</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Zhang_Modeling_Indirect_Illumination_for_Inverse_Rendering_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Zhang_Modeling_Indirect_Illumination_for_Inverse_Rendering_CVPR_2022_paper.pdf">Modeling Indirect Illumination for Inverse Rendering</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">60</td>
<td style="text-align: center;">2021-04-29</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Hampali_Keypoint_Transformer_Solving_Joint_Identification_in_Challenging_Hands_and_Object_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Hampali_Keypoint_Transformer_Solving_Joint_Identification_in_Challenging_Hands_and_Object_CVPR_2022_paper.pdf">Keypoint Transformer: Solving Joint Identification in Challenging Hands and<br />Object Interactions for Accurate 3D Pose Estimation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">60</td>
<td style="text-align: center;">2022-01-03</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Wu_Language_As_Queries_for_Referring_Video_Object_Segmentation_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Wu_Language_As_Queries_for_Referring_Video_Object_Segmentation_CVPR_2022_paper.pdf">Language as Queries for Referring Video Object Segmentation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">59</td>
<td style="text-align: center;">2021-11-29</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Ma_Deblur-NeRF_Neural_Radiance_Fields_From_Blurry_Images_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Ma_Deblur-NeRF_Neural_Radiance_Fields_From_Blurry_Images_CVPR_2022_paper.pdf">Deblur-NeRF: Neural Radiance Fields from Blurry Images</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">59</td>
<td style="text-align: center;">2022-03-28</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Liu_Towards_Implicit_Text-Guided_3D_Shape_Generation_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Liu_Towards_Implicit_Text-Guided_3D_Shape_Generation_CVPR_2022_paper.pdf">Towards Implicit Text-Guided 3D Shape Generation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">59</td>
<td style="text-align: center;">2021-11-26</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Ke_Mask_Transfiner_for_High-Quality_Instance_Segmentation_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Ke_Mask_Transfiner_for_High-Quality_Instance_Segmentation_CVPR_2022_paper.pdf">Mask Transfiner for High-Quality Instance Segmentation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">59</td>
<td style="text-align: center;">2022-03-17</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Su_ZebraPose_Coarse_To_Fine_Surface_Encoding_for_6DoF_Object_Pose_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Su_ZebraPose_Coarse_To_Fine_Surface_Encoding_for_6DoF_Object_Pose_CVPR_2022_paper.pdf">ZebraPose: Coarse to Fine Surface Encoding for 6DoF Object<br />Pose Estimation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">59</td>
<td style="text-align: center;">2022-06-01</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Liang_Expressive_Talking_Head_Generation_With_Granular_Audio-Visual_Control_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Liang_Expressive_Talking_Head_Generation_With_Granular_Audio-Visual_Control_CVPR_2022_paper.pdf">Expressive Talking Head Generation with Granular Audio-Visual Control</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">59</td>
<td style="text-align: center;">2022-03-10</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Wang_Domain_Generalization_via_Shuffled_Style_Assembly_for_Face_Anti-Spoofing_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Wang_Domain_Generalization_via_Shuffled_Style_Assembly_for_Face_Anti-Spoofing_CVPR_2022_paper.pdf">Domain Generalization via Shuffled Style Assembly for Face Anti-Spoofing</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">59</td>
<td style="text-align: center;">2022-04-05</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Zhang_IRON_Inverse_Rendering_by_Optimizing_Neural_SDFs_and_Materials_From_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Zhang_IRON_Inverse_Rendering_by_Optimizing_Neural_SDFs_and_Materials_From_CVPR_2022_paper.pdf">IRON: Inverse Rendering by Optimizing Neural SDFs and Materials<br />from Photometric Images</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">59</td>
<td style="text-align: center;">2022-03-21</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Huang_MonoDTR_Monocular_3D_Object_Detection_With_Depth-Aware_Transformer_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Huang_MonoDTR_Monocular_3D_Object_Detection_With_Depth-Aware_Transformer_CVPR_2022_paper.pdf">MonoDTR: Monocular 3D Object Detection with Depth-Aware Transformer</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">59</td>
<td style="text-align: center;">2021-12-02</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Yuan_GLAMR_Global_Occlusion-Aware_Human_Mesh_Recovery_With_Dynamic_Cameras_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Yuan_GLAMR_Global_Occlusion-Aware_Human_Mesh_Recovery_With_Dynamic_Cameras_CVPR_2022_paper.pdf">GLAMR: Global Occlusion-Aware Human Mesh Recovery with Dynamic Cameras</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">59</td>
<td style="text-align: center;">2022-05-01</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Meng_Training_High-Performance_Low-Latency_Spiking_Neural_Networks_by_Differentiation_on_Spike_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Meng_Training_High-Performance_Low-Latency_Spiking_Neural_Networks_by_Differentiation_on_Spike_CVPR_2022_paper.pdf">Training High-Performance Low-Latency Spiking Neural Networks by Differentiation on<br />Spike Representation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">59</td>
<td style="text-align: center;">2022-03-31</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Schaefer_AEGNN_Asynchronous_Event-Based_Graph_Neural_Networks_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Schaefer_AEGNN_Asynchronous_Event-Based_Graph_Neural_Networks_CVPR_2022_paper.pdf">AEGNN: Asynchronous Event-based Graph Neural Networks</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">59</td>
<td style="text-align: center;">2022-03-24</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Chen_EPro-PnP_Generalized_End-to-End_Probabilistic_Perspective-N-Points_for_Monocular_Object_Pose_Estimation_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Chen_EPro-PnP_Generalized_End-to-End_Probabilistic_Perspective-N-Points_for_Monocular_Object_Pose_Estimation_CVPR_2022_paper.pdf">EPro-PnP: Generalized End-to-End Probabilistic Perspective-n-Points for Monocular Object Pose<br />Estimation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">58</td>
<td style="text-align: center;">2022-04-14</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Yang_Cross-Image_Relational_Knowledge_Distillation_for_Semantic_Segmentation_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Yang_Cross-Image_Relational_Knowledge_Distillation_for_Semantic_Segmentation_CVPR_2022_paper.pdf">Cross-Image Relational Knowledge Distillation for Semantic Segmentation</a></td>
<td style="text-align: left;"></td>
</tr>
</tbody>
</table>
              
            </div>
          </div><footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="Footer Navigation">
        <a href="../../ACL/ACL_2023/" class="btn btn-neutral float-left" title="ACL 2023"><span class="icon icon-circle-arrow-left"></span> Previous</a>
        <a href="../CVPR_2023/" class="btn btn-neutral float-right" title="CVPR 2023">Next <span class="icon icon-circle-arrow-right"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
          
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="Versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    
    
      <span><a href="../../ACL/ACL_2023/" style="color: #fcfcfc">&laquo; Previous</a></span>
    
    
      <span><a href="../CVPR_2023/" style="color: #fcfcfc">Next &raquo;</a></span>
    
  </span>
</div>
    <script src="../../js/jquery-3.6.0.min.js"></script>
    <script>var base_url = "../..";</script>
    <script src="../../js/theme_extra.js"></script>
    <script src="../../js/theme.js"></script>
      <script src="../../search/main.js"></script>
    <script>
        jQuery(function () {
            SphinxRtdTheme.Navigation.enable(true);
        });
    </script>

</body>
</html>
