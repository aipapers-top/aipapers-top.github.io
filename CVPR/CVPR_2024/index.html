<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
      <link rel="shortcut icon" href="../../img/favicon.ico" />
    <title>CVPR 2024 - AI Papers</title>
    <link rel="stylesheet" href="../../css/theme.css" />
    <link rel="stylesheet" href="../../css/theme_extra.css" />
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/github.min.css" />
        <link href="../../mytheme.css" rel="stylesheet" />
    
      <script>
        // Current page data
        var mkdocs_page_name = "CVPR 2024";
        var mkdocs_page_input_path = "CVPR/CVPR_2024.md";
        var mkdocs_page_url = null;
      </script>
    
    <!--[if lt IE 9]>
      <script src="../../js/html5shiv.min.js"></script>
    <![endif]-->
      <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
      <script>hljs.highlightAll();</script> 
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
          <a href="../.." class="icon icon-home"> AI Papers
        </a><div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../../search.html" method="get">
      <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../..">AI Papers</a>
                </li>
              </ul>
              <p class="caption"><span class="caption-text">ACL</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../ACL/ACL_2024/">ACL 2024</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">COLM</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../COLM/COLM_2024/">COLM 2024</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">CVPR</span></p>
              <ul class="current">
                  <li class="toctree-l1 current"><a class="reference internal current" href="#">CVPR 2024</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">EMNLP</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../EMNLP/EMNLP_2024/">EMNLP 2024</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">ICLR</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../ICLR/ICLR_2024/">ICLR 2024</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../ICLR/ICLR_2025/">ICLR 2025</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">ICML</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../ICML/ICML_2023/">ICML 2023</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../ICML/ICML_2024/">ICML 2024</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">NIPS</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../NIPS/NIPS_2023/">NIPS 2023</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../NIPS/NIPS_2024/">NIPS 2024</a>
                  </li>
              </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      <nav class="wy-nav-top" role="navigation" aria-label="Mobile navigation menu">
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../..">AI Papers</a>
        
      </nav>
      <div class="wy-nav-content">
        <div class="rst-content"><div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../.." class="icon icon-home" aria-label="Docs"></a></li>
          <li class="breadcrumb-item">CVPR</li>
      <li class="breadcrumb-item active">CVPR 2024</li>
    <li class="wy-breadcrumbs-aside">
    </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
            <div class="section" itemprop="articleBody">
              
                <p>Last updated: 2025-03-08 05:52:26. Maintained by <a href="https://wayson-ust.github.io/">Weisen Jiang</a>.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">citation</th>
<th style="text-align: center;">publish date</th>
<th style="text-align: center;">title (pdf)</th>
<th style="text-align: left;">review</th>
<th style="text-align: left;">authors</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">1862</td>
<td style="text-align: center;">2023-10-05</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Liu_Improved_Baselines_with_Visual_Instruction_Tuning_CVPR_2024_paper.pdf">Improved Baselines with Visual Instruction Tuning</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Liu_Improved_Baselines_with_Visual_Instruction_Tuning_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">533</td>
<td style="text-align: center;">2023-11-27</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Yue_MMMU_A_Massive_Multi-discipline_Multimodal_Understanding_and_Reasoning_Benchmark_for_CVPR_2024_paper.pdf">MMMU: A Massive Multi-discipline Multimodal Understanding and Reasoning Benchmark<br />for Expert AGI</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Yue_MMMU_A_Massive_Multi-discipline_Multimodal_Understanding_and_Reasoning_Benchmark_for_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">527</td>
<td style="text-align: center;">2023-04-17</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Zhao_DETRs_Beat_YOLOs_on_Real-time_Object_Detection_CVPR_2024_paper.pdf">DETRs Beat YOLOs on Real-time Object Detection</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Zhao_DETRs_Beat_YOLOs_on_Real-time_Object_Detection_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">460</td>
<td style="text-align: center;">2024-01-19</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Yang_Depth_Anything_Unleashing_the_Power_of_Large-Scale_Unlabeled_Data_CVPR_2024_paper.pdf">Depth Anything: Unleashing the Power of Large-Scale Unlabeled Data</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Yang_Depth_Anything_Unleashing_the_Power_of_Large-Scale_Unlabeled_Data_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">409</td>
<td style="text-align: center;">2023-10-12</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Wu_4D_Gaussian_Splatting_for_Real-Time_Dynamic_Scene_Rendering_CVPR_2024_paper.pdf">4D Gaussian Splatting for Real-Time Dynamic Scene Rendering</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Wu_4D_Gaussian_Splatting_for_Real-Time_Dynamic_Scene_Rendering_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">312</td>
<td style="text-align: center;">2023-11-07</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Ye_mPLUG-Owl2_Revolutionizing_Multi-modal_Large_Language_Model_with_Modality_Collaboration_CVPR_2024_paper.pdf">mPLUG-Owl2: Revolutionizing Multi-modal Large Language Model with Modality Collaboration</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Ye_mPLUG-Owl2_Revolutionizing_Multi-modal_Large_Language_Model_with_Modality_Collaboration_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">302</td>
<td style="text-align: center;">2023-10-23</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Long_Wonder3D_Single_Image_to_3D_using_Cross-Domain_Diffusion_CVPR_2024_paper.pdf">Wonder3D: Single Image to 3D using Cross-Domain Diffusion</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Long_Wonder3D_Single_Image_to_3D_using_Cross-Domain_Diffusion_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">278</td>
<td style="text-align: center;">2023-08-01</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Lai_LISA_Reasoning_Segmentation_via_Large_Language_Model_CVPR_2024_paper.pdf">LISA: Reasoning Segmentation via Large Language Model</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Lai_LISA_Reasoning_Segmentation_via_Large_Language_Model_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">260</td>
<td style="text-align: center;">2023-09-22</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Yang_Deformable_3D_Gaussians_for_High-Fidelity_Monocular_Dynamic_Scene_Reconstruction_CVPR_2024_paper.pdf">Deformable 3D Gaussians for High-Fidelity Monocular Dynamic Scene Reconstruction</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Yang_Deformable_3D_Gaussians_for_High-Fidelity_Monocular_Dynamic_Scene_Reconstruction_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">249</td>
<td style="text-align: center;">2023-11-21</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Guedon_SuGaR_Surface-Aligned_Gaussian_Splatting_for_Efficient_3D_Mesh_Reconstruction_and_CVPR_2024_paper.pdf">SuGaR: Surface-Aligned Gaussian Splatting for Efficient 3D Mesh Reconstruction<br />and High-Quality Mesh Rendering</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Guedon_SuGaR_Surface-Aligned_Gaussian_Splatting_for_Efficient_3D_Mesh_Reconstruction_and_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">242</td>
<td style="text-align: center;">2023-11-28</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Li_MVBench_A_Comprehensive_Multi-modal_Video_Understanding_Benchmark_CVPR_2024_paper.pdf">MVBench: A Comprehensive Multi-modal Video Understanding Benchmark</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Li_MVBench_A_Comprehensive_Multi-modal_Video_Understanding_Benchmark_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">237</td>
<td style="text-align: center;">2023-12-14</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Hong_CogAgent_A_Visual_Language_Model_for_GUI_Agents_CVPR_2024_paper.pdf">CogAgent: A Visual Language Model for GUI Agents</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Hong_CogAgent_A_Visual_Language_Model_for_GUI_Agents_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">234</td>
<td style="text-align: center;">2023-12-12</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Lin_VILA_On_Pre-training_for_Visual_Language_Models_CVPR_2024_paper.pdf">VILA: On Pre-training for Visual Language Models</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Lin_VILA_On_Pre-training_for_Visual_Language_Models_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">228</td>
<td style="text-align: center;">2023-11-28</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Hu_Animate_Anyone_Consistent_and_Controllable_Image-to-Video_Synthesis_for_Character_Animation_CVPR_2024_paper.pdf">Animate Anyone: Consistent and Controllable Image-to-Video Synthesis for Character<br />Animation</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Hu_Animate_Anyone_Consistent_and_Controllable_Image-to-Video_Synthesis_for_Character_Animation_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">226</td>
<td style="text-align: center;">2023-04-06</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Shi_InstantBooth_Personalized_Text-to-Image_Generation_without_Test-Time_Finetuning_CVPR_2024_paper.pdf">InstantBooth: Personalized Text-to-Image Generation without Test-Time Finetuning</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Shi_InstantBooth_Personalized_Text-to-Image_Generation_without_Test-Time_Finetuning_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">207</td>
<td style="text-align: center;">2023-11-27</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Yu_Mip-Splatting_Alias-free_3D_Gaussian_Splatting_CVPR_2024_paper.pdf">Mip-Splatting: Alias-free 3D Gaussian Splatting</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Yu_Mip-Splatting_Alias-free_3D_Gaussian_Splatting_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">201</td>
<td style="text-align: center;">2024-01-11</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Tong_Eyes_Wide_Shut_Exploring_the_Visual_Shortcomings_of_Multimodal_LLMs_CVPR_2024_paper.pdf">Eyes Wide Shut? Exploring the Visual Shortcomings of Multimodal<br />LLMs</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Tong_Eyes_Wide_Shut_Exploring_the_Visual_Shortcomings_of_Multimodal_LLMs_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">194</td>
<td style="text-align: center;">2023-11-29</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Huang_VBench_Comprehensive_Benchmark_Suite_for_Video_Generative_Models_CVPR_2024_paper.pdf">VBench: Comprehensive Benchmark Suite for Video Generative Models</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Huang_VBench_Comprehensive_Benchmark_Suite_for_Video_Generative_Models_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">193</td>
<td style="text-align: center;">2023-07-18</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Chen_AnyDoor_Zero-shot_Object-level_Image_Customization_CVPR_2024_paper.pdf">AnyDoor: Zero-shot Object-level Image Customization</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Chen_AnyDoor_Zero-shot_Object-level_Image_Customization_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">189</td>
<td style="text-align: center;">2023-11-11</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Li_Monkey_Image_Resolution_and_Text_Label_Are_Important_Things_for_CVPR_2024_paper.pdf">Monkey: Image Resolution and Text Label Are Important Things<br />for Large Multi-modal Models</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Li_Monkey_Image_Resolution_and_Text_Label_Are_Important_Things_for_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">188</td>
<td style="text-align: center;">2023-12-20</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Sun_Generative_Multimodal_Models_are_In-Context_Learners_CVPR_2024_paper.pdf">Generative Multimodal Models are In-Context Learners</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Sun_Generative_Multimodal_Models_are_In-Context_Learners_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">188</td>
<td style="text-align: center;">2023-12-04</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Keetha_SplaTAM_Splat_Track__Map_3D_Gaussians_for_Dense_RGB-D_CVPR_2024_paper.pdf">SplaTAM: Splat Track &amp; Map 3D Gaussians for Dense<br />RGB-D SLAM</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Keetha_SplaTAM_Splat_Track__Map_3D_Gaussians_for_Dense_RGB-D_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">179</td>
<td style="text-align: center;">2023-09-28</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Chen_Text-to-3D_using_Gaussian_Splatting_CVPR_2024_paper.pdf">Text-to-3D using Gaussian Splatting</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Chen_Text-to-3D_using_Gaussian_Splatting_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">179</td>
<td style="text-align: center;">2024-01-17</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Chen_VideoCrafter2_Overcoming_Data_Limitations_for_High-Quality_Video_Diffusion_Models_CVPR_2024_paper.pdf">VideoCrafter2: Overcoming Data Limitations for High-Quality Video Diffusion Models</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Chen_VideoCrafter2_Overcoming_Data_Limitations_for_High-Quality_Video_Diffusion_Models_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">174</td>
<td style="text-align: center;">2023-12-21</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Wang_DUSt3R_Geometric_3D_Vision_Made_Easy_CVPR_2024_paper.pdf">DUSt3R: Geometric 3D Vision Made Easy</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Wang_DUSt3R_Geometric_3D_Vision_Made_Easy_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">172</td>
<td style="text-align: center;">2023-11-30</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Lu_Scaffold-GS_Structured_3D_Gaussians_for_View-Adaptive_Rendering_CVPR_2024_paper.pdf">Scaffold-GS: Structured 3D Gaussians for View-Adaptive Rendering</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Lu_Scaffold-GS_Structured_3D_Gaussians_for_View-Adaptive_Rendering_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">171</td>
<td style="text-align: center;">2023-12-11</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Matsuki_Gaussian_Splatting_SLAM_CVPR_2024_paper.pdf">Gaussian Splatting SLAM</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Matsuki_Gaussian_Splatting_SLAM_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">163</td>
<td style="text-align: center;">2023-07-31</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Song_MovieChat_From_Dense_Token_to_Sparse_Memory_for_Long_Video_CVPR_2024_paper.pdf">MovieChat: From Dense Token to Sparse Memory for Long<br />Video Understanding</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Song_MovieChat_From_Dense_Token_to_Sparse_Memory_for_Long_Video_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">163</td>
<td style="text-align: center;">2023-03-08</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Liu_Video-P2P_Video_Editing_with_Cross-attention_Control_CVPR_2024_paper.pdf">Video-P2P: Video Editing with Cross-attention Control</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Liu_Video-P2P_Video_Editing_with_Cross-attention_Control_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">159</td>
<td style="text-align: center;">2024-01-30</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Cheng_YOLO-World_Real-Time_Open-Vocabulary_Object_Detection_CVPR_2024_paper.pdf">YOLO-World: Real-Time Open-Vocabulary Object Detection</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Cheng_YOLO-World_Real-Time_Open-Vocabulary_Object_Detection_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">152</td>
<td style="text-align: center;">2023-12-19</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Charatan_pixelSplat_3D_Gaussian_Splats_from_Image_Pairs_for_Scalable_Generalizable_CVPR_2024_paper.pdf">pixelSplat: 3D Gaussian Splats from Image Pairs for Scalable<br />Generalizable 3D Reconstruction</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Charatan_pixelSplat_3D_Gaussian_Splats_from_Image_Pairs_for_Scalable_Generalizable_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">148</td>
<td style="text-align: center;">2023-06-26</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Shi_DragDiffusion_Harnessing_Diffusion_Models_for_Interactive_Point-based_Image_Editing_CVPR_2024_paper.pdf">DragDiffusion: Harnessing Diffusion Models for Interactive Point-based Image Editing</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Shi_DragDiffusion_Harnessing_Diffusion_Models_for_Interactive_Point-based_Image_Editing_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">145</td>
<td style="text-align: center;">2023-11-20</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Yan_GS-SLAM_Dense_Visual_SLAM_with_3D_Gaussian_Splatting_CVPR_2024_paper.pdf">GS-SLAM: Dense Visual SLAM with 3D Gaussian Splatting</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Yan_GS-SLAM_Dense_Visual_SLAM_with_3D_Gaussian_Splatting_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">143</td>
<td style="text-align: center;">2023-11-28</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Leng_Mitigating_Object_Hallucinations_in_Large_Vision-Language_Models_through_Visual_Contrastive_CVPR_2024_paper.pdf">Mitigating Object Hallucinations in Large Vision-Language Models through Visual<br />Contrastive Decoding</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Leng_Mitigating_Object_Hallucinations_in_Large_Vision-Language_Models_through_Visual_Contrastive_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">142</td>
<td style="text-align: center;">2023-07-13</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Ruiz_HyperDreamBooth_HyperNetworks_for_Fast_Personalization_of_Text-to-Image_Models_CVPR_2024_paper.pdf">HyperDreamBooth: HyperNetworks for Fast Personalization of Text-to-Image Models</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Ruiz_HyperDreamBooth_HyperNetworks_for_Fast_Personalization_of_Text-to-Image_Models_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">141</td>
<td style="text-align: center;">2023-11-21</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Wallace_Diffusion_Model_Alignment_Using_Direct_Preference_Optimization_CVPR_2024_paper.pdf">Diffusion Model Alignment Using Direct Preference Optimization</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Wallace_Diffusion_Model_Alignment_Using_Direct_Preference_Optimization_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">141</td>
<td style="text-align: center;">2023-11-14</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Liu_One-2-3-45_Fast_Single_Image_to_3D_Objects_with_Consistent_Multi-View_CVPR_2024_paper.pdf">One-2-3-45++: Fast Single Image to 3D Objects with Consistent<br />Multi-View Generation and 3D Diffusion</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Liu_One-2-3-45_Fast_Single_Image_to_3D_Objects_with_Consistent_Multi-View_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">138</td>
<td style="text-align: center;">2023-11-30</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Yin_One-step_Diffusion_with_Distribution_Matching_Distillation_CVPR_2024_paper.pdf">One-step Diffusion with Distribution Matching Distillation</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Yin_One-step_Diffusion_with_Distribution_Matching_Distillation_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">135</td>
<td style="text-align: center;">2023-12-01</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Yu_RLHF-V_Towards_Trustworthy_MLLMs_via_Behavior_Alignment_from_Fine-grained_Correctional_CVPR_2024_paper.pdf">RLHF-V: Towards Trustworthy MLLMs via Behavior Alignment from Fine-grained<br />Correctional Human Feedback</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Yu_RLHF-V_Towards_Trustworthy_MLLMs_via_Behavior_Alignment_from_Fine-grained_Correctional_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">135</td>
<td style="text-align: center;">2023-11-14</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Jin_Chat-UniVi_Unified_Visual_Representation_Empowers_Large_Language_Models_with_Image_CVPR_2024_paper.pdf">Chat-UniVi: Unified Visual Representation Empowers Large Language Models with<br />Image and Video Understanding</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Jin_Chat-UniVi_Unified_Visual_Representation_Empowers_Large_Language_Models_with_Image_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">135</td>
<td style="text-align: center;">2023-11-19</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Liang_LucidDreamer_Towards_High-Fidelity_Text-to-3D_Generation_via_Interval_Score_Matching_CVPR_2024_paper.pdf">LucidDreamer: Towards High-Fidelity Text-to-3D Generation via Interval Score Matching</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Liang_LucidDreamer_Towards_High-Fidelity_Text-to-3D_Generation_via_Interval_Score_Matching_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">133</td>
<td style="text-align: center;">2023-11-24</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Chen_GaussianEditor_Swift_and_Controllable_3D_Editing_with_Gaussian_Splatting_CVPR_2024_paper.pdf">GaussianEditor: Swift and Controllable 3D Editing with Gaussian Splatting</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Chen_GaussianEditor_Swift_and_Controllable_3D_Editing_with_Gaussian_Splatting_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">132</td>
<td style="text-align: center;">2023-12-07</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Li_PhotoMaker_Customizing_Realistic_Human_Photos_via_Stacked_ID_Embedding_CVPR_2024_paper.pdf">PhotoMaker: Customizing Realistic Human Photos via Stacked ID Embedding</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Li_PhotoMaker_Customizing_Realistic_Human_Photos_via_Stacked_ID_Embedding_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">131</td>
<td style="text-align: center;">2023-11-27</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Xu_MagicAnimate_Temporally_Consistent_Human_Image_Animation_using_Diffusion_Model_CVPR_2024_paper.pdf">MagicAnimate: Temporally Consistent Human Image Animation using Diffusion Model</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Xu_MagicAnimate_Temporally_Consistent_Human_Image_Animation_using_Diffusion_Model_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">129</td>
<td style="text-align: center;">2023-12-14</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Zou_Triplane_Meets_Gaussian_Splatting_Fast_and_Generalizable_Single-View_3D_Reconstruction_CVPR_2024_paper.pdf">Triplane Meets Gaussian Splatting: Fast and Generalizable Single-View 3D<br />Reconstruction with Transformers</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Zou_Triplane_Meets_Gaussian_Splatting_Fast_and_Generalizable_Single-View_3D_Reconstruction_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">128</td>
<td style="text-align: center;">2023-11-06</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Rasheed_GLaMM_Pixel_Grounding_Large_Multimodal_Model_CVPR_2024_paper.pdf">GLaMM: Pixel Grounding Large Multimodal Model</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Rasheed_GLaMM_Pixel_Grounding_Large_Multimodal_Model_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">127</td>
<td style="text-align: center;">2023-12-15</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Wu_Point_Transformer_V3_Simpler_Faster_Stronger_CVPR_2024_paper.pdf">Point Transformer V3: Simpler Faster Stronger</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Wu_Point_Transformer_V3_Simpler_Faster_Stronger_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">124</td>
<td style="text-align: center;">2023-11-22</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Lee_Compact_3D_Gaussian_Representation_for_Radiance_Field_CVPR_2024_paper.pdf">Compact 3D Gaussian Representation for Radiance Field</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Lee_Compact_3D_Gaussian_Representation_for_Radiance_Field_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">123</td>
<td style="text-align: center;">2023-11-29</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Huang_OPERA_Alleviating_Hallucination_in_Multi-Modal_Large_Language_Models_via_Over-Trust_CVPR_2024_paper.pdf">OPERA: Alleviating Hallucination in Multi-Modal Large Language Models via<br />Over-Trust Penalty and Retrospection-Allocation</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Huang_OPERA_Alleviating_Hallucination_in_Multi-Modal_Large_Language_Models_via_Over-Trust_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">121</td>
<td style="text-align: center;">2023-11-20</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Xie_PhysGaussian_Physics-Integrated_3D_Gaussians_for_Generative_Dynamics_CVPR_2024_paper.pdf">PhysGaussian: Physics-Integrated 3D Gaussians for Generative Dynamics</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Xie_PhysGaussian_Physics-Integrated_3D_Gaussians_for_Generative_Dynamics_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">119</td>
<td style="text-align: center;">2023-12-20</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Szymanowicz_Splatter_Image_Ultra-Fast_Single-View_3D_Reconstruction_CVPR_2024_paper.pdf">Splatter Image: Ultra-Fast Single-View 3D Reconstruction</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Szymanowicz_Splatter_Image_Ultra-Fast_Single-View_3D_Reconstruction_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">119</td>
<td style="text-align: center;">2023-12-05</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Wu_ReconFusion_3D_Reconstruction_with_Diffusion_Priors_CVPR_2024_paper.pdf">ReconFusion: 3D Reconstruction with Diffusion Priors</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Wu_ReconFusion_3D_Reconstruction_with_Diffusion_Priors_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">118</td>
<td style="text-align: center;">2023-04-03</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Cao_DreamAvatar_Text-and-Shape_Guided_3D_Human_Avatar_Generation_via_Diffusion_Models_CVPR_2024_paper.pdf">DreamAvatar: Text-and-Shape Guided 3D Human Avatar Generation via Diffusion<br />Models</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Cao_DreamAvatar_Text-and-Shape_Guided_3D_Human_Avatar_Generation_via_Diffusion_Models_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">118</td>
<td style="text-align: center;">2023-07-18</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Wang_RepViT_Revisiting_Mobile_CNN_From_ViT_Perspective_CVPR_2024_paper.pdf">RepViT: Revisiting Mobile CNN From ViT Perspective</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Wang_RepViT_Revisiting_Mobile_CNN_From_ViT_Perspective_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">116</td>
<td style="text-align: center;">2023-12-01</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Bai_Sequential_Modeling_Enables_Scalable_Learning_for_Large_Vision_Models_CVPR_2024_paper.pdf">Sequential Modeling Enables Scalable Learning for Large Vision Models</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Bai_Sequential_Modeling_Enables_Scalable_Learning_for_Large_Vision_Models_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">115</td>
<td style="text-align: center;">2024-02-29</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Chen_Panda-70M_Captioning_70M_Videos_with_Multiple_Cross-Modality_Teachers_CVPR_2024_paper.pdf">Panda-70M: Captioning 70M Videos with Multiple Cross-Modality Teachers</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Chen_Panda-70M_Captioning_70M_Videos_with_Multiple_Cross-Modality_Teachers_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">111</td>
<td style="text-align: center;">2023-12-28</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Lu_Unified-IO_2_Scaling_Autoregressive_Multimodal_Models_with_Vision_Language_Audio_CVPR_2024_paper.pdf">Unified-IO 2: Scaling Autoregressive Multimodal Models with Vision Language<br />Audio and Action</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Lu_Unified-IO_2_Scaling_Autoregressive_Multimodal_Models_with_Vision_Language_Audio_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">110</td>
<td style="text-align: center;">2023-12-13</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Zhou_DrivingGaussian_Composite_Gaussian_Splatting_for_Surrounding_Dynamic_Autonomous_Driving_Scenes_CVPR_2024_paper.pdf">DrivingGaussian: Composite Gaussian Splatting for Surrounding Dynamic Autonomous Driving<br />Scenes</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Zhou_DrivingGaussian_Composite_Gaussian_Splatting_for_Surrounding_Dynamic_Autonomous_Driving_Scenes_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">110</td>
<td style="text-align: center;">2024-01-22</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Chen_SpatialVLM_Endowing_Vision-Language_Models_with_Spatial_Reasoning_Capabilities_CVPR_2024_paper.pdf">SpatialVLM: Endowing Vision-Language Models with Spatial Reasoning Capabilities</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Chen_SpatialVLM_Endowing_Vision-Language_Models_with_Spatial_Reasoning_Capabilities_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">109</td>
<td style="text-align: center;">2023-12-04</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Ren_TimeChat_A_Time-sensitive_Multimodal_Large_Language_Model_for_Long_Video_CVPR_2024_paper.pdf">TimeChat: A Time-sensitive Multimodal Large Language Model for Long<br />Video Understanding</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Ren_TimeChat_A_Time-sensitive_Multimodal_Large_Language_Model_for_Long_Video_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">107</td>
<td style="text-align: center;">2023-12-26</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Qin_LangSplat_3D_Language_Gaussian_Splatting_CVPR_2024_paper.pdf">LangSplat: 3D Language Gaussian Splatting</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Qin_LangSplat_3D_Language_Gaussian_Splatting_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">106</td>
<td style="text-align: center;">2023-12-04</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Huang_SC-GS_Sparse-Controlled_Gaussian_Splatting_for_Editable_Dynamic_Scenes_CVPR_2024_paper.pdf">SC-GS: Sparse-Controlled Gaussian Splatting for Editable Dynamic Scenes</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Huang_SC-GS_Sparse-Controlled_Gaussian_Splatting_for_Editable_Dynamic_Scenes_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">105</td>
<td style="text-align: center;">2023-12-01</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Xiong_EfficientSAM_Leveraged_Masked_Image_Pretraining_for_Efficient_Segment_Anything_CVPR_2024_paper.pdf">EfficientSAM: Leveraged Masked Image Pretraining for Efficient Segment Anything</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Xiong_EfficientSAM_Leveraged_Masked_Image_Pretraining_for_Efficient_Segment_Anything_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">104</td>
<td style="text-align: center;">2023-10-23</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Guan_HallusionBench_An_Advanced_Diagnostic_Suite_for_Entangled_Language_Hallucination_and_CVPR_2024_paper.pdf">HallusionBench: An Advanced Diagnostic Suite for Entangled Language Hallucination<br />and Visual Illusion in Large Vision-Language Models</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Guan_HallusionBench_An_Advanced_Diagnostic_Suite_for_Entangled_Language_Hallucination_and_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">104</td>
<td style="text-align: center;">2023-12-06</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Zhou_Feature_3DGS_Supercharging_3D_Gaussian_Splatting_to_Enable_Distilled_Feature_CVPR_2024_paper.pdf">Feature 3DGS: Supercharging 3D Gaussian Splatting to Enable Distilled<br />Feature Fields</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Zhou_Feature_3DGS_Supercharging_3D_Gaussian_Splatting_to_Enable_Distilled_Feature_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">103</td>
<td style="text-align: center;">2023-04-12</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Huberman-Spiegelglas_An_Edit_Friendly_DDPM_Noise_Space_Inversion_and_Manipulations_CVPR_2024_paper.pdf">An Edit Friendly DDPM Noise Space: Inversion and Manipulations</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Huberman-Spiegelglas_An_Edit_Friendly_DDPM_Noise_Space_Inversion_and_Manipulations_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">101</td>
<td style="text-align: center;">2023-12-05</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Karras_Analyzing_and_Improving_the_Training_Dynamics_of_Diffusion_Models_CVPR_2024_paper.pdf">Analyzing and Improving the Training Dynamics of Diffusion Models</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Karras_Analyzing_and_Improving_the_Training_Dynamics_of_Diffusion_Models_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">100</td>
<td style="text-align: center;">2023-12-13</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Wen_FoundationPose_Unified_6D_Pose_Estimation_and_Tracking_of_Novel_Objects_CVPR_2024_paper.pdf">FoundationPose: Unified 6D Pose Estimation and Tracking of Novel<br />Objects</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Wen_FoundationPose_Unified_6D_Pose_Estimation_and_Tracking_of_Novel_Objects_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">98</td>
<td style="text-align: center;">2023-12-04</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Ke_Repurposing_Diffusion-Based_Image_Generators_for_Monocular_Depth_Estimation_CVPR_2024_paper.pdf">Repurposing Diffusion-Based Image Generators for Monocular Depth Estimation</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Ke_Repurposing_Diffusion-Based_Image_Generators_for_Monocular_Depth_Estimation_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">96</td>
<td style="text-align: center;">2023-05-14</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Xue_ULIP-2_Towards_Scalable_Multimodal_Pre-training_for_3D_Understanding_CVPR_2024_paper.pdf">ULIP-2: Towards Scalable Multimodal Pre-training for 3D Understanding</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Xue_ULIP-2_Towards_Scalable_Multimodal_Pre-training_for_3D_Understanding_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">95</td>
<td style="text-align: center;">2023-11-30</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Grauman_Ego-Exo4D_Understanding_Skilled_Human_Activity_from_First-_and_Third-Person_Perspectives_CVPR_2024_paper.pdf">Ego-Exo4D: Understanding Skilled Human Activity from First- and Third-Person<br />Perspectives</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Grauman_Ego-Exo4D_Understanding_Skilled_Human_Activity_from_First-_and_Third-Person_Perspectives_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">94</td>
<td style="text-align: center;">2023-03-29</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Yu_InceptionNeXt_When_Inception_Meets_ConvNeXt_CVPR_2024_paper.pdf">InceptionNeXt: When Inception Meets ConvNeXt</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Yu_InceptionNeXt_When_Inception_Meets_ConvNeXt_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">91</td>
<td style="text-align: center;">2023-09-20</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Si_FreeU_Free_Lunch_in_Diffusion_U-Net_CVPR_2024_paper.pdf">FreeU: Free Lunch in Diffusion U-Net</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Si_FreeU_Free_Lunch_in_Diffusion_U-Net_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">90</td>
<td style="text-align: center;">2023-11-29</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Jiang_GaussianShader_3D_Gaussian_Splatting_with_Shading_Functions_for_Reflective_Surfaces_CVPR_2024_paper.pdf">GaussianShader: 3D Gaussian Splatting with Shading Functions for Reflective<br />Surfaces</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Jiang_GaussianShader_3D_Gaussian_Splatting_with_Shading_Functions_for_Reflective_Surfaces_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">89</td>
<td style="text-align: center;">2023-11-24</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Kuckreja_GeoChat_Grounded_Large_Vision-Language_Model_for_Remote_Sensing_CVPR_2024_paper.pdf">GeoChat: Grounded Large Vision-Language Model for Remote Sensing</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Kuckreja_GeoChat_Grounded_Large_Vision-Language_Model_for_Remote_Sensing_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">86</td>
<td style="text-align: center;">2023-10-17</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Liu_EvalCrafter_Benchmarking_and_Evaluating_Large_Video_Generation_Models_CVPR_2024_paper.pdf">EvalCrafter: Benchmarking and Evaluating Large Video Generation Models</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Liu_EvalCrafter_Benchmarking_and_Evaluating_Large_Video_Generation_Models_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">85</td>
<td style="text-align: center;">2023-12-01</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Ma_DeepCache_Accelerating_Diffusion_Models_for_Free_CVPR_2024_paper.pdf">DeepCache: Accelerating Diffusion Models for Free</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Ma_DeepCache_Accelerating_Diffusion_Models_for_Free_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">85</td>
<td style="text-align: center;">2023-12-04</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Qian_GaussianAvatars_Photorealistic_Head_Avatars_with_Rigged_3D_Gaussians_CVPR_2024_paper.pdf">GaussianAvatars: Photorealistic Head Avatars with Rigged 3D Gaussians</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Qian_GaussianAvatars_Photorealistic_Head_Avatars_with_Rigged_3D_Gaussians_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">85</td>
<td style="text-align: center;">2023-12-21</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Ling_Align_Your_Gaussians_Text-to-4D_with_Dynamic_3D_Gaussians_and_Composed_CVPR_2024_paper.pdf">Align Your Gaussians: Text-to-4D with Dynamic 3D Gaussians and<br />Composed Diffusion Models</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Ling_Align_Your_Gaussians_Text-to-4D_with_Dynamic_3D_Gaussians_and_Composed_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">85</td>
<td style="text-align: center;">2023-11-16</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Sheynin_Emu_Edit_Precise_Image_Editing_via_Recognition_and_Generation_Tasks_CVPR_2024_paper.pdf">Emu Edit: Precise Image Editing via Recognition and Generation<br />Tasks</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Sheynin_Emu_Edit_Precise_Image_Editing_via_Recognition_and_Generation_Tasks_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">85</td>
<td style="text-align: center;">2023-12-28</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Li_Spacetime_Gaussian_Feature_Splatting_for_Real-Time_Dynamic_View_Synthesis_CVPR_2024_paper.pdf">Spacetime Gaussian Feature Splatting for Real-Time Dynamic View Synthesis</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Li_Spacetime_Gaussian_Feature_Splatting_for_Real-Time_Dynamic_View_Synthesis_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">84</td>
<td style="text-align: center;">2023-11-28</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Qiu_RichDreamer_A_Generalizable_Normal-Depth_Diffusion_Model_for_Detail_Richness_in_CVPR_2024_paper.pdf">RichDreamer: A Generalizable Normal-Depth Diffusion Model for Detail Richness<br />in Text-to-3D</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Qiu_RichDreamer_A_Generalizable_Normal-Depth_Diffusion_Model_for_Detail_Richness_in_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">83</td>
<td style="text-align: center;">2023-11-27</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Wang_GaussianEditor_Editing_3D_Gaussians_Delicately_with_Text_Instructions_CVPR_2024_paper.pdf">GaussianEditor: Editing 3D Gaussians Delicately with Text Instructions</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Wang_GaussianEditor_Editing_3D_Gaussians_Delicately_with_Text_Instructions_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">80</td>
<td style="text-align: center;">2023-12-04</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Hertz_Style_Aligned_Image_Generation_via_Shared_Attention_CVPR_2024_paper.pdf">Style Aligned Image Generation via Shared Attention</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Hertz_Style_Aligned_Image_Generation_via_Shared_Attention_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">80</td>
<td style="text-align: center;">2024-01-24</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Yu_Scaling_Up_to_Excellence_Practicing_Model_Scaling_for_Photo-Realistic_Image_CVPR_2024_paper.pdf">Scaling Up to Excellence: Practicing Model Scaling for Photo-Realistic<br />Image Restoration In the Wild</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Yu_Scaling_Up_to_Excellence_Practicing_Model_Scaling_for_Photo-Realistic_Image_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">80</td>
<td style="text-align: center;">2023-10-12</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Yi_GaussianDreamer_Fast_Generation_from_Text_to_3D_Gaussians_by_Bridging_CVPR_2024_paper.pdf">GaussianDreamer: Fast Generation from Text to 3D Gaussians by<br />Bridging 2D and 3D Diffusion Models</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Yi_GaussianDreamer_Fast_Generation_from_Text_to_3D_Gaussians_by_Bridging_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">80</td>
<td style="text-align: center;">2023-11-10</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Xiao_Florence-2_Advancing_a_Unified_Representation_for_a_Variety_of_Vision_CVPR_2024_paper.pdf">Florence-2: Advancing a Unified Representation for a Variety of<br />Vision Tasks</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Xiao_Florence-2_Advancing_a_Unified_Representation_for_a_Variety_of_Vision_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">79</td>
<td style="text-align: center;">2023-06-08</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Phung_Grounded_Text-to-Image_Synthesis_with_Attention_Refocusing_CVPR_2024_paper.pdf">Grounded Text-to-Image Synthesis with Attention Refocusing</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Phung_Grounded_Text-to-Image_Synthesis_with_Attention_Refocusing_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">79</td>
<td style="text-align: center;">2023-11-30</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Jayasumana_Rethinking_FID_Towards_a_Better_Evaluation_Metric_for_Image_Generation_CVPR_2024_paper.pdf">Rethinking FID: Towards a Better Evaluation Metric for Image<br />Generation</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Jayasumana_Rethinking_FID_Towards_a_Better_Evaluation_Metric_for_Image_Generation_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">79</td>
<td style="text-align: center;">2023-12-11</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Cha_Honeybee_Locality-enhanced_Projector_for_Multimodal_LLM_CVPR_2024_paper.pdf">Honeybee: Locality-enhanced Projector for Multimodal LLM</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Cha_Honeybee_Locality-enhanced_Projector_for_Multimodal_LLM_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">79</td>
<td style="text-align: center;">2023-12-15</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Guo_SkySense_A_Multi-Modal_Remote_Sensing_Foundation_Model_Towards_Universal_Interpretation_CVPR_2024_paper.pdf">SkySense: A Multi-Modal Remote Sensing Foundation Model Towards Universal<br />Interpretation for Earth Observation Imagery</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Guo_SkySense_A_Multi-Modal_Remote_Sensing_Foundation_Model_Towards_Universal_Interpretation_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">78</td>
<td style="text-align: center;">2023-11-29</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Wang_Driving_into_the_Future_Multiview_Visual_Forecasting_and_Planning_with_CVPR_2024_paper.pdf">Driving into the Future: Multiview Visual Forecasting and Planning<br />with World Model for Autonomous Driving</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Wang_Driving_into_the_Future_Multiview_Visual_Forecasting_and_Planning_with_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">78</td>
<td style="text-align: center;">2023-12-21</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Wu_V_Guided_Visual_Search_as_a_Core_Mechanism_in_Multimodal_CVPR_2024_paper.pdf">V?: Guided Visual Search as a Core Mechanism in<br />Multimodal LLMs</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Wu_V_Guided_Visual_Search_as_a_Core_Mechanism_in_Multimodal_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">78</td>
<td style="text-align: center;">2023-03-16</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Zhang_HIVE_Harnessing_Human_Feedback_for_Instructional_Visual_Editing_CVPR_2024_paper.pdf">HIVE: Harnessing Human Feedback for Instructional Visual Editing</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Zhang_HIVE_Harnessing_Human_Feedback_for_Instructional_Visual_Editing_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">77</td>
<td style="text-align: center;">2023-12-06</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Han_OneLLM_One_Framework_to_Align_All_Modalities_with_Language_CVPR_2024_paper.pdf">OneLLM: One Framework to Align All Modalities with Language</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Han_OneLLM_One_Framework_to_Align_All_Modalities_with_Language_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">76</td>
<td style="text-align: center;">2023-11-29</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Bahmani_4D-fy_Text-to-4D_Generation_Using_Hybrid_Score_Distillation_Sampling_CVPR_2024_paper.pdf">4D-fy: Text-to-4D Generation Using Hybrid Score Distillation Sampling</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Bahmani_4D-fy_Text-to-4D_Generation_Using_Hybrid_Score_Distillation_Sampling_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">76</td>
<td style="text-align: center;">2023-11-14</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Xu_UFOGen_You_Forward_Once_Large_Scale_Text-to-Image_Generation_via_Diffusion_CVPR_2024_paper.pdf">UFOGen: You Forward Once Large Scale Text-to-Image Generation via<br />Diffusion GANs</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Xu_UFOGen_You_Forward_Once_Large_Scale_Text-to-Image_Generation_via_Diffusion_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">75</td>
<td style="text-align: center;">2024-02-27</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Lin_VastGaussian_Vast_3D_Gaussians_for_Large_Scene_Reconstruction_CVPR_2024_paper.pdf">VastGaussian: Vast 3D Gaussians for Large Scene Reconstruction</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Lin_VastGaussian_Vast_3D_Gaussians_for_Large_Scene_Reconstruction_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">75</td>
<td style="text-align: center;">2023-03-21</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Cho_CAT-Seg_Cost_Aggregation_for_Open-Vocabulary_Semantic_Segmentation_CVPR_2024_paper.pdf">CAT-Seg: Cost Aggregation for Open-Vocabulary Semantic Segmentation</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Cho_CAT-Seg_Cost_Aggregation_for_Open-Vocabulary_Semantic_Segmentation_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">73</td>
<td style="text-align: center;">2023-11-17</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Niedermayr_Compressed_3D_Gaussian_Splatting_for_Accelerated_Novel_View_Synthesis_CVPR_2024_paper.pdf">Compressed 3D Gaussian Splatting for Accelerated Novel View Synthesis</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Niedermayr_Compressed_3D_Gaussian_Splatting_for_Accelerated_Novel_View_Synthesis_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">72</td>
<td style="text-align: center;">2023-11-27</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Ding_UniRepLKNet_A_Universal_Perception_Large-Kernel_ConvNet_for_Audio_Video_Point_CVPR_2024_paper.pdf">UniRepLKNet: A Universal Perception Large-Kernel ConvNet for Audio Video<br />Point Cloud Time-Series and Image Recognition</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Ding_UniRepLKNet_A_Universal_Perception_Large-Kernel_ConvNet_for_Audio_Video_Point_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">71</td>
<td style="text-align: center;">2023-11-27</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Wu_SeeSR_Towards_Semantics-Aware_Real-World_Image_Super-Resolution_CVPR_2024_paper.pdf">SeeSR: Towards Semantics-Aware Real-World Image Super-Resolution</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Wu_SeeSR_Towards_Semantics-Aware_Real-World_Image_Super-Resolution_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">70</td>
<td style="text-align: center;">2023-12-04</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Hu_GaussianAvatar_Towards_Realistic_Human_Avatar_Modeling_from_a_Single_Video_CVPR_2024_paper.pdf">GaussianAvatar: Towards Realistic Human Avatar Modeling from a Single<br />Video via Animatable 3D Gaussians</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Hu_GaussianAvatar_Towards_Realistic_Human_Avatar_Modeling_from_a_Single_Video_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">70</td>
<td style="text-align: center;">2023-09-07</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Geng_InstructDiffusion_A_Generalist_Modeling_Interface_for_Vision_Tasks_CVPR_2024_paper.pdf">InstructDiffusion: A Generalist Modeling Interface for Vision Tasks</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Geng_InstructDiffusion_A_Generalist_Modeling_Interface_for_Vision_Tasks_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">69</td>
<td style="text-align: center;">2023-10-19</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Cheng_Putting_the_Object_Back_into_Video_Object_Segmentation_CVPR_2024_paper.pdf">Putting the Object Back into Video Object Segmentation</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Cheng_Putting_the_Object_Back_into_Video_Object_Segmentation_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">69</td>
<td style="text-align: center;">2024-03-27</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Piccinelli_UniDepth_Universal_Monocular_Metric_Depth_Estimation_CVPR_2024_paper.pdf">UniDepth: Universal Monocular Metric Depth Estimation</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Piccinelli_UniDepth_Universal_Monocular_Metric_Depth_Estimation_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">69</td>
<td style="text-align: center;">2023-12-12</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Fu_COLMAP-Free_3D_Gaussian_Splatting_CVPR_2024_paper.pdf">COLMAP-Free 3D Gaussian Splatting</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Fu_COLMAP-Free_3D_Gaussian_Splatting_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">69</td>
<td style="text-align: center;">2023-12-12</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Shao_LMDrive_Closed-Loop_End-to-End_Driving_with_Large_Language_Models_CVPR_2024_paper.pdf">LMDrive: Closed-Loop End-to-End Driving with Large Language Models</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Shao_LMDrive_Closed-Loop_End-to-End_Driving_with_Large_Language_Models_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">69</td>
<td style="text-align: center;">2024-06-16</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Li_Animatable_Gaussians_Learning_Pose-dependent_Gaussian_Maps_for_High-fidelity_Human_Avatar_CVPR_2024_paper.pdf">Animatable Gaussians: Learning Pose-dependent Gaussian Maps for High-fidelity Human<br />Avatar Modeling</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Li_Animatable_Gaussians_Learning_Pose-dependent_Gaussian_Maps_for_High-fidelity_Human_Avatar_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">68</td>
<td style="text-align: center;">2023-11-27</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Siddiqui_MeshGPT_Generating_Triangle_Meshes_with_Decoder-Only_Transformers_CVPR_2024_paper.pdf">MeshGPT: Generating Triangle Meshes with Decoder-Only Transformers</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Siddiqui_MeshGPT_Generating_Triangle_Meshes_with_Decoder-Only_Transformers_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">68</td>
<td style="text-align: center;">2024-01-08</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Wu_GPT-4Vision_is_a_Human-Aligned_Evaluator_for_Text-to-3D_Generation_CVPR_2024_paper.pdf">GPT-4V(ision) is a Human-Aligned Evaluator for Text-to-3D Generation</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Wu_GPT-4Vision_is_a_Human-Aligned_Evaluator_for_Text-to-3D_Generation_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">68</td>
<td style="text-align: center;">2023-11-26</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Liang_GS-IR_3D_Gaussian_Splatting_for_Inverse_Rendering_CVPR_2024_paper.pdf">GS-IR: 3D Gaussian Splatting for Inverse Rendering</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Liang_GS-IR_3D_Gaussian_Splatting_for_Inverse_Rendering_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">68</td>
<td style="text-align: center;">2023-08-15</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Ouyang_CoDeF_Content_Deformation_Fields_for_Temporally_Consistent_Video_Processing_CVPR_2024_paper.pdf">CoDeF: Content Deformation Fields for Temporally Consistent Video Processing</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Ouyang_CoDeF_Content_Deformation_Fields_for_Temporally_Consistent_Video_Processing_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">67</td>
<td style="text-align: center;">2023-11-18</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Zeng_Make_Pixels_Dance_High-Dynamic_Video_Generation_CVPR_2024_paper.pdf">Make Pixels Dance: High-Dynamic Video Generation</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Zeng_Make_Pixels_Dance_High-Dynamic_Video_Generation_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">66</td>
<td style="text-align: center;">2023-11-30</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Huang_VTimeLLM_Empower_LLM_to_Grasp_Video_Moments_CVPR_2024_paper.pdf">VTimeLLM: Empower LLM to Grasp Video Moments</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Huang_VTimeLLM_Empower_LLM_to_Grasp_Video_Moments_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">66</td>
<td style="text-align: center;">2023-12-05</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Hu_GauHuman_Articulated_Gaussian_Splatting_from_Monocular_Human_Videos_CVPR_2024_paper.pdf">GauHuman: Articulated Gaussian Splatting from Monocular Human Videos</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Hu_GauHuman_Articulated_Gaussian_Splatting_from_Monocular_Human_Videos_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">66</td>
<td style="text-align: center;">2023-11-28</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Huang_Photo-SLAM_Real-time_Simultaneous_Localization_and_Photorealistic_Mapping_for_Monocular_Stereo_CVPR_2024_paper.pdf">Photo-SLAM: Real-time Simultaneous Localization and Photorealistic Mapping for Monocular<br />Stereo and RGB-D Cameras</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Huang_Photo-SLAM_Real-time_Simultaneous_Localization_and_Photorealistic_Mapping_for_Monocular_Stereo_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">65</td>
<td style="text-align: center;">2023-12-14</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Qian_3DGS-Avatar_Animatable_Avatars_via_Deformable_3D_Gaussian_Splatting_CVPR_2024_paper.pdf">3DGS-Avatar: Animatable Avatars via Deformable 3D Gaussian Splatting</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Qian_3DGS-Avatar_Animatable_Avatars_via_Deformable_3D_Gaussian_Splatting_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">65</td>
<td style="text-align: center;">2024-06-16</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Majumdar_OpenEQA_Embodied_Question_Answering_in_the_Era_of_Foundation_Models_CVPR_2024_paper.pdf">OpenEQA: Embodied Question Answering in the Era of Foundation<br />Models</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Majumdar_OpenEQA_Embodied_Question_Answering_in_the_Era_of_Foundation_Models_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">64</td>
<td style="text-align: center;">2023-08-18</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Xing_SimDA_Simple_Diffusion_Adapter_for_Efficient_Video_Generation_CVPR_2024_paper.pdf">SimDA: Simple Diffusion Adapter for Efficient Video Generation</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Xing_SimDA_Simple_Diffusion_Adapter_for_Efficient_Video_Generation_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">64</td>
<td style="text-align: center;">2024-03-11</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Li_DNGaussian_Optimizing_Sparse-View_3D_Gaussian_Radiance_Fields_with_Global-Local_Depth_CVPR_2024_paper.pdf">DNGaussian: Optimizing Sparse-View 3D Gaussian Radiance Fields with Global-Local<br />Depth Normalization</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Li_DNGaussian_Optimizing_Sparse-View_3D_Gaussian_Radiance_Fields_with_Global-Local_Depth_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">64</td>
<td style="text-align: center;">2023-12-04</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Kim_StableVITON_Learning_Semantic_Correspondence_with_Latent_Diffusion_Model_for_Virtual_CVPR_2024_paper.pdf">StableVITON: Learning Semantic Correspondence with Latent Diffusion Model for<br />Virtual Try-On</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Kim_StableVITON_Learning_Semantic_Correspondence_with_Latent_Diffusion_Model_for_Virtual_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">62</td>
<td style="text-align: center;">2023-11-28</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Liu_HumanGaussian_Text-Driven_3D_Human_Generation_with_Gaussian_Splatting_CVPR_2024_paper.pdf">HumanGaussian: Text-Driven 3D Human Generation with Gaussian Splatting</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Liu_HumanGaussian_Text-Driven_3D_Human_Generation_with_Gaussian_Splatting_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">62</td>
<td style="text-align: center;">2023-12-04</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Zheng_GPS-Gaussian_Generalizable_Pixel-wise_3D_Gaussian_Splatting_for_Real-time_Human_Novel_CVPR_2024_paper.pdf">GPS-Gaussian: Generalizable Pixel-wise 3D Gaussian Splatting for Real-time Human<br />Novel View Synthesis</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Zheng_GPS-Gaussian_Generalizable_Pixel-wise_3D_Gaussian_Splatting_for_Real-time_Human_Novel_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">61</td>
<td style="text-align: center;">2023-12-01</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Cai_ViP-LLaVA_Making_Large_Multimodal_Models_Understand_Arbitrary_Visual_Prompts_CVPR_2024_paper.pdf">ViP-LLaVA: Making Large Multimodal Models Understand Arbitrary Visual Prompts</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Cai_ViP-LLaVA_Making_Large_Multimodal_Models_Understand_Arbitrary_Visual_Prompts_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">61</td>
<td style="text-align: center;">2023-12-07</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Wei_DreamVideo_Composing_Your_Dream_Videos_with_Customized_Subject_and_Motion_CVPR_2024_paper.pdf">DreamVideo: Composing Your Dream Videos with Customized Subject and<br />Motion</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Wei_DreamVideo_Composing_Your_Dream_Videos_with_Customized_Subject_and_Motion_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">61</td>
<td style="text-align: center;">2023-11-28</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Yan_Multi-Scale_3D_Gaussian_Splatting_for_Anti-Aliased_Rendering_CVPR_2024_paper.pdf">Multi-Scale 3D Gaussian Splatting for Anti-Aliased Rendering</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Yan_Multi-Scale_3D_Gaussian_Splatting_for_Anti-Aliased_Rendering_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">60</td>
<td style="text-align: center;">2023-12-06</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Lin_Gaussian-Flow_4D_Reconstruction_with_Dynamic_3D_Gaussian_Particle_CVPR_2024_paper.pdf">Gaussian-Flow: 4D Reconstruction with Dynamic 3D Gaussian Particle</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Lin_Gaussian-Flow_4D_Reconstruction_with_Dynamic_3D_Gaussian_Particle_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">59</td>
<td style="text-align: center;">2023-11-29</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Guo_MoMask_Generative_Masked_Modeling_of_3D_Human_Motions_CVPR_2024_paper.pdf">MoMask: Generative Masked Modeling of 3D Human Motions</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Guo_MoMask_Generative_Masked_Modeling_of_3D_Human_Motions_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">59</td>
<td style="text-align: center;">2023-11-29</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Kocabas_HUGS_Human_Gaussian_Splats_CVPR_2024_paper.pdf">HUGS: Human Gaussian Splats</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Kocabas_HUGS_Human_Gaussian_Splats_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">57</td>
<td style="text-align: center;">2023-12-24</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Li_ManipLLM_Embodied_Multimodal_Large_Language_Model_for_Object-Centric_Robotic_Manipulation_CVPR_2024_paper.pdf">ManipLLM: Embodied Multimodal Large Language Model for Object-Centric Robotic<br />Manipulation</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Li_ManipLLM_Embodied_Multimodal_Large_Language_Model_for_Object-Centric_Robotic_Manipulation_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">56</td>
<td style="text-align: center;">2023-12-08</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Pavlakos_Reconstructing_Hands_in_3D_with_Transformers_CVPR_2024_paper.pdf">Reconstructing Hands in 3D with Transformers</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Pavlakos_Reconstructing_Hands_in_3D_with_Transformers_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">55</td>
<td style="text-align: center;">2023-06-16</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Wang_PanoOcc_Unified_Occupancy_Representation_for_Camera-based_3D_Panoptic_Segmentation_CVPR_2024_paper.pdf">PanoOcc: Unified Occupancy Representation for Camera-based 3D Panoptic Segmentation</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Wang_PanoOcc_Unified_Occupancy_Representation_for_Camera-based_3D_Panoptic_Segmentation_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">54</td>
<td style="text-align: center;">2024-04-12</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Banani_Probing_the_3D_Awareness_of_Visual_Foundation_Models_CVPR_2024_paper.pdf">Probing the 3D Awareness of Visual Foundation Models</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Banani_Probing_the_3D_Awareness_of_Visual_Foundation_Models_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">54</td>
<td style="text-align: center;">2023-12-15</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Yuan_Osprey_Pixel_Understanding_with_Visual_Instruction_Tuning_CVPR_2024_paper.pdf">Osprey: Pixel Understanding with Visual Instruction Tuning</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Yuan_Osprey_Pixel_Understanding_with_Visual_Instruction_Tuning_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">54</td>
<td style="text-align: center;">2023-11-30</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Shi_Language_Embedded_3D_Gaussians_for_Open-Vocabulary_Scene_Understanding_CVPR_2024_paper.pdf">Language Embedded 3D Gaussians for Open-Vocabulary Scene Understanding</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Shi_Language_Embedded_3D_Gaussians_for_Open-Vocabulary_Scene_Understanding_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">54</td>
<td style="text-align: center;">2023-11-27</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Mitra_Compositional_Chain-of-Thought_Prompting_for_Large_Multimodal_Models_CVPR_2024_paper.pdf">Compositional Chain-of-Thought Prompting for Large Multimodal Models</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Mitra_Compositional_Chain-of-Thought_Prompting_for_Large_Multimodal_Models_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">53</td>
<td style="text-align: center;">2023-12-06</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Sun_Alpha-CLIP_A_CLIP_Model_Focusing_on_Wherever_You_Want_CVPR_2024_paper.pdf">Alpha-CLIP: A CLIP Model Focusing on Wherever You Want</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Sun_Alpha-CLIP_A_CLIP_Model_Focusing_on_Wherever_You_Want_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">53</td>
<td style="text-align: center;">2023-11-12</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Wu_Q-Instruct_Improving_Low-level_Visual_Abilities_for_Multi-modality_Foundation_Models_CVPR_2024_paper.pdf">Q-Instruct: Improving Low-level Visual Abilities for Multi-modality Foundation Models</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Wu_Q-Instruct_Improving_Low-level_Visual_Abilities_for_Multi-modality_Foundation_Models_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">53</td>
<td style="text-align: center;">2023-11-22</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Yang_Using_Human_Feedback_to_Fine-tune_Diffusion_Models_without_Any_Reward_CVPR_2024_paper.pdf">Using Human Feedback to Fine-tune Diffusion Models without Any<br />Reward Model</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Yang_Using_Human_Feedback_to_Fine-tune_Diffusion_Models_without_Any_Reward_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">53</td>
<td style="text-align: center;">2023-08-23</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Tian_Diffuse_Attend_and_Segment_Unsupervised_Zero-Shot_Segmentation_using_Stable_Diffusion_CVPR_2024_paper.pdf">Diffuse Attend and Segment: Unsupervised Zero-Shot Segmentation using Stable<br />Diffusion</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Tian_Diffuse_Attend_and_Segment_Unsupervised_Zero-Shot_Segmentation_using_Stable_Diffusion_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">51</td>
<td style="text-align: center;">2023-11-27</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Izquierdo_Optimal_Transport_Aggregation_for_Visual_Place_Recognition_CVPR_2024_paper.pdf">Optimal Transport Aggregation for Visual Place Recognition</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Izquierdo_Optimal_Transport_Aggregation_for_Visual_Place_Recognition_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">51</td>
<td style="text-align: center;">2023-05-25</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Xu_Prompt-Free_Diffusion_Taking_Text_out_of_Text-to-Image_Diffusion_Models_CVPR_2024_paper.pdf">Prompt-Free Diffusion: Taking "Text" out of Text-to-Image Diffusion Models</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Xu_Prompt-Free_Diffusion_Taking_Text_out_of_Text-to-Image_Diffusion_Models_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">51</td>
<td style="text-align: center;">2024-04-08</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/He_MA-LMM_Memory-Augmented_Large_Multimodal_Model_for_Long-Term_Video_Understanding_CVPR_2024_paper.pdf">MA-LMM: Memory-Augmented Large Multimodal Model for Long-Term Video Understanding</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/He_MA-LMM_Memory-Augmented_Large_Multimodal_Model_for_Long-Term_Video_Understanding_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">51</td>
<td style="text-align: center;">2023-11-21</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Huang_SelfOcc_Self-Supervised_Vision-Based_3D_Occupancy_Prediction_CVPR_2024_paper.pdf">SelfOcc: Self-Supervised Vision-Based 3D Occupancy Prediction</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Huang_SelfOcc_Self-Supervised_Vision-Based_3D_Occupancy_Prediction_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">50</td>
<td style="text-align: center;">2023-11-30</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Yan_Diffusion_Models_Without_Attention_CVPR_2024_paper.pdf">Diffusion Models Without Attention</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Yan_Diffusion_Models_Without_Attention_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">50</td>
<td style="text-align: center;">2023-12-14</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Yang_Holodeck_Language_Guided_Generation_of_3D_Embodied_AI_Environments_CVPR_2024_paper.pdf">Holodeck: Language Guided Generation of 3D Embodied AI Environments</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Yang_Holodeck_Language_Guided_Generation_of_3D_Embodied_AI_Environments_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">50</td>
<td style="text-align: center;">2023-12-26</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Ling_DL3DV-10K_A_Large-Scale_Scene_Dataset_for_Deep_Learning-based_3D_Vision_CVPR_2024_paper.pdf">DL3DV-10K: A Large-Scale Scene Dataset for Deep Learning-based 3D<br />Vision</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Ling_DL3DV-10K_A_Large-Scale_Scene_Dataset_for_Deep_Learning-based_3D_Vision_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">50</td>
<td style="text-align: center;">2024-03-08</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Shao_SplattingAvatar_Realistic_Real-Time_Human_Avatars_with_Mesh-Embedded_Gaussian_Splatting_CVPR_2024_paper.pdf">SplattingAvatar: Realistic Real-Time Human Avatars with Mesh-Embedded Gaussian Splatting</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Shao_SplattingAvatar_Realistic_Real-Time_Human_Avatars_with_Mesh-Embedded_Gaussian_Splatting_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">50</td>
<td style="text-align: center;">2023-10-02</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Huang_HumanNorm_Learning_Normal_Diffusion_Model_for_High-quality_and_Realistic_3D_CVPR_2024_paper.pdf">HumanNorm: Learning Normal Diffusion Model for High-quality and Realistic<br />3D Human Generation</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Huang_HumanNorm_Learning_Normal_Diffusion_Model_for_High-quality_and_Realistic_3D_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">49</td>
<td style="text-align: center;">2023-06-20</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Khanna_Habitat_Synthetic_Scenes_Dataset_HSSD-200_An_Analysis_of_3D_Scene_CVPR_2024_paper.pdf">Habitat Synthetic Scenes Dataset (HSSD-200): An Analysis of 3D<br />Scene Scale and Realism Tradeoffs for ObjectGoal Navigation</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Khanna_Habitat_Synthetic_Scenes_Dataset_HSSD-200_An_Analysis_of_3D_Scene_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">49</td>
<td style="text-align: center;">2023-12-15</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Deng_PLGSLAM_Progressive_Neural_Scene_Represenation_with_Local_to_Global_Bundle_CVPR_2024_paper.pdf">PLGSLAM: Progressive Neural Scene Represenation with Local to Global<br />Bundle Adjustment</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Deng_PLGSLAM_Progressive_Neural_Scene_Represenation_with_Local_to_Global_Bundle_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">49</td>
<td style="text-align: center;">2023-12-06</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Saito_Relightable_Gaussian_Codec_Avatars_CVPR_2024_paper.pdf">Relightable Gaussian Codec Avatars</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Saito_Relightable_Gaussian_Codec_Avatars_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">49</td>
<td style="text-align: center;">2023-11-27</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Lei_GART_Gaussian_Articulated_Template_Models_CVPR_2024_paper.pdf">GART: Gaussian Articulated Template Models</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Lei_GART_Gaussian_Articulated_Template_Models_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">48</td>
<td style="text-align: center;">2023-11-23</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Wang_SinSR_Diffusion-Based_Image_Super-Resolution_in_a_Single_Step_CVPR_2024_paper.pdf">SinSR: Diffusion-Based Image Super-Resolution in a Single Step</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Wang_SinSR_Diffusion-Based_Image_Super-Resolution_in_a_Single_Step_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">48</td>
<td style="text-align: center;">2023-12-07</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Fan_Scaling_Laws_of_Synthetic_Images_for_Model_Training_..._for_CVPR_2024_paper.pdf">Scaling Laws of Synthetic Images for Model Training ...<br />for Now</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Fan_Scaling_Laws_of_Synthetic_Images_for_Model_Training_..._for_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">48</td>
<td style="text-align: center;">2023-11-16</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Chen_DRESS_Instructing_Large_Vision-Language_Models_to_Align_and_Interact_with_CVPR_2024_paper.pdf">DRESS: Instructing Large Vision-Language Models to Align and Interact<br />with Humans via Natural Language Feedback</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Chen_DRESS_Instructing_Large_Vision-Language_Models_to_Align_and_Interact_with_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">48</td>
<td style="text-align: center;">2023-09-20</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Fan_RMT_Retentive_Networks_Meet_Vision_Transformers_CVPR_2024_paper.pdf">RMT: Retentive Networks Meet Vision Transformers</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Fan_RMT_Retentive_Networks_Meet_Vision_Transformers_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">48</td>
<td style="text-align: center;">2023-12-11</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Chung_Style_Injection_in_Diffusion_A_Training-free_Approach_for_Adapting_Large-scale_CVPR_2024_paper.pdf">Style Injection in Diffusion: A Training-free Approach for Adapting<br />Large-scale Diffusion Models for Style Transfer</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Chung_Style_Injection_in_Diffusion_A_Training-free_Approach_for_Adapting_Large-scale_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">47</td>
<td style="text-align: center;">2023-11-30</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Chen_LL3DA_Visual_Interactive_Instruction_Tuning_for_Omni-3D_Understanding_Reasoning_and_CVPR_2024_paper.pdf">LL3DA: Visual Interactive Instruction Tuning for Omni-3D Understanding Reasoning<br />and Planning</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Chen_LL3DA_Visual_Interactive_Instruction_Tuning_for_Omni-3D_Understanding_Reasoning_and_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">47</td>
<td style="text-align: center;">2023-06-30</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Wang_DisCo_Disentangled_Control_for_Realistic_Human_Dance_Generation_CVPR_2024_paper.pdf">DisCo: Disentangled Control for Realistic Human Dance Generation</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Wang_DisCo_Disentangled_Control_for_Realistic_Human_Dance_Generation_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">47</td>
<td style="text-align: center;">2023-12-04</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Ren_PixelLM_Pixel_Reasoning_with_Large_Multimodal_Model_CVPR_2024_paper.pdf">PixelLM: Pixel Reasoning with Large Multimodal Model</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Ren_PixelLM_Pixel_Reasoning_with_Large_Multimodal_Model_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">47</td>
<td style="text-align: center;">2023-09-06</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Goli_Bayes_Rays_Uncertainty_Quantification_for_Neural_Radiance_Fields_CVPR_2024_paper.pdf">Bayes' Rays: Uncertainty Quantification for Neural Radiance Fields</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Goli_Bayes_Rays_Uncertainty_Quantification_for_Neural_Radiance_Fields_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">46</td>
<td style="text-align: center;">2023-04-03</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Yang_RegionPLC_Regional_Point-Language_Contrastive_Learning_for_Open-World_3D_Scene_Understanding_CVPR_2024_paper.pdf">RegionPLC: Regional Point-Language Contrastive Learning for Open-World 3D Scene<br />Understanding</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Yang_RegionPLC_Regional_Point-Language_Contrastive_Learning_for_Open-World_3D_Scene_Understanding_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">46</td>
<td style="text-align: center;">2023-05-24</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Edstedt_RoMa_Robust_Dense_Feature_Matching_CVPR_2024_paper.pdf">RoMa: Robust Dense Feature Matching</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Edstedt_RoMa_Robust_Dense_Feature_Matching_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">46</td>
<td style="text-align: center;">2023-09-14</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Li_Generative_Image_Dynamics_CVPR_2024_paper.pdf">Generative Image Dynamics</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Li_Generative_Image_Dynamics_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">46</td>
<td style="text-align: center;">2024-02-05</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Wang_InstanceDiffusion_Instance-level_Control_for_Image_Generation_CVPR_2024_paper.pdf">InstanceDiffusion: Instance-level Control for Image Generation</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Wang_InstanceDiffusion_Instance-level_Control_for_Image_Generation_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">46</td>
<td style="text-align: center;">2023-12-01</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Jiang_VideoBooth_Diffusion-based_Video_Generation_with_Image_Prompts_CVPR_2024_paper.pdf">VideoBooth: Diffusion-based Video Generation with Image Prompts</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Jiang_VideoBooth_Diffusion-based_Video_Generation_with_Image_Prompts_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">46</td>
<td style="text-align: center;">2023-11-22</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Yu_HalluciDoctor_Mitigating_Hallucinatory_Toxicity_in_Visual_Instruction_Data_CVPR_2024_paper.pdf">HalluciDoctor: Mitigating Hallucinatory Toxicity in Visual Instruction Data</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Yu_HalluciDoctor_Mitigating_Hallucinatory_Toxicity_in_Visual_Instruction_Data_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">45</td>
<td style="text-align: center;">2024-03-18</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Yu_Boosting_Continual_Learning_of_Vision-Language_Models_via_Mixture-of-Experts_Adapters_CVPR_2024_paper.pdf">Boosting Continual Learning of Vision-Language Models via Mixture-of-Experts Adapters</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Yu_Boosting_Continual_Learning_of_Vision-Language_Models_via_Mixture-of-Experts_Adapters_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">45</td>
<td style="text-align: center;">2024-06-16</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Tan_Rethinking_the_Up-Sampling_Operations_in_CNN-based_Generative_Network_for_Generalizable_CVPR_2024_paper.pdf">Rethinking the Up-Sampling Operations in CNN-based Generative Network for<br />Generalizable Deepfake Detection</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Tan_Rethinking_the_Up-Sampling_Operations_in_CNN-based_Generative_Network_for_Generalizable_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">45</td>
<td style="text-align: center;">2023-11-28</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Moreau_Human_Gaussian_Splatting_Real-time_Rendering_of_Animatable_Avatars_CVPR_2024_paper.pdf">Human Gaussian Splatting: Real-time Rendering of Animatable Avatars</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Moreau_Human_Gaussian_Splatting_Real-time_Rendering_of_Animatable_Avatars_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">44</td>
<td style="text-align: center;">2024-03-29</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Ma_Rewrite_the_Stars_CVPR_2024_paper.pdf">Rewrite the Stars</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Ma_Rewrite_the_Stars_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">44</td>
<td style="text-align: center;">2023-11-28</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Brack_LEDITS_Limitless_Image_Editing_using_Text-to-Image_Models_CVPR_2024_paper.pdf">LEDITS++: Limitless Image Editing using Text-to-Image Models</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Brack_LEDITS_Limitless_Image_Editing_using_Text-to-Image_Models_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">44</td>
<td style="text-align: center;">2023-10-31</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Yu_CapsFusion_Rethinking_Image-Text_Data_at_Scale_CVPR_2024_paper.pdf">CapsFusion: Rethinking Image-Text Data at Scale</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Yu_CapsFusion_Rethinking_Image-Text_Data_at_Scale_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">44</td>
<td style="text-align: center;">2023-12-12</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Ma_VISTA-LLAMA_Reducing_Hallucination_in_Video_Language_Models_via_Equal_Distance_CVPR_2024_paper.pdf">VISTA-LLAMA: Reducing Hallucination in Video Language Models via Equal<br />Distance to Visual Tokens</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Ma_VISTA-LLAMA_Reducing_Hallucination_in_Video_Language_Models_via_Equal_Distance_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">43</td>
<td style="text-align: center;">2023-12-05</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Li_Is_Ego_Status_All_You_Need_for_Open-Loop_End-to-End_Autonomous_CVPR_2024_paper.pdf">Is Ego Status All You Need for Open-Loop End-to-End<br />Autonomous Driving?</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Li_Is_Ego_Status_All_You_Need_for_Open-Loop_End-to-End_Autonomous_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">43</td>
<td style="text-align: center;">2023-07-14</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Kulkarni_NIFTY_Neural_Object_Interaction_Fields_for_Guided_Human_Motion_Synthesis_CVPR_2024_paper.pdf">NIFTY: Neural Object Interaction Fields for Guided Human Motion<br />Synthesis</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Kulkarni_NIFTY_Neural_Object_Interaction_Fields_for_Guided_Human_Motion_Synthesis_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">43</td>
<td style="text-align: center;">2024-03-20</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Favero_Multi-Modal_Hallucination_Control_by_Visual_Information_Grounding_CVPR_2024_paper.pdf">Multi-Modal Hallucination Control by Visual Information Grounding</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Favero_Multi-Modal_Hallucination_Control_by_Visual_Information_Grounding_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">42</td>
<td style="text-align: center;">2024-03-10</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Lu_MACE_Mass_Concept_Erasure_in_Diffusion_Models_CVPR_2024_paper.pdf">MACE: Mass Concept Erasure in Diffusion Models</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Lu_MACE_Mass_Concept_Erasure_in_Diffusion_Models_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">42</td>
<td style="text-align: center;">2024-02-08</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Zhou_MIGC_Multi-Instance_Generation_Controller_for_Text-to-Image_Synthesis_CVPR_2024_paper.pdf">MIGC: Multi-Instance Generation Controller for Text-to-Image Synthesis</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Zhou_MIGC_Multi-Instance_Generation_Controller_for_Text-to-Image_Synthesis_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">42</td>
<td style="text-align: center;">2023-12-08</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Nguyen_SwiftBrush_One-Step_Text-to-Image_Diffusion_Model_with_Variational_Score_Distillation_CVPR_2024_paper.pdf">SwiftBrush: One-Step Text-to-Image Diffusion Model with Variational Score Distillation</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Nguyen_SwiftBrush_One-Step_Text-to-Image_Diffusion_Model_with_Variational_Score_Distillation_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">42</td>
<td style="text-align: center;">2023-10-17</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Xu_4K4D_Real-Time_4D_View_Synthesis_at_4K_Resolution_CVPR_2024_paper.pdf">4K4D: Real-Time 4D View Synthesis at 4K Resolution</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Xu_4K4D_Real-Time_4D_View_Synthesis_at_4K_Resolution_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">42</td>
<td style="text-align: center;">2023-12-21</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Zeng_Paint3D_Paint_Anything_3D_with_Lighting-Less_Texture_Diffusion_Models_CVPR_2024_paper.pdf">Paint3D: Paint Anything 3D with Lighting-Less Texture Diffusion Models</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Zeng_Paint3D_Paint_Anything_3D_with_Lighting-Less_Texture_Diffusion_Models_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">41</td>
<td style="text-align: center;">2024-03-10</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Cai_Poly_Kernel_Inception_Network_for_Remote_Sensing_Detection_CVPR_2024_paper.pdf">Poly Kernel Inception Network for Remote Sensing Detection</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Cai_Poly_Kernel_Inception_Network_for_Remote_Sensing_Detection_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">41</td>
<td style="text-align: center;">2023-11-28</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Shi_TransNeXt_Robust_Foveal_Visual_Perception_for_Vision_Transformers_CVPR_2024_paper.pdf">TransNeXt: Robust Foveal Visual Perception for Vision Transformers</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Shi_TransNeXt_Robust_Foveal_Visual_Perception_for_Vision_Transformers_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">40</td>
<td style="text-align: center;">2023-12-12</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Mo_FreeControl_Training-Free_Spatial_Control_of_Any_Text-to-Image_Diffusion_Model_with_CVPR_2024_paper.pdf">FreeControl: Training-Free Spatial Control of Any Text-to-Image Diffusion Model<br />with Any Condition</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Mo_FreeControl_Training-Free_Spatial_Control_of_Any_Text-to-Image_Diffusion_Model_with_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">40</td>
<td style="text-align: center;">2023-11-29</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Yang_MMA-Diffusion_MultiModal_Attack_on_Diffusion_Models_CVPR_2024_paper.pdf">MMA-Diffusion: MultiModal Attack on Diffusion Models</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Yang_MMA-Diffusion_MultiModal_Attack_on_Diffusion_Models_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">40</td>
<td style="text-align: center;">2023-11-30</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Stevens_BioCLIP_A_Vision_Foundation_Model_for_the_Tree_of_Life_CVPR_2024_paper.pdf">BioCLIP: A Vision Foundation Model for the Tree of<br />Life</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Stevens_BioCLIP_A_Vision_Foundation_Model_for_the_Tree_of_Life_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">39</td>
<td style="text-align: center;">2023-12-10</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Pang_ASH_Animatable_Gaussian_Splats_for_Efficient_and_Photoreal_Human_Rendering_CVPR_2024_paper.pdf">ASH: Animatable Gaussian Splats for Efficient and Photoreal Human<br />Rendering</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Pang_ASH_Animatable_Gaussian_Splats_for_Efficient_and_Photoreal_Human_Rendering_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">39</td>
<td style="text-align: center;">2023-12-06</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Wimbauer_Cache_Me_if_You_Can_Accelerating_Diffusion_Models_through_Block_CVPR_2024_paper.pdf">Cache Me if You Can: Accelerating Diffusion Models through<br />Block Caching</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Wimbauer_Cache_Me_if_You_Can_Accelerating_Diffusion_Models_through_Block_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">39</td>
<td style="text-align: center;">2023-11-28</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Zheng_A_Unified_Approach_for_Text-_and_Image-guided_4D_Scene_Generation_CVPR_2024_paper.pdf">A Unified Approach for Text- and Image-guided 4D Scene<br />Generation</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Zheng_A_Unified_Approach_for_Text-_and_Image-guided_4D_Scene_Generation_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">39</td>
<td style="text-align: center;">2024-03-03</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Sun_3DGStream_On-the-Fly_Training_of_3D_Gaussians_for_Efficient_Streaming_of_CVPR_2024_paper.pdf">3DGStream: On-the-Fly Training of 3D Gaussians for Efficient Streaming<br />of Photo-Realistic Free-Viewpoint Videos</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Sun_3DGStream_On-the-Fly_Training_of_3D_Gaussians_for_Efficient_Streaming_of_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">39</td>
<td style="text-align: center;">2023-06-27</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Jiang_Symphonize_3D_Semantic_Scene_Completion_with_Contextual_Instance_Queries_CVPR_2024_paper.pdf">Symphonize 3D Semantic Scene Completion with Contextual Instance Queries</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Jiang_Symphonize_3D_Semantic_Scene_Completion_with_Contextual_Instance_Queries_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">38</td>
<td style="text-align: center;">2024-03-14</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Yang_Generalized_Predictive_Model_for_Autonomous_Driving_CVPR_2024_paper.pdf">Generalized Predictive Model for Autonomous Driving</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Yang_Generalized_Predictive_Model_for_Autonomous_Driving_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">38</td>
<td style="text-align: center;">2023-12-26</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Wang_EmbodiedScan_A_Holistic_Multi-Modal_3D_Perception_Suite_Towards_Embodied_AI_CVPR_2024_paper.pdf">EmbodiedScan: A Holistic Multi-Modal 3D Perception Suite Towards Embodied<br />AI</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Wang_EmbodiedScan_A_Holistic_Multi-Modal_3D_Perception_Suite_Towards_Embodied_AI_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">38</td>
<td style="text-align: center;">2023-12-12</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Shin_WHAM_Reconstructing_World-grounded_Humans_with_Accurate_3D_Motion_CVPR_2024_paper.pdf">WHAM: Reconstructing World-grounded Humans with Accurate 3D Motion</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Shin_WHAM_Reconstructing_World-grounded_Humans_with_Accurate_3D_Motion_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">38</td>
<td style="text-align: center;">2023-11-27</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Lin_SAM-6D_Segment_Anything_Model_Meets_Zero-Shot_6D_Object_Pose_Estimation_CVPR_2024_paper.pdf">SAM-6D: Segment Anything Model Meets Zero-Shot 6D Object Pose<br />Estimation</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Lin_SAM-6D_Segment_Anything_Model_Meets_Zero-Shot_6D_Object_Pose_Estimation_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">37</td>
<td style="text-align: center;">2024-01-18</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Li_OMG-Seg_Is_One_Model_Good_Enough_For_All_Segmentation_CVPR_2024_paper.pdf">OMG-Seg: Is One Model Good Enough For All Segmentation?</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Li_OMG-Seg_Is_One_Model_Good_Enough_For_All_Segmentation_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">37</td>
<td style="text-align: center;">2023-08-18</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Wu_Towards_Large-scale_3D_Representation_Learning_with_Multi-dataset_Point_Prompt_Training_CVPR_2024_paper.pdf">Towards Large-scale 3D Representation Learning with Multi-dataset Point Prompt<br />Training</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Wu_Towards_Large-scale_3D_Representation_Learning_with_Multi-dataset_Point_Prompt_Training_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">37</td>
<td style="text-align: center;">2023-11-28</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Wen_Panacea_Panoramic_and_Controllable_Video_Generation_for_Autonomous_Driving_CVPR_2024_paper.pdf">Panacea: Panoramic and Controllable Video Generation for Autonomous Driving</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Wen_Panacea_Panoramic_and_Controllable_Video_Generation_for_Autonomous_Driving_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">37</td>
<td style="text-align: center;">2023-12-26</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Zhang_SSR-Encoder_Encoding_Selective_Subject_Representation_for_Subject-Driven_Generation_CVPR_2024_paper.pdf">SSR-Encoder: Encoding Selective Subject Representation for Subject-Driven Generation</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Zhang_SSR-Encoder_Encoding_Selective_Subject_Representation_for_Subject-Driven_Generation_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">37</td>
<td style="text-align: center;">2023-11-27</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Diller_CG-HOI_Contact-Guided_3D_Human-Object_Interaction_Generation_CVPR_2024_paper.pdf">CG-HOI: Contact-Guided 3D Human-Object Interaction Generation</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Diller_CG-HOI_Contact-Guided_3D_Human-Object_Interaction_Generation_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">37</td>
<td style="text-align: center;">2024-02-27</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Li_Neural_Video_Compression_with_Feature_Modulation_CVPR_2024_paper.pdf">Neural Video Compression with Feature Modulation</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Li_Neural_Video_Compression_with_Feature_Modulation_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">36</td>
<td style="text-align: center;">2023-12-15</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Liang_Rich_Human_Feedback_for_Text-to-Image_Generation_CVPR_2024_paper.pdf">Rich Human Feedback for Text-to-Image Generation</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Liang_Rich_Human_Feedback_for_Text-to-Image_Generation_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">36</td>
<td style="text-align: center;">2023-11-26</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Tonderski_NeuRAD_Neural_Rendering_for_Autonomous_Driving_CVPR_2024_paper.pdf">NeuRAD: Neural Rendering for Autonomous Driving</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Tonderski_NeuRAD_Neural_Rendering_for_Autonomous_Driving_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">35</td>
<td style="text-align: center;">2024-01-31</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Yang_Binding_Touch_to_Everything_Learning_Unified_Multimodal_Tactile_Representations_CVPR_2024_paper.pdf">Binding Touch to Everything: Learning Unified Multimodal Tactile Representations</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Yang_Binding_Touch_to_Everything_Learning_Unified_Multimodal_Tactile_Representations_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">35</td>
<td style="text-align: center;">2023-12-11</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Huang_SmartEdit_Exploring_Complex_Instruction-based_Image_Editing_with_Multimodal_Large_Language_CVPR_2024_paper.pdf">SmartEdit: Exploring Complex Instruction-based Image Editing with Multimodal Large<br />Language Models</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Huang_SmartEdit_Exploring_Complex_Instruction-based_Image_Editing_with_Multimodal_Large_Language_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">35</td>
<td style="text-align: center;">2024-02-22</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Menapace_Snap_Video_Scaled_Spatiotemporal_Transformers_for_Text-to-Video_Synthesis_CVPR_2024_paper.pdf">Snap Video: Scaled Spatiotemporal Transformers for Text-to-Video Synthesis</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Menapace_Snap_Video_Scaled_Spatiotemporal_Transformers_for_Text-to-Video_Synthesis_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">35</td>
<td style="text-align: center;">2023-12-17</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Nguyen_Open3DIS_Open-Vocabulary_3D_Instance_Segmentation_with_2D_Mask_Guidance_CVPR_2024_paper.pdf">Open3DIS: Open-Vocabulary 3D Instance Segmentation with 2D Mask Guidance</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Nguyen_Open3DIS_Open-Vocabulary_3D_Instance_Segmentation_with_2D_Mask_Guidance_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">35</td>
<td style="text-align: center;">2024-03-03</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Sun_Logit_Standardization_in_Knowledge_Distillation_CVPR_2024_paper.pdf">Logit Standardization in Knowledge Distillation</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Sun_Logit_Standardization_in_Knowledge_Distillation_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">35</td>
<td style="text-align: center;">2023-11-20</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Liang_BadCLIP_Dual-Embedding_Guided_Backdoor_Attack_on_Multimodal_Contrastive_Learning_CVPR_2024_paper.pdf">BadCLIP: Dual-Embedding Guided Backdoor Attack on Multimodal Contrastive Learning</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Liang_BadCLIP_Dual-Embedding_Guided_Backdoor_Attack_on_Multimodal_Contrastive_Learning_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">35</td>
<td style="text-align: center;">2024-02-08</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Wei_Editable_Scene_Simulation_for_Autonomous_Driving_via_Collaborative_LLM-Agents_CVPR_2024_paper.pdf">Editable Scene Simulation for Autonomous Driving via Collaborative LLM-Agents</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Wei_Editable_Scene_Simulation_for_Autonomous_Driving_via_Collaborative_LLM-Agents_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">34</td>
<td style="text-align: center;">2024-02-27</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Xing_Seeing_and_Hearing_Open-domain_Visual-Audio_Generation_with_Diffusion_Latent_Aligners_CVPR_2024_paper.pdf">Seeing and Hearing: Open-domain Visual-Audio Generation with Diffusion Latent<br />Aligners</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Xing_Seeing_and_Hearing_Open-domain_Visual-Audio_Generation_with_Diffusion_Latent_Aligners_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">34</td>
<td style="text-align: center;">2023-12-11</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Peng_PortraitBooth_A_Versatile_Portrait_Model_for_Fast_Identity-preserved_Personalization_CVPR_2024_paper.pdf">PortraitBooth: A Versatile Portrait Model for Fast Identity-preserved Personalization</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Peng_PortraitBooth_A_Versatile_Portrait_Model_for_Fast_Identity-preserved_Personalization_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">34</td>
<td style="text-align: center;">2023-12-01</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Jeong_VMC_Video_Motion_Customization_using_Temporal_Attention_Adaption_for_Text-to-Video_CVPR_2024_paper.pdf">VMC: Video Motion Customization using Temporal Attention Adaption for<br />Text-to-Video Diffusion Models</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Jeong_VMC_Video_Motion_Customization_using_Temporal_Attention_Adaption_for_Text-to-Video_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">34</td>
<td style="text-align: center;">2024-03-12</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Xia_ViT-CoMer_Vision_Transformer_with_Convolutional_Multi-scale_Feature_Interaction_for_Dense_CVPR_2024_paper.pdf">ViT-CoMer: Vision Transformer with Convolutional Multi-scale Feature Interaction for<br />Dense Predictions</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Xia_ViT-CoMer_Vision_Transformer_with_Convolutional_Multi-scale_Feature_Interaction_for_Dense_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">34</td>
<td style="text-align: center;">2023-12-12</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Jiang_Hallucination_Augmented_Contrastive_Learning_for_Multimodal_Large_Language_Model_CVPR_2024_paper.pdf">Hallucination Augmented Contrastive Learning for Multimodal Large Language Model</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Jiang_Hallucination_Augmented_Contrastive_Learning_for_Multimodal_Large_Language_Model_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">34</td>
<td style="text-align: center;">2023-11-24</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Du_DemoFusion_Democratising_High-Resolution_Image_Generation_With_No__CVPR_2024_paper.pdf">DemoFusion: Democratising High-Resolution Image Generation With No $$$</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Du_DemoFusion_Democratising_High-Resolution_Image_Generation_With_No__CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">34</td>
<td style="text-align: center;">2023-11-27</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Sun_CoSeR_Bridging_Image_and_Language_for_Cognitive_Super-Resolution_CVPR_2024_paper.pdf">CoSeR: Bridging Image and Language for Cognitive Super-Resolution</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Sun_CoSeR_Bridging_Image_and_Language_for_Cognitive_Super-Resolution_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">34</td>
<td style="text-align: center;">2024-06-16</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Li_SEED-Bench_Benchmarking_Multimodal_Large_Language_Models_CVPR_2024_paper.pdf">SEED-Bench: Benchmarking Multimodal Large Language Models</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Li_SEED-Bench_Benchmarking_Multimodal_Large_Language_Models_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">33</td>
<td style="text-align: center;">2023-12-02</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Das_Neural_Parametric_Gaussians_for_Monocular_Non-Rigid_Object_Reconstruction_CVPR_2024_paper.pdf">Neural Parametric Gaussians for Monocular Non-Rigid Object Reconstruction</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Das_Neural_Parametric_Gaussians_for_Monocular_Non-Rigid_Object_Reconstruction_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">33</td>
<td style="text-align: center;">2023-12-18</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Youwang_Paint-it_Text-to-Texture_Synthesis_via_Deep_Convolutional_Texture_Map_Optimization_and_CVPR_2024_paper.pdf">Paint-it: Text-to-Texture Synthesis via Deep Convolutional Texture Map Optimization<br />and Physically-Based Rendering</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Youwang_Paint-it_Text-to-Texture_Synthesis_via_Deep_Convolutional_Texture_Map_Optimization_and_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">33</td>
<td style="text-align: center;">2023-11-18</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Zhu_SNI-SLAM_Semantic_Neural_Implicit_SLAM_CVPR_2024_paper.pdf">SNI-SLAM: Semantic Neural Implicit SLAM</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Zhu_SNI-SLAM_Semantic_Neural_Implicit_SLAM_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">33</td>
<td style="text-align: center;">2023-11-30</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Gao_GraphDreamer_Compositional_3D_Scene_Synthesis_from_Scene_Graphs_CVPR_2024_paper.pdf">GraphDreamer: Compositional 3D Scene Synthesis from Scene Graphs</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Gao_GraphDreamer_Compositional_3D_Scene_Synthesis_from_Scene_Graphs_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">33</td>
<td style="text-align: center;">2023-06-01</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Liu_Intelligent_Grimm_-_Open-ended_Visual_Storytelling_via_Latent_Diffusion_Models_CVPR_2024_paper.pdf">Intelligent Grimm - Open-ended Visual Storytelling via Latent Diffusion<br />Models</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Liu_Intelligent_Grimm_-_Open-ended_Visual_Storytelling_via_Latent_Diffusion_Models_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">32</td>
<td style="text-align: center;">2024-01-10</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Pan_VLP_Vision_Language_Planning_for_Autonomous_Driving_CVPR_2024_paper.pdf">VLP: Vision Language Planning for Autonomous Driving</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Pan_VLP_Vision_Language_Planning_for_Autonomous_Driving_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">32</td>
<td style="text-align: center;">2023-11-30</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Tang_CoDi-2_In-Context_Interleaved_and_Interactive_Any-to-Any_Generation_CVPR_2024_paper.pdf">CoDi-2: In-Context Interleaved and Interactive Any-to-Any Generation</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Tang_CoDi-2_In-Context_Interleaved_and_Interactive_Any-to-Any_Generation_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">32</td>
<td style="text-align: center;">2023-11-30</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Zhou_Fast_ODE-based_Sampling_for_Diffusion_Models_in_Around_5_Steps_CVPR_2024_paper.pdf">Fast ODE-based Sampling for Diffusion Models in Around 5<br />Steps</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Zhou_Fast_ODE-based_Sampling_for_Diffusion_Models_in_Around_5_Steps_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">32</td>
<td style="text-align: center;">2024-03-19</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Zhou_HUGS_Holistic_Urban_3D_Scene_Understanding_via_Gaussian_Splatting_CVPR_2024_paper.pdf">HUGS: Holistic Urban 3D Scene Understanding via Gaussian Splatting</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Zhou_HUGS_Holistic_Urban_3D_Scene_Understanding_via_Gaussian_Splatting_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">32</td>
<td style="text-align: center;">2024-01-17</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Kim_GARField_Group_Anything_with_Radiance_Fields_CVPR_2024_paper.pdf">GARField: Group Anything with Radiance Fields</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Kim_GARField_Group_Anything_with_Radiance_Fields_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">32</td>
<td style="text-align: center;">2024-04-09</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Lu_3D_Geometry-Aware_Deformable_Gaussian_Splatting_for_Dynamic_View_Synthesis_CVPR_2024_paper.pdf">3D Geometry-Aware Deformable Gaussian Splatting for Dynamic View Synthesis</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Lu_3D_Geometry-Aware_Deformable_Gaussian_Splatting_for_Dynamic_View_Synthesis_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">32</td>
<td style="text-align: center;">2024-02-20</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Islam_Video_ReCap_Recursive_Captioning_of_Hour-Long_Videos_CVPR_2024_paper.pdf">Video ReCap: Recursive Captioning of Hour-Long Videos</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Islam_Video_ReCap_Recursive_Captioning_of_Hour-Long_Videos_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">32</td>
<td style="text-align: center;">2023-12-07</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Zheng_Free3D_Consistent_Novel_View_Synthesis_without_3D_Representation_CVPR_2024_paper.pdf">Free3D: Consistent Novel View Synthesis without 3D Representation</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Zheng_Free3D_Consistent_Novel_View_Synthesis_without_3D_Representation_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">32</td>
<td style="text-align: center;">2024-01-03</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Hu_Instruct-Imagen_Image_Generation_with_Multi-modal_Instruction_CVPR_2024_paper.pdf">Instruct-Imagen: Image Generation with Multi-modal Instruction</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Hu_Instruct-Imagen_Image_Generation_with_Multi-modal_Instruction_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">31</td>
<td style="text-align: center;">2023-12-14</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Yariv_Mosaic-SDF_for_3D_Generative_Models_CVPR_2024_paper.pdf">Mosaic-SDF for 3D Generative Models</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Yariv_Mosaic-SDF_for_3D_Generative_Models_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">31</td>
<td style="text-align: center;">2023-12-18</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Yuan_GAvatar_Animatable_3D_Gaussian_Avatars_with_Implicit_Mesh_Learning_CVPR_2024_paper.pdf">GAvatar: Animatable 3D Gaussian Avatars with Implicit Mesh Learning</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Yuan_GAvatar_Animatable_3D_Gaussian_Avatars_with_Implicit_Mesh_Learning_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">31</td>
<td style="text-align: center;">2023-09-28</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Feng_CCEdit_Creative_and_Controllable_Video_Editing_via_Diffusion_Models_CVPR_2024_paper.pdf">CCEdit: Creative and Controllable Video Editing via Diffusion Models</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Feng_CCEdit_Creative_and_Controllable_Video_Editing_via_Diffusion_Models_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">31</td>
<td style="text-align: center;">2023-10-12</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Yang_UniPAD_A_Universal_Pre-training_Paradigm_for_Autonomous_Driving_CVPR_2024_paper.pdf">UniPAD: A Universal Pre-training Paradigm for Autonomous Driving</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Yang_UniPAD_A_Universal_Pre-training_Paradigm_for_Autonomous_Driving_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">31</td>
<td style="text-align: center;">2023-11-24</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Kolodiazhnyi_OneFormer3D_One_Transformer_for_Unified_Point_Cloud_Segmentation_CVPR_2024_paper.pdf">OneFormer3D: One Transformer for Unified Point Cloud Segmentation</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Kolodiazhnyi_OneFormer3D_One_Transformer_for_Unified_Point_Cloud_Segmentation_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">31</td>
<td style="text-align: center;">2023-11-29</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Abdal_Gaussian_Shell_Maps_for_Efficient_3D_Human_Generation_CVPR_2024_paper.pdf">Gaussian Shell Maps for Efficient 3D Human Generation</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Abdal_Gaussian_Shell_Maps_for_Efficient_3D_Human_Generation_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">31</td>
<td style="text-align: center;">2023-10-01</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Xuan_Pink_Unveiling_the_Power_of_Referential_Comprehension_for_Multi-modal_LLMs_CVPR_2024_paper.pdf">Pink: Unveiling the Power of Referential Comprehension for Multi-modal<br />LLMs</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Xuan_Pink_Unveiling_the_Power_of_Referential_Comprehension_for_Multi-modal_LLMs_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">31</td>
<td style="text-align: center;">2024-02-15</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Hamdi_GES__Generalized_Exponential_Splatting_for_Efficient_Radiance_Field_Rendering_CVPR_2024_paper.pdf">GES : Generalized Exponential Splatting for Efficient Radiance Field<br />Rendering</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Hamdi_GES__Generalized_Exponential_Splatting_for_Efficient_Radiance_Field_Rendering_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">31</td>
<td style="text-align: center;">2023-04-13</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Jaume_Modeling_Dense_Multimodal_Interactions_Between_Biological_Pathways_and_Histology_for_CVPR_2024_paper.pdf">Modeling Dense Multimodal Interactions Between Biological Pathways and Histology<br />for Survival Prediction</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Jaume_Modeling_Dense_Multimodal_Interactions_Between_Biological_Pathways_and_Histology_for_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">31</td>
<td style="text-align: center;">2023-12-26</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Lyu_One-dimensional_Adapter_to_Rule_Them_All_Concepts_Diffusion_Models_and_CVPR_2024_paper.pdf">One-dimensional Adapter to Rule Them All: Concepts Diffusion Models<br />and Erasing Applications</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Lyu_One-dimensional_Adapter_to_Rule_Them_All_Concepts_Diffusion_Models_and_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">30</td>
<td style="text-align: center;">2023-11-28</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Sarkar_Shadows_Dont_Lie_and_Lines_Cant_Bend_Generative_Models_dont_CVPR_2024_paper.pdf">Shadows Don't Lie and Lines Can't Bend! Generative Models<br />don't know Projective Geometry...for now</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Sarkar_Shadows_Dont_Lie_and_Lines_Cant_Bend_Generative_Models_dont_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">30</td>
<td style="text-align: center;">2023-12-12</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Zhang_EditGuard_Versatile_Image_Watermarking_for_Tamper_Localization_and_Copyright_Protection_CVPR_2024_paper.pdf">EditGuard: Versatile Image Watermarking for Tamper Localization and Copyright<br />Protection</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Zhang_EditGuard_Versatile_Image_Watermarking_for_Tamper_Localization_and_Copyright_Protection_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">30</td>
<td style="text-align: center;">2024-01-17</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Wu_Consistent3D_Towards_Consistent_High-Fidelity_Text-to-3D_Generation_with_Deterministic_Sampling_Prior_CVPR_2024_paper.pdf">Consistent3D: Towards Consistent High-Fidelity Text-to-3D Generation with Deterministic Sampling<br />Prior</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Wu_Consistent3D_Towards_Consistent_High-Fidelity_Text-to-3D_Generation_with_Deterministic_Sampling_Prior_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">30</td>
<td style="text-align: center;">2023-09-04</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Xiao_Can_I_Trust_Your_Answer_Visually_Grounded_Video_Question_Answering_CVPR_2024_paper.pdf">Can I Trust Your Answer? Visually Grounded Video Question<br />Answering</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Xiao_Can_I_Trust_Your_Answer_Visually_Grounded_Video_Question_Answering_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">30</td>
<td style="text-align: center;">2023-12-03</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Kwak_ViVid-1-to-3_Novel_View_Synthesis_with_Video_Diffusion_Models_CVPR_2024_paper.pdf">ViVid-1-to-3: Novel View Synthesis with Video Diffusion Models</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Kwak_ViVid-1-to-3_Novel_View_Synthesis_with_Video_Diffusion_Models_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">30</td>
<td style="text-align: center;">2024-04-05</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Xiao_SpatialTracker_Tracking_Any_2D_Pixels_in_3D_Space_CVPR_2024_paper.pdf">SpatialTracker: Tracking Any 2D Pixels in 3D Space</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Xiao_SpatialTracker_Tracking_Any_2D_Pixels_in_3D_Space_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">29</td>
<td style="text-align: center;">2023-12-27</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Liu_Forgery-aware_Adaptive_Transformer_for_Generalizable_Synthetic_Image_Detection_CVPR_2024_paper.pdf">Forgery-aware Adaptive Transformer for Generalizable Synthetic Image Detection</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Liu_Forgery-aware_Adaptive_Transformer_for_Generalizable_Synthetic_Image_Detection_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">29</td>
<td style="text-align: center;">2024-03-18</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Zhou_Expandable_Subspace_Ensemble_for_Pre-Trained_Model-Based_Class-Incremental_Learning_CVPR_2024_paper.pdf">Expandable Subspace Ensemble for Pre-Trained Model-Based Class-Incremental Learning</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Zhou_Expandable_Subspace_Ensemble_for_Pre-Trained_Model-Based_Class-Incremental_Learning_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">29</td>
<td style="text-align: center;">2023-09-26</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Wang_Event_Stream-based_Visual_Object_Tracking_A_High-Resolution_Benchmark_Dataset_and_CVPR_2024_paper.pdf">Event Stream-based Visual Object Tracking: A High-Resolution Benchmark Dataset<br />and A Novel Baseline</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Wang_Event_Stream-based_Visual_Object_Tracking_A_High-Resolution_Benchmark_Dataset_and_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">29</td>
<td style="text-align: center;">2023-12-06</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Yu_WonderJourney_Going_from_Anywhere_to_Everywhere_CVPR_2024_paper.pdf">WonderJourney: Going from Anywhere to Everywhere</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Yu_WonderJourney_Going_from_Anywhere_to_Everywhere_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">29</td>
<td style="text-align: center;">2024-02-29</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Li_DistriFusion_Distributed_Parallel_Inference_for_High-Resolution_Diffusion_Models_CVPR_2024_paper.pdf">DistriFusion: Distributed Parallel Inference for High-Resolution Diffusion Models</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Li_DistriFusion_Distributed_Parallel_Inference_for_High-Resolution_Diffusion_Models_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">29</td>
<td style="text-align: center;">2023-11-27</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Xie_SED_A_Simple_Encoder-Decoder_for_Open-Vocabulary_Semantic_Segmentation_CVPR_2024_paper.pdf">SED: A Simple Encoder-Decoder for Open-Vocabulary Semantic Segmentation</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Xie_SED_A_Simple_Encoder-Decoder_for_Open-Vocabulary_Semantic_Segmentation_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">29</td>
<td style="text-align: center;">2024-03-04</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Guo_RegionGPT_Towards_Region_Understanding_Vision_Language_Model_CVPR_2024_paper.pdf">RegionGPT: Towards Region Understanding Vision Language Model</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Guo_RegionGPT_Towards_Region_Understanding_Vision_Language_Model_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">29</td>
<td style="text-align: center;">2024-03-11</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Zhang_FreGS_3D_Gaussian_Splatting_with_Progressive_Frequency_Regularization_CVPR_2024_paper.pdf">FreGS: 3D Gaussian Splatting with Progressive Frequency Regularization</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Zhang_FreGS_3D_Gaussian_Splatting_with_Progressive_Frequency_Regularization_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">28</td>
<td style="text-align: center;">2023-11-26</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Bai_BadCLIP_Trigger-Aware_Prompt_Learning_for_Backdoor_Attacks_on_CLIP_CVPR_2024_paper.pdf">BadCLIP: Trigger-Aware Prompt Learning for Backdoor Attacks on CLIP</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Bai_BadCLIP_Trigger-Aware_Prompt_Learning_for_Backdoor_Attacks_on_CLIP_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">28</td>
<td style="text-align: center;">2023-12-29</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Yang_Visual_Point_Cloud_Forecasting_enables_Scalable_Autonomous_Driving_CVPR_2024_paper.pdf">Visual Point Cloud Forecasting enables Scalable Autonomous Driving</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Yang_Visual_Point_Cloud_Forecasting_enables_Scalable_Autonomous_Driving_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">28</td>
<td style="text-align: center;">2023-12-07</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Kara_RAVE_Randomized_Noise_Shuffling_for_Fast_and_Consistent_Video_Editing_CVPR_2024_paper.pdf">RAVE: Randomized Noise Shuffling for Fast and Consistent Video<br />Editing with Diffusion Models</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Kara_RAVE_Randomized_Noise_Shuffling_for_Fast_and_Consistent_Video_Editing_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">28</td>
<td style="text-align: center;">2024-02-29</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Chen_Towards_Generalizable_Tumor_Synthesis_CVPR_2024_paper.pdf">Towards Generalizable Tumor Synthesis</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Chen_Towards_Generalizable_Tumor_Synthesis_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">28</td>
<td style="text-align: center;">2023-12-06</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Cui_On_the_Robustness_of_Large_Multimodal_Models_Against_Image_Adversarial_CVPR_2024_paper.pdf">On the Robustness of Large Multimodal Models Against Image<br />Adversarial Attacks</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Cui_On_the_Robustness_of_Large_Multimodal_Models_Against_Image_Adversarial_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">28</td>
<td style="text-align: center;">2024-02-14</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Hu_OmniMedVQA_A_New_Large-Scale_Comprehensive_Evaluation_Benchmark_for_Medical_LVLM_CVPR_2024_paper.pdf">OmniMedVQA: A New Large-Scale Comprehensive Evaluation Benchmark for Medical<br />LVLM</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Hu_OmniMedVQA_A_New_Large-Scale_Comprehensive_Evaluation_Benchmark_for_Medical_LVLM_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">28</td>
<td style="text-align: center;">2023-12-19</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Yuan_InstructVideo_Instructing_Video_Diffusion_Models_with_Human_Feedback_CVPR_2024_paper.pdf">InstructVideo: Instructing Video Diffusion Models with Human Feedback</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Yuan_InstructVideo_Instructing_Video_Diffusion_Models_with_Human_Feedback_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">28</td>
<td style="text-align: center;">2023-12-07</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Ma_LaMPilot_An_Open_Benchmark_Dataset_for_Autonomous_Driving_with_Language_CVPR_2024_paper.pdf">LaMPilot: An Open Benchmark Dataset for Autonomous Driving with<br />Language Model Programs</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Ma_LaMPilot_An_Open_Benchmark_Dataset_for_Autonomous_Driving_with_Language_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">28</td>
<td style="text-align: center;">2023-12-14</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Urbanek_A_Picture_is_Worth_More_Than_77_Text_Tokens_Evaluating_CVPR_2024_paper.pdf">A Picture is Worth More Than 77 Text Tokens:<br />Evaluating CLIP-Style Models on Dense Captions</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Urbanek_A_Picture_is_Worth_More_Than_77_Text_Tokens_Evaluating_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">27</td>
<td style="text-align: center;">2023-12-06</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Jiang_HiFi4G_High-Fidelity_Human_Performance_Rendering_via_Compact_Gaussian_Splatting_CVPR_2024_paper.pdf">HiFi4G: High-Fidelity Human Performance Rendering via Compact Gaussian Splatting</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Jiang_HiFi4G_High-Fidelity_Human_Performance_Rendering_via_Compact_Gaussian_Splatting_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">27</td>
<td style="text-align: center;">2024-02-06</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Kong_EscherNet_A_Generative_Model_for_Scalable_View_Synthesis_CVPR_2024_paper.pdf">EscherNet: A Generative Model for Scalable View Synthesis</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Kong_EscherNet_A_Generative_Model_for_Scalable_View_Synthesis_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">27</td>
<td style="text-align: center;">2023-11-20</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Chen_LION_Empowering_Multimodal_Large_Language_Model_with_Dual-Level_Visual_Knowledge_CVPR_2024_paper.pdf">LION: Empowering Multimodal Large Language Model with Dual-Level Visual<br />Knowledge</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Chen_LION_Empowering_Multimodal_Large_Language_Model_with_Dual-Level_Visual_Knowledge_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">27</td>
<td style="text-align: center;">2024-03-04</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Hollein_ViewDiff_3D-Consistent_Image_Generation_with_Text-to-Image_Models_CVPR_2024_paper.pdf">ViewDiff: 3D-Consistent Image Generation with Text-to-Image Models</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Hollein_ViewDiff_3D-Consistent_Image_Generation_with_Text-to-Image_Models_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">27</td>
<td style="text-align: center;">2024-03-06</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Ma_Hierarchical_Diffusion_Policy_for_Kinematics-Aware_Multi-Task_Robotic_Manipulation_CVPR_2024_paper.pdf">Hierarchical Diffusion Policy for Kinematics-Aware Multi-Task Robotic Manipulation</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Ma_Hierarchical_Diffusion_Policy_for_Kinematics-Aware_Multi-Task_Robotic_Manipulation_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">27</td>
<td style="text-align: center;">2023-12-14</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Wu_General_Object_Foundation_Model_for_Images_and_Videos_at_Scale_CVPR_2024_paper.pdf">General Object Foundation Model for Images and Videos at<br />Scale</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Wu_General_Object_Foundation_Model_for_Images_and_Videos_at_Scale_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">27</td>
<td style="text-align: center;">2023-11-19</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Yan_Transcending_Forgery_Specificity_with_Latent_Space_Augmentation_for_Generalizable_Deepfake_CVPR_2024_paper.pdf">Transcending Forgery Specificity with Latent Space Augmentation for Generalizable<br />Deepfake Detection</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Yan_Transcending_Forgery_Specificity_with_Latent_Space_Augmentation_for_Generalizable_Deepfake_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">27</td>
<td style="text-align: center;">2023-12-06</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Sun_On_the_Diversity_and_Realism_of_Distilled_Dataset_An_Efficient_CVPR_2024_paper.pdf">On the Diversity and Realism of Distilled Dataset: An<br />Efficient Dataset Distillation Paradigm</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Sun_On_the_Diversity_and_Realism_of_Distilled_Dataset_An_Efficient_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">27</td>
<td style="text-align: center;">2023-12-19</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Karunratanakul_Optimizing_Diffusion_Noise_Can_Serve_As_Universal_Motion_Priors_CVPR_2024_paper.pdf">Optimizing Diffusion Noise Can Serve As Universal Motion Priors</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Karunratanakul_Optimizing_Diffusion_Noise_Can_Serve_As_Universal_Motion_Priors_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">27</td>
<td style="text-align: center;">2023-09-06</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Ryu_Diffusion-EDFs_Bi-equivariant_Denoising_Generative_Modeling_on_SE3_for_Visual_Robotic_CVPR_2024_paper.pdf">Diffusion-EDFs: Bi-equivariant Denoising Generative Modeling on SE(3) for Visual<br />Robotic Manipulation</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Ryu_Diffusion-EDFs_Bi-equivariant_Denoising_Generative_Modeling_on_SE3_for_Visual_Robotic_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">27</td>
<td style="text-align: center;">2023-12-13</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Zhang_Towards_Text-guided_3D_Scene_Composition_CVPR_2024_paper.pdf">Towards Text-guided 3D Scene Composition</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Zhang_Towards_Text-guided_3D_Scene_Composition_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">27</td>
<td style="text-align: center;">2023-11-27</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Lu_Direct2.5_Diverse_Text-to-3D_Generation_via_Multi-view_2.5D_Diffusion_CVPR_2024_paper.pdf">Direct2.5: Diverse Text-to-3D Generation via Multi-view 2.5D Diffusion</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Lu_Direct2.5_Diverse_Text-to-3D_Generation_via_Multi-view_2.5D_Diffusion_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">27</td>
<td style="text-align: center;">2023-11-28</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Yatim_Space-Time_Diffusion_Features_for_Zero-Shot_Text-Driven_Motion_Transfer_CVPR_2024_paper.pdf">Space-Time Diffusion Features for Zero-Shot Text-Driven Motion Transfer</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Yatim_Space-Time_Diffusion_Features_for_Zero-Shot_Text-Driven_Motion_Transfer_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">27</td>
<td style="text-align: center;">2023-11-28</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Zhu_LLaFS_When_Large_Language_Models_Meet_Few-Shot_Segmentation_CVPR_2024_paper.pdf">LLaFS: When Large Language Models Meet Few-Shot Segmentation</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Zhu_LLaFS_When_Large_Language_Models_Meet_Few-Shot_Segmentation_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">26</td>
<td style="text-align: center;">2023-09-01</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Xie_CityDreamer_Compositional_Generative_Model_of_Unbounded_3D_Cities_CVPR_2024_paper.pdf">CityDreamer: Compositional Generative Model of Unbounded 3D Cities</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Xie_CityDreamer_Compositional_Generative_Model_of_Unbounded_3D_Cities_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">26</td>
<td style="text-align: center;">2023-12-21</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Zhang_PIA_Your_Personalized_Image_Animator_via_Plug-and-Play_Modules_in_Text-to-Image_CVPR_2024_paper.pdf">PIA: Your Personalized Image Animator via Plug-and-Play Modules in<br />Text-to-Image Models</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Zhang_PIA_Your_Personalized_Image_Animator_via_Plug-and-Play_Modules_in_Text-to-Image_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">26</td>
<td style="text-align: center;">2023-12-01</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Rout_Beyond_First-Order_Tweedie_Solving_Inverse_Problems_using_Latent_Diffusion_CVPR_2024_paper.pdf">Beyond First-Order Tweedie: Solving Inverse Problems using Latent Diffusion</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Rout_Beyond_First-Order_Tweedie_Solving_Inverse_Problems_using_Latent_Diffusion_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">26</td>
<td style="text-align: center;">2023-11-27</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Wu_Self-correcting_LLM-controlled_Diffusion_Models_CVPR_2024_paper.pdf">Self-correcting LLM-controlled Diffusion Models</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Wu_Self-correcting_LLM-controlled_Diffusion_Models_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">26</td>
<td style="text-align: center;">2024-04-10</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Goyal_Scaling_Laws_for_Data_Filtering--_Data_Curation_cannot_be_Compute_CVPR_2024_paper.pdf">Scaling Laws for Data Filtering-- Data Curation cannot be<br />Compute Agnostic</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Goyal_Scaling_Laws_for_Data_Filtering--_Data_Curation_cannot_be_Compute_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">26</td>
<td style="text-align: center;">2023-12-17</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Li_VidToMe_Video_Token_Merging_for_Zero-Shot_Video_Editing_CVPR_2024_paper.pdf">VidToMe: Video Token Merging for Zero-Shot Video Editing</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Li_VidToMe_Video_Token_Merging_for_Zero-Shot_Video_Editing_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">26</td>
<td style="text-align: center;">2024-03-19</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Wang_Real-IAD_A_Real-World_Multi-View_Dataset_for_Benchmarking_Versatile_Industrial_Anomaly_CVPR_2024_paper.pdf">Real-IAD: A Real-World Multi-View Dataset for Benchmarking Versatile Industrial<br />Anomaly Detection</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Wang_Real-IAD_A_Real-World_Multi-View_Dataset_for_Benchmarking_Versatile_Industrial_Anomaly_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">26</td>
<td style="text-align: center;">2023-12-04</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Gu_VideoSwap_Customized_Video_Subject_Swapping_with_Interactive_Semantic_Point_Correspondence_CVPR_2024_paper.pdf">VideoSwap: Customized Video Subject Swapping with Interactive Semantic Point<br />Correspondence</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Gu_VideoSwap_Customized_Video_Subject_Swapping_with_Interactive_Semantic_Point_Correspondence_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">26</td>
<td style="text-align: center;">2023-12-15</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Xia_GSVA_Generalized_Segmentation_via_Multimodal_Large_Language_Models_CVPR_2024_paper.pdf">GSVA: Generalized Segmentation via Multimodal Large Language Models</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Xia_GSVA_Generalized_Segmentation_via_Multimodal_Large_Language_Models_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">26</td>
<td style="text-align: center;">2023-12-07</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Wei_Stronger_Fewer__Superior_Harnessing_Vision_Foundation_Models_for_Domain_CVPR_2024_paper.pdf">Stronger Fewer &amp; Superior: Harnessing Vision Foundation Models for<br />Domain Generalized Semantic Segmentation</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Wei_Stronger_Fewer__Superior_Harnessing_Vision_Foundation_Models_for_Domain_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">26</td>
<td style="text-align: center;">2024-01-23</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Parashar_The_Neglected_Tails_in_Vision-Language_Models_CVPR_2024_paper.pdf">The Neglected Tails in Vision-Language Models</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Parashar_The_Neglected_Tails_in_Vision-Language_Models_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">26</td>
<td style="text-align: center;">2023-11-23</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Nguyen_GigaPose_Fast_and_Robust_Novel_Object_Pose_Estimation_via_One_CVPR_2024_paper.pdf">GigaPose: Fast and Robust Novel Object Pose Estimation via<br />One Correspondence</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Nguyen_GigaPose_Fast_and_Robust_Novel_Object_Pose_Estimation_via_One_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">25</td>
<td style="text-align: center;">2023-12-17</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Yin_SAI3D_Segment_Any_Instance_in_3D_Scenes_CVPR_2024_paper.pdf">SAI3D: Segment Any Instance in 3D Scenes</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Yin_SAI3D_Segment_Any_Instance_in_3D_Scenes_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">25</td>
<td style="text-align: center;">2023-12-04</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Zheng_Towards_Learning_a_Generalist_Model_for_Embodied_Navigation_CVPR_2024_paper.pdf">Towards Learning a Generalist Model for Embodied Navigation</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Zheng_Towards_Learning_a_Generalist_Model_for_Embodied_Navigation_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">25</td>
<td style="text-align: center;">2024-03-13</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Jiang_Scaling_Up_Dynamic_Human-Scene_Interaction_Modeling_CVPR_2024_paper.pdf">Scaling Up Dynamic Human-Scene Interaction Modeling</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Jiang_Scaling_Up_Dynamic_Human-Scene_Interaction_Modeling_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">25</td>
<td style="text-align: center;">2024-02-26</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Zhang_GROUNDHOG_Grounding_Large_Language_Models_to_Holistic_Segmentation_CVPR_2024_paper.pdf">GROUNDHOG: Grounding Large Language Models to Holistic Segmentation</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Zhang_GROUNDHOG_Grounding_Large_Language_Models_to_Holistic_Segmentation_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">25</td>
<td style="text-align: center;">2023-12-01</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Bousselham_Grounding_Everything_Emerging_Localization_Properties_in_Vision-Language_Transformers_CVPR_2024_paper.pdf">Grounding Everything: Emerging Localization Properties in Vision-Language Transformers</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Bousselham_Grounding_Everything_Emerging_Localization_Properties_in_Vision-Language_Transformers_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">25</td>
<td style="text-align: center;">2024-04-08</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Luo_LayoutLLM_Layout_Instruction_Tuning_with_Large_Language_Models_for_Document_CVPR_2024_paper.pdf">LayoutLLM: Layout Instruction Tuning with Large Language Models for<br />Document Understanding</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Luo_LayoutLLM_Layout_Instruction_Tuning_with_Large_Language_Models_for_Document_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">25</td>
<td style="text-align: center;">2023-06-07</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Kim_WOUAF_Weight_Modulation_for_User_Attribution_and_Fingerprinting_in_Text-to-Image_CVPR_2024_paper.pdf">WOUAF: Weight Modulation for User Attribution and Fingerprinting in<br />Text-to-Image Diffusion Models</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Kim_WOUAF_Weight_Modulation_for_User_Attribution_and_Fingerprinting_in_Text-to-Image_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">25</td>
<td style="text-align: center;">2023-12-14</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Li_Auto_MC-Reward_Automated_Dense_Reward_Design_with_Large_Language_Models_CVPR_2024_paper.pdf">Auto MC-Reward: Automated Dense Reward Design with Large Language<br />Models for Minecraft</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Li_Auto_MC-Reward_Automated_Dense_Reward_Design_with_Large_Language_Models_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">25</td>
<td style="text-align: center;">2023-12-07</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Wang_VGGSfM_Visual_Geometry_Grounded_Deep_Structure_From_Motion_CVPR_2024_paper.pdf">VGGSfM: Visual Geometry Grounded Deep Structure From Motion</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Wang_VGGSfM_Visual_Geometry_Grounded_Deep_Structure_From_Motion_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">24</td>
<td style="text-align: center;">2023-11-29</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Hudson_SODA_Bottleneck_Diffusion_Models_for_Representation_Learning_CVPR_2024_paper.pdf">SODA: Bottleneck Diffusion Models for Representation Learning</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Hudson_SODA_Bottleneck_Diffusion_Models_for_Representation_Learning_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">24</td>
<td style="text-align: center;">2024-04-15</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Tang_SparseOcc_Rethinking_Sparse_Latent_Representation_for_Vision-Based_Semantic_Occupancy_Prediction_CVPR_2024_paper.pdf">SparseOcc: Rethinking Sparse Latent Representation for Vision-Based Semantic Occupancy<br />Prediction</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Tang_SparseOcc_Rethinking_Sparse_Latent_Representation_for_Vision-Based_Semantic_Occupancy_Prediction_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">24</td>
<td style="text-align: center;">2024-02-27</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Lin_Preserving_Fairness_Generalization_in_Deepfake_Detection_CVPR_2024_paper.pdf">Preserving Fairness Generalization in Deepfake Detection</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Lin_Preserving_Fairness_Generalization_in_Deepfake_Detection_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">24</td>
<td style="text-align: center;">2023-09-27</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Liu_BT-Adapter_Video_Conversation_is_Feasible_Without_Video_Instruction_Tuning_CVPR_2024_paper.pdf">BT-Adapter: Video Conversation is Feasible Without Video Instruction Tuning</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Liu_BT-Adapter_Video_Conversation_is_Feasible_Without_Video_Instruction_Tuning_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">24</td>
<td style="text-align: center;">2023-12-12</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Qin_MP5_A_Multi-modal_Open-ended_Embodied_System_in_Minecraft_via_Active_CVPR_2024_paper.pdf">MP5: A Multi-modal Open-ended Embodied System in Minecraft via<br />Active Perception</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Qin_MP5_A_Multi-modal_Open-ended_Embodied_System_in_Minecraft_via_Active_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">24</td>
<td style="text-align: center;">2024-03-14</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Hong_OneTracker_Unifying_Visual_Object_Tracking_with_Foundation_Models_and_Efficient_CVPR_2024_paper.pdf">OneTracker: Unifying Visual Object Tracking with Foundation Models and<br />Efficient Tuning</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Hong_OneTracker_Unifying_Visual_Object_Tracking_with_Foundation_Models_and_Efficient_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">24</td>
<td style="text-align: center;">2024-02-04</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Mou_DiffEditor_Boosting_Accuracy_and_Flexibility_on_Diffusion-based_Image_Editing_CVPR_2024_paper.pdf">DiffEditor: Boosting Accuracy and Flexibility on Diffusion-based Image Editing</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Mou_DiffEditor_Boosting_Accuracy_and_Flexibility_on_Diffusion-based_Image_Editing_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">24</td>
<td style="text-align: center;">2024-02-15</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Nam_DreamMatcher_Appearance_Matching_Self-Attention_for_Semantically-Consistent_Text-to-Image_Personalization_CVPR_2024_paper.pdf">DreamMatcher: Appearance Matching Self-Attention for Semantically-Consistent Text-to-Image Personalization</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Nam_DreamMatcher_Appearance_Matching_Self-Attention_for_Semantically-Consistent_Text-to-Image_Personalization_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">24</td>
<td style="text-align: center;">2024-03-11</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Zhu_Toward_Generalist_Anomaly_Detection_via_In-context_Residual_Learning_with_Few-shot_CVPR_2024_paper.pdf">Toward Generalist Anomaly Detection via In-context Residual Learning with<br />Few-shot Sample Prompts</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Zhu_Toward_Generalist_Anomaly_Detection_via_In-context_Residual_Learning_with_Few-shot_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">24</td>
<td style="text-align: center;">2024-01-11</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Xiong_Efficient_Deformable_ConvNets_Rethinking_Dynamic_and_Sparse_Operator_for_Vision_CVPR_2024_paper.pdf">Efficient Deformable ConvNets: Rethinking Dynamic and Sparse Operator for<br />Vision Applications</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Xiong_Efficient_Deformable_ConvNets_Rethinking_Dynamic_and_Sparse_Operator_for_Vision_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">24</td>
<td style="text-align: center;">2023-11-28</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Feng_Ranni_Taming_Text-to-Image_Diffusion_for_Accurate_Instruction_Following_CVPR_2024_paper.pdf">Ranni: Taming Text-to-Image Diffusion for Accurate Instruction Following</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Feng_Ranni_Taming_Text-to-Image_Diffusion_for_Accurate_Instruction_Following_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">24</td>
<td style="text-align: center;">2023-11-29</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Peng_SyncTalk_The_Devil_is_in_the_Synchronization_for_Talking_Head_CVPR_2024_paper.pdf">SyncTalk: The Devil is in the Synchronization for Talking<br />Head Synthesis</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Peng_SyncTalk_The_Devil_is_in_the_Synchronization_for_Talking_Head_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">24</td>
<td style="text-align: center;">2023-12-07</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Chen_GenTron_Diffusion_Transformers_for_Image_and_Video_Generation_CVPR_2024_paper.pdf">GenTron: Diffusion Transformers for Image and Video Generation</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Chen_GenTron_Diffusion_Transformers_for_Image_and_Video_Generation_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">24</td>
<td style="text-align: center;">2024-04-15</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Yang_PhyScene_Physically_Interactable_3D_Scene_Synthesis_for_Embodied_AI_CVPR_2024_paper.pdf">PhyScene: Physically Interactable 3D Scene Synthesis for Embodied AI</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Yang_PhyScene_Physically_Interactable_3D_Scene_Synthesis_for_Embodied_AI_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">24</td>
<td style="text-align: center;">2023-11-28</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Zhao_Wavelet-based_Fourier_Information_Interaction_with_Frequency_Diffusion_Adjustment_for_Underwater_CVPR_2024_paper.pdf">Wavelet-based Fourier Information Interaction with Frequency Diffusion Adjustment for<br />Underwater Image Restoration</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Zhao_Wavelet-based_Fourier_Information_Interaction_with_Frequency_Diffusion_Adjustment_for_Underwater_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">23</td>
<td style="text-align: center;">2024-01-17</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Yeh_TextureDreamer_Image-Guided_Texture_Synthesis_Through_Geometry-Aware_Diffusion_CVPR_2024_paper.pdf">TextureDreamer: Image-Guided Texture Synthesis Through Geometry-Aware Diffusion</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Yeh_TextureDreamer_Image-Guided_Texture_Synthesis_Through_Geometry-Aware_Diffusion_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">23</td>
<td style="text-align: center;">2023-12-07</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Qing_Hierarchical_Spatio-temporal_Decoupling_for_Text-to-Video_Generation_CVPR_2024_paper.pdf">Hierarchical Spatio-temporal Decoupling for Text-to-Video Generation</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Qing_Hierarchical_Spatio-temporal_Decoupling_for_Text-to-Video_Generation_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">23</td>
<td style="text-align: center;">2023-12-05</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Zhong_Lets_Think_Outside_the_Box_Exploring_Leap-of-Thought_in_Large_Language_CVPR_2024_paper.pdf">Let's Think Outside the Box: Exploring Leap-of-Thought in Large<br />Language Models with Creative Humor Generation</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Zhong_Lets_Think_Outside_the_Box_Exploring_Leap-of-Thought_in_Large_Language_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">23</td>
<td style="text-align: center;">2024-01-17</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Zhuang_Vlogger_Make_Your_Dream_A_Vlog_CVPR_2024_paper.pdf">Vlogger: Make Your Dream A Vlog</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Zhuang_Vlogger_Make_Your_Dream_A_Vlog_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">23</td>
<td style="text-align: center;">2023-12-18</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Liang_SkillDiffuser_Interpretable_Hierarchical_Planning_via_Skill_Abstractions_in_Diffusion-Based_Task_CVPR_2024_paper.pdf">SkillDiffuser: Interpretable Hierarchical Planning via Skill Abstractions in Diffusion-Based<br />Task Execution</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Liang_SkillDiffuser_Interpretable_Hierarchical_Planning_via_Skill_Abstractions_in_Diffusion-Based_Task_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">23</td>
<td style="text-align: center;">2023-12-13</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Aneja_FaceTalk_Audio-Driven_Motion_Diffusion_for_Neural_Parametric_Head_Models_CVPR_2024_paper.pdf">FaceTalk: Audio-Driven Motion Diffusion for Neural Parametric Head Models</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Aneja_FaceTalk_Audio-Driven_Motion_Diffusion_for_Neural_Parametric_Head_Models_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">23</td>
<td style="text-align: center;">2024-02-23</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Zubic_State_Space_Models_for_Event_Cameras_CVPR_2024_paper.pdf">State Space Models for Event Cameras</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Zubic_State_Space_Models_for_Event_Cameras_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">23</td>
<td style="text-align: center;">2023-12-04</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Ma_COTR_Compact_Occupancy_TRansformer_for_Vision-based_3D_Occupancy_Prediction_CVPR_2024_paper.pdf">COTR: Compact Occupancy TRansformer for Vision-based 3D Occupancy Prediction</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Ma_COTR_Compact_Occupancy_TRansformer_for_Vision-based_3D_Occupancy_Prediction_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">23</td>
<td style="text-align: center;">2023-09-29</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Kondapaneni_Text-Image_Alignment_for_Diffusion-Based_Perception_CVPR_2024_paper.pdf">Text-Image Alignment for Diffusion-Based Perception</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Kondapaneni_Text-Image_Alignment_for_Diffusion-Based_Perception_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">23</td>
<td style="text-align: center;">2023-08-20</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Wang_Boosting_Adversarial_Transferability_by_Block_Shuffle_and_Rotation_CVPR_2024_paper.pdf">Boosting Adversarial Transferability by Block Shuffle and Rotation</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Wang_Boosting_Adversarial_Transferability_by_Block_Shuffle_and_Rotation_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">23</td>
<td style="text-align: center;">2023-09-12</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Liu_Language_Models_as_Black-Box_Optimizers_for_Vision-Language_Models_CVPR_2024_paper.pdf">Language Models as Black-Box Optimizers for Vision-Language Models</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Liu_Language_Models_as_Black-Box_Optimizers_for_Vision-Language_Models_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">23</td>
<td style="text-align: center;">2024-04-30</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Potje_XFeat_Accelerated_Features_for_Lightweight_Image_Matching_CVPR_2024_paper.pdf">XFeat: Accelerated Features for Lightweight Image Matching</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Potje_XFeat_Accelerated_Features_for_Lightweight_Image_Matching_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">23</td>
<td style="text-align: center;">2023-11-22</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Li_Visual_In-Context_Prompting_CVPR_2024_paper.pdf">Visual In-Context Prompting</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Li_Visual_In-Context_Prompting_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">23</td>
<td style="text-align: center;">2023-11-28</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Vasu_MobileCLIP_Fast_Image-Text_Models_through_Multi-Modal_Reinforced_Training_CVPR_2024_paper.pdf">MobileCLIP: Fast Image-Text Models through Multi-Modal Reinforced Training</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Vasu_MobileCLIP_Fast_Image-Text_Models_through_Multi-Modal_Reinforced_Training_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">23</td>
<td style="text-align: center;">2023-05-19</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Zhao_Equivariant_Multi-Modality_Image_Fusion_CVPR_2024_paper.pdf">Equivariant Multi-Modality Image Fusion</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Zhao_Equivariant_Multi-Modality_Image_Fusion_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">23</td>
<td style="text-align: center;">2024-03-06</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Liu_Towards_Understanding_Cross_and_Self-Attention_in_Stable_Diffusion_for_Text-Guided_CVPR_2024_paper.pdf">Towards Understanding Cross and Self-Attention in Stable Diffusion for<br />Text-Guided Image Editing</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Liu_Towards_Understanding_Cross_and_Self-Attention_in_Stable_Diffusion_for_Text-Guided_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">23</td>
<td style="text-align: center;">2024-04-07</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Yang_Gaussian_Shading_Provable_Performance-Lossless_Image_Watermarking_for_Diffusion_Models_CVPR_2024_paper.pdf">Gaussian Shading: Provable Performance-Lossless Image Watermarking for Diffusion Models</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Yang_Gaussian_Shading_Provable_Performance-Lossless_Image_Watermarking_for_Diffusion_Models_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">23</td>
<td style="text-align: center;">2023-06-27</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/He_Detector-Free_Structure_from_Motion_CVPR_2024_paper.pdf">Detector-Free Structure from Motion</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/He_Detector-Free_Structure_from_Motion_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">23</td>
<td style="text-align: center;">2023-12-12</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Sun_CLIP_as_RNN_Segment_Countless_Visual_Concepts_without_Training_Endeavor_CVPR_2024_paper.pdf">CLIP as RNN: Segment Countless Visual Concepts without Training<br />Endeavor</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Sun_CLIP_as_RNN_Segment_Countless_Visual_Concepts_without_Training_Endeavor_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">22</td>
<td style="text-align: center;">2024-03-01</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Bae_Rethinking_Inductive_Biases_for_Surface_Normal_Estimation_CVPR_2024_paper.pdf">Rethinking Inductive Biases for Surface Normal Estimation</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Bae_Rethinking_Inductive_Biases_for_Surface_Normal_Estimation_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">22</td>
<td style="text-align: center;">2023-12-05</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Qi_GPT4Point_A_Unified_Framework_for_Point-Language_Understanding_and_Generation_CVPR_2024_paper.pdf">GPT4Point: A Unified Framework for Point-Language Understanding and Generation</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Qi_GPT4Point_A_Unified_Framework_for_Point-Language_Understanding_and_Generation_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">22</td>
<td style="text-align: center;">2024-02-07</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Kant_SPAD_Spatially_Aware_Multi-View_Diffusers_CVPR_2024_paper.pdf">SPAD: Spatially Aware Multi-View Diffusers</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Kant_SPAD_Spatially_Aware_Multi-View_Diffusers_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">22</td>
<td style="text-align: center;">2023-12-03</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Xiang_FlashAvatar_High-fidelity_Head_Avatar_with_Efficient_Gaussian_Embedding_CVPR_2024_paper.pdf">FlashAvatar: High-fidelity Head Avatar with Efficient Gaussian Embedding</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Xiang_FlashAvatar_High-fidelity_Head_Avatar_with_Efficient_Gaussian_Embedding_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">22</td>
<td style="text-align: center;">2023-12-19</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Liu_Rotated_Multi-Scale_Interaction_Network_for_Referring_Remote_Sensing_Image_Segmentation_CVPR_2024_paper.pdf">Rotated Multi-Scale Interaction Network for Referring Remote Sensing Image<br />Segmentation</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Liu_Rotated_Multi-Scale_Interaction_Network_for_Referring_Remote_Sensing_Image_Segmentation_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">22</td>
<td style="text-align: center;">2024-03-25</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Lin_RCBEVDet_Radar-camera_Fusion_in_Birds_Eye_View_for_3D_Object_CVPR_2024_paper.pdf">RCBEVDet: Radar-camera Fusion in Bird's Eye View for 3D<br />Object Detection</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Lin_RCBEVDet_Radar-camera_Fusion_in_Birds_Eye_View_for_3D_Object_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">22</td>
<td style="text-align: center;">2023-12-10</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Ranzinger_AM-RADIO_Agglomerative_Vision_Foundation_Model_Reduce_All_Domains_Into_One_CVPR_2024_paper.pdf">AM-RADIO: Agglomerative Vision Foundation Model Reduce All Domains Into<br />One</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Ranzinger_AM-RADIO_Agglomerative_Vision_Foundation_Model_Reduce_All_Domains_Into_One_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">22</td>
<td style="text-align: center;">2024-03-01</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Wang_Selective-Stereo_Adaptive_Frequency_Information_Selection_for_Stereo_Matching_CVPR_2024_paper.pdf">Selective-Stereo: Adaptive Frequency Information Selection for Stereo Matching</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Wang_Selective-Stereo_Adaptive_Frequency_Information_Selection_for_Stereo_Matching_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">22</td>
<td style="text-align: center;">2024-03-30</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Liang_InfLoRA_Interference-Free_Low-Rank_Adaptation_for_Continual_Learning_CVPR_2024_paper.pdf">InfLoRA: Interference-Free Low-Rank Adaptation for Continual Learning</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Liang_InfLoRA_Interference-Free_Low-Rank_Adaptation_for_Continual_Learning_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">22</td>
<td style="text-align: center;">2023-12-06</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Zhang_AVID_Any-Length_Video_Inpainting_with_Diffusion_Model_CVPR_2024_paper.pdf">AVID: Any-Length Video Inpainting with Diffusion Model</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Zhang_AVID_Any-Length_Video_Inpainting_with_Diffusion_Model_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">22</td>
<td style="text-align: center;">2023-12-18</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Jiang_SCEdit_Efficient_and_Controllable_Image_Diffusion_Generation_via_Skip_Connection_CVPR_2024_paper.pdf">SCEdit: Efficient and Controllable Image Diffusion Generation via Skip<br />Connection Editing</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Jiang_SCEdit_Efficient_and_Controllable_Image_Diffusion_Generation_via_Skip_Connection_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">22</td>
<td style="text-align: center;">2023-12-07</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Seyfioglu_Quilt-LLaVA_Visual_Instruction_Tuning_by_Extracting_Localized_Narratives_from_Open-Source_CVPR_2024_paper.pdf">Quilt-LLaVA: Visual Instruction Tuning by Extracting Localized Narratives from<br />Open-Source Histopathology Videos</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Seyfioglu_Quilt-LLaVA_Visual_Instruction_Tuning_by_Extracting_Localized_Narratives_from_Open-Source_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">22</td>
<td style="text-align: center;">2024-05-07</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Min_DriveWorld_4D_Pre-trained_Scene_Understanding_via_World_Models_for_Autonomous_CVPR_2024_paper.pdf">DriveWorld: 4D Pre-trained Scene Understanding via World Models for<br />Autonomous Driving</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Min_DriveWorld_4D_Pre-trained_Scene_Understanding_via_World_Models_for_Autonomous_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">22</td>
<td style="text-align: center;">2024-01-03</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Ng_From_Audio_to_Photoreal_Embodiment_Synthesizing_Humans_in_Conversations_CVPR_2024_paper.pdf">From Audio to Photoreal Embodiment: Synthesizing Humans in Conversations</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Ng_From_Audio_to_Photoreal_Embodiment_Synthesizing_Humans_in_Conversations_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">22</td>
<td style="text-align: center;">2023-12-11</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Huang_EpiDiff_Enhancing_Multi-View_Synthesis_via_Localized_Epipolar-Constrained_Diffusion_CVPR_2024_paper.pdf">EpiDiff: Enhancing Multi-View Synthesis via Localized Epipolar-Constrained Diffusion</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Huang_EpiDiff_Enhancing_Multi-View_Synthesis_via_Localized_Epipolar-Constrained_Diffusion_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">22</td>
<td style="text-align: center;">2024-04-01</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Jain_Video_Interpolation_with_Diffusion_Models_CVPR_2024_paper.pdf">Video Interpolation with Diffusion Models</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Jain_Video_Interpolation_with_Diffusion_Models_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">22</td>
<td style="text-align: center;">2024-03-02</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Zimmer_TUMTraf_V2X_Cooperative_Perception_Dataset_CVPR_2024_paper.pdf">TUMTraf V2X Cooperative Perception Dataset</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Zimmer_TUMTraf_V2X_Cooperative_Perception_Dataset_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">22</td>
<td style="text-align: center;">2023-11-28</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Yang_Embodied_Multi-Modal_Agent_trained_by_an_LLM_from_a_Parallel_CVPR_2024_paper.pdf">Embodied Multi-Modal Agent trained by an LLM from a<br />Parallel TextWorld</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Yang_Embodied_Multi-Modal_Agent_trained_by_an_LLM_from_a_Parallel_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">22</td>
<td style="text-align: center;">2023-12-29</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Liang_FlowVid_Taming_Imperfect_Optical_Flows_for_Consistent_Video-to-Video_Synthesis_CVPR_2024_paper.pdf">FlowVid: Taming Imperfect Optical Flows for Consistent Video-to-Video Synthesis</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Liang_FlowVid_Taming_Imperfect_Optical_Flows_for_Consistent_Video-to-Video_Synthesis_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">22</td>
<td style="text-align: center;">2024-03-09</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Zhang_RealNet_A_Feature_Selection_Network_with_Realistic_Synthetic_Anomaly_for_CVPR_2024_paper.pdf">RealNet: A Feature Selection Network with Realistic Synthetic Anomaly<br />for Anomaly Detection</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Zhang_RealNet_A_Feature_Selection_Network_with_Realistic_Synthetic_Anomaly_for_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">22</td>
<td style="text-align: center;">2024-03-18</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Nguyen_MCD_Diverse_Large-Scale_Multi-Campus_Dataset_for_Robot_Perception_CVPR_2024_paper.pdf">MCD: Diverse Large-Scale Multi-Campus Dataset for Robot Perception</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Nguyen_MCD_Diverse_Large-Scale_Multi-Campus_Dataset_for_Robot_Perception_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">22</td>
<td style="text-align: center;">2024-03-24</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Huang_EgoExoLearn_A_Dataset_for_Bridging_Asynchronous_Ego-_and_Exo-centric_View_CVPR_2024_paper.pdf">EgoExoLearn: A Dataset for Bridging Asynchronous Ego- and Exo-centric<br />View of Procedural Activities in Real World</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Huang_EgoExoLearn_A_Dataset_for_Bridging_Asynchronous_Ego-_and_Exo-centric_View_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">21</td>
<td style="text-align: center;">2024-06-16</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Chen_VideoLLM-online_Online_Video_Large_Language_Model_for_Streaming_Video_CVPR_2024_paper.pdf">VideoLLM-online: Online Video Large Language Model for Streaming Video</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Chen_VideoLLM-online_Online_Video_Large_Language_Model_for_Streaming_Video_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">21</td>
<td style="text-align: center;">2024-01-29</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Yun_SHViT_Single-Head_Vision_Transformer_with_Memory_Efficient_Macro_Design_CVPR_2024_paper.pdf">SHViT: Single-Head Vision Transformer with Memory Efficient Macro Design</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Yun_SHViT_Single-Head_Vision_Transformer_with_Memory_Efficient_Macro_Design_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">21</td>
<td style="text-align: center;">2023-11-29</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Zhang_MM-Narrator_Narrating_Long-form_Videos_with_Multimodal_In-Context_Learning_CVPR_2024_paper.pdf">MM-Narrator: Narrating Long-form Videos with Multimodal In-Context Learning</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Zhang_MM-Narrator_Narrating_Long-form_Videos_with_Multimodal_In-Context_Learning_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">21</td>
<td style="text-align: center;">2023-12-04</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Shen_Aligning_and_Prompting_Everything_All_at_Once_for_Universal_Visual_CVPR_2024_paper.pdf">Aligning and Prompting Everything All at Once for Universal<br />Visual Perception</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Shen_Aligning_and_Prompting_Everything_All_at_Once_for_Universal_Visual_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">21</td>
<td style="text-align: center;">2024-04-04</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Zeng_WorDepth_Variational_Language_Prior_for_Monocular_Depth_Estimation_CVPR_2024_paper.pdf">WorDepth: Variational Language Prior for Monocular Depth Estimation</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Zeng_WorDepth_Variational_Language_Prior_for_Monocular_Depth_Estimation_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">21</td>
<td style="text-align: center;">2024-02-08</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Li_Driving_Everywhere_with_Large_Language_Model_Policy_Adaptation_CVPR_2024_paper.pdf">Driving Everywhere with Large Language Model Policy Adaptation</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Li_Driving_Everywhere_with_Large_Language_Model_Policy_Adaptation_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">21</td>
<td style="text-align: center;">2023-11-18</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Cai_Structure-Aware_Sparse-View_X-ray_3D_Reconstruction_CVPR_2024_paper.pdf">Structure-Aware Sparse-View X-ray 3D Reconstruction</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Cai_Structure-Aware_Sparse-View_X-ray_3D_Reconstruction_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">21</td>
<td style="text-align: center;">2023-11-28</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Zhang_Telling_Left_from_Right_Identifying_Geometry-Aware_Semantic_Correspondence_CVPR_2024_paper.pdf">Telling Left from Right: Identifying Geometry-Aware Semantic Correspondence</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Zhang_Telling_Left_from_Right_Identifying_Geometry-Aware_Semantic_Correspondence_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">21</td>
<td style="text-align: center;">2023-11-27</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Huang_TFMQ-DM_Temporal_Feature_Maintenance_Quantization_for_Diffusion_Models_CVPR_2024_paper.pdf">TFMQ-DM: Temporal Feature Maintenance Quantization for Diffusion Models</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Huang_TFMQ-DM_Temporal_Feature_Maintenance_Quantization_for_Diffusion_Models_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">21</td>
<td style="text-align: center;">2023-12-05</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Dunlap_Describing_Differences_in_Image_Sets_with_Natural_Language_CVPR_2024_paper.pdf">Describing Differences in Image Sets with Natural Language</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Dunlap_Describing_Differences_in_Image_Sets_with_Natural_Language_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">21</td>
<td style="text-align: center;">2023-05-25</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Peng_Learning_Occupancy_for_Monocular_3D_Object_Detection_CVPR_2024_paper.pdf">Learning Occupancy for Monocular 3D Object Detection</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Peng_Learning_Occupancy_for_Monocular_3D_Object_Detection_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">21</td>
<td style="text-align: center;">2023-12-08</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Schult_ControlRoom3D_Room_Generation_using_Semantic_Proxy_Rooms_CVPR_2024_paper.pdf">ControlRoom3D: Room Generation using Semantic Proxy Rooms</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Schult_ControlRoom3D_Room_Generation_using_Semantic_Proxy_Rooms_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">21</td>
<td style="text-align: center;">2023-11-30</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Feng_ChatPose_Chatting_about_3D_Human_Pose_CVPR_2024_paper.pdf">ChatPose: Chatting about 3D Human Pose</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Feng_ChatPose_Chatting_about_3D_Human_Pose_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">21</td>
<td style="text-align: center;">2024-03-07</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Wang_Efficient_LoFTR_Semi-Dense_Local_Feature_Matching_with_Sparse-Like_Speed_CVPR_2024_paper.pdf">Efficient LoFTR: Semi-Dense Local Feature Matching with Sparse-Like Speed</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Wang_Efficient_LoFTR_Semi-Dense_Local_Feature_Matching_with_Sparse-Like_Speed_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">21</td>
<td style="text-align: center;">2024-01-31</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Ricker_AEROBLADE_Training-Free_Detection_of_Latent_Diffusion_Images_Using_Autoencoder_Reconstruction_CVPR_2024_paper.pdf">AEROBLADE: Training-Free Detection of Latent Diffusion Images Using Autoencoder<br />Reconstruction Error</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Ricker_AEROBLADE_Training-Free_Detection_of_Latent_Diffusion_Images_Using_Autoencoder_Reconstruction_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">21</td>
<td style="text-align: center;">2023-12-06</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Pinyoanuntapong_MMM_Generative_Masked_Motion_Model_CVPR_2024_paper.pdf">MMM: Generative Masked Motion Model</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Pinyoanuntapong_MMM_Generative_Masked_Motion_Model_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">21</td>
<td style="text-align: center;">2024-03-15</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Xie_Autoregressive_Queries_for_Adaptive_Tracking_with_Spatio-Temporal_Transformers_CVPR_2024_paper.pdf">Autoregressive Queries for Adaptive Tracking with Spatio-Temporal Transformers</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Xie_Autoregressive_Queries_for_Adaptive_Tracking_with_Spatio-Temporal_Transformers_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">20</td>
<td style="text-align: center;">2023-12-07</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Liu_Open-Vocabulary_Segmentation_with_Semantic-Assisted_Calibration_CVPR_2024_paper.pdf">Open-Vocabulary Segmentation with Semantic-Assisted Calibration</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Liu_Open-Vocabulary_Segmentation_with_Semantic-Assisted_Calibration_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">20</td>
<td style="text-align: center;">2023-11-30</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Kirschstein_DiffusionAvatars_Deferred_Diffusion_for_High-fidelity_3D_Head_Avatars_CVPR_2024_paper.pdf">DiffusionAvatars: Deferred Diffusion for High-fidelity 3D Head Avatars</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Kirschstein_DiffusionAvatars_Deferred_Diffusion_for_High-fidelity_3D_Head_Avatars_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">20</td>
<td style="text-align: center;">2023-10-27</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Sargent_ZeroNVS_Zero-Shot_360-Degree_View_Synthesis_from_a_Single_Image_CVPR_2024_paper.pdf">ZeroNVS: Zero-Shot 360-Degree View Synthesis from a Single Image</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Sargent_ZeroNVS_Zero-Shot_360-Degree_View_Synthesis_from_a_Single_Image_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">20</td>
<td style="text-align: center;">2023-12-11</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Liu_Sherpa3D_Boosting_High-Fidelity_Text-to-3D_Generation_via_Coarse_3D_Prior_CVPR_2024_paper.pdf">Sherpa3D: Boosting High-Fidelity Text-to-3D Generation via Coarse 3D Prior</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Liu_Sherpa3D_Boosting_High-Fidelity_Text-to-3D_Generation_via_Coarse_3D_Prior_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">20</td>
<td style="text-align: center;">2023-12-02</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Pandey_Diffusion_Handles_Enabling_3D_Edits_for_Diffusion_Models_by_Lifting_CVPR_2024_paper.pdf">Diffusion Handles Enabling 3D Edits for Diffusion Models by<br />Lifting Activations to 3D</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Pandey_Diffusion_Handles_Enabling_3D_Edits_for_Diffusion_Models_by_Lifting_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">20</td>
<td style="text-align: center;">2023-12-19</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Pramanick_Jack_of_All_Tasks_Master_of_Many_Designing_General-Purpose_Coarse-to-Fine_CVPR_2024_paper.pdf">Jack of All Tasks Master of Many: Designing General-Purpose<br />Coarse-to-Fine Vision-Language Model</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Pramanick_Jack_of_All_Tasks_Master_of_Many_Designing_General-Purpose_Coarse-to-Fine_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">20</td>
<td style="text-align: center;">2023-12-28</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Li_ZONE_Zero-Shot_Instruction-Guided_Local_Editing_CVPR_2024_paper.pdf">ZONE: Zero-Shot Instruction-Guided Local Editing</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Li_ZONE_Zero-Shot_Instruction-Guided_Local_Editing_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">20</td>
<td style="text-align: center;">2024-03-29</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Luo_FairCLIP_Harnessing_Fairness_in_Vision-Language_Learning_CVPR_2024_paper.pdf">FairCLIP: Harnessing Fairness in Vision-Language Learning</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Luo_FairCLIP_Harnessing_Fairness_in_Vision-Language_Learning_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">20</td>
<td style="text-align: center;">2024-03-15</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Song_IMPRINT_Generative_Object_Compositing_by_Learning_Identity-Preserving_Representation_CVPR_2024_paper.pdf">IMPRINT: Generative Object Compositing by Learning Identity-Preserving Representation</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Song_IMPRINT_Generative_Object_Compositing_by_Learning_Identity-Preserving_Representation_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">20</td>
<td style="text-align: center;">2023-12-05</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Po_Orthogonal_Adaptation_for_Modular_Customization_of_Diffusion_Models_CVPR_2024_paper.pdf">Orthogonal Adaptation for Modular Customization of Diffusion Models</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Po_Orthogonal_Adaptation_for_Modular_Customization_of_Diffusion_Models_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">20</td>
<td style="text-align: center;">2024-01-03</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Sharma_A_Vision_Check-up_for_Language_Models_CVPR_2024_paper.pdf">A Vision Check-up for Language Models</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Sharma_A_Vision_Check-up_for_Language_Models_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">20</td>
<td style="text-align: center;">2023-11-27</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Wu_Single-Model_and_Any-Modality_for_Video_Object_Tracking_CVPR_2024_paper.pdf">Single-Model and Any-Modality for Video Object Tracking</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Wu_Single-Model_and_Any-Modality_for_Video_Object_Tracking_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">20</td>
<td style="text-align: center;">2023-11-28</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Chen_SceneTex_High-Quality_Texture_Synthesis_for_Indoor_Scenes_via_Diffusion_Priors_CVPR_2024_paper.pdf">SceneTex: High-Quality Texture Synthesis for Indoor Scenes via Diffusion<br />Priors</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Chen_SceneTex_High-Quality_Texture_Synthesis_for_Indoor_Scenes_via_Diffusion_Priors_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">20</td>
<td style="text-align: center;">2024-03-04</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Narasimhaswamy_HanDiffuser_Text-to-Image_Generation_With_Realistic_Hand_Appearances_CVPR_2024_paper.pdf">HanDiffuser: Text-to-Image Generation With Realistic Hand Appearances</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Narasimhaswamy_HanDiffuser_Text-to-Image_Generation_With_Realistic_Hand_Appearances_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">20</td>
<td style="text-align: center;">2023-11-11</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Pi_PerceptionGPT_Effectively_Fusing_Visual_Perception_into_LLM_CVPR_2024_paper.pdf">PerceptionGPT: Effectively Fusing Visual Perception into LLM</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Pi_PerceptionGPT_Effectively_Fusing_Visual_Perception_into_LLM_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">20</td>
<td style="text-align: center;">2024-02-29</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Lu_CricaVPR_Cross-image_Correlation-aware_Representation_Learning_for_Visual_Place_Recognition_CVPR_2024_paper.pdf">CricaVPR: Cross-image Correlation-aware Representation Learning for Visual Place Recognition</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Lu_CricaVPR_Cross-image_Correlation-aware_Representation_Learning_for_Visual_Place_Recognition_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">20</td>
<td style="text-align: center;">2024-03-26</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Wang_Move_as_You_Say_Interact_as_You_Can_Language-guided_Human_CVPR_2024_paper.pdf">Move as You Say Interact as You Can: Language-guided<br />Human Motion Generation with Scene Affordance</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Wang_Move_as_You_Say_Interact_as_You_Can_Language-guided_Human_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">20</td>
<td style="text-align: center;">2024-02-14</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Liso_Loopy-SLAM_Dense_Neural_SLAM_with_Loop_Closures_CVPR_2024_paper.pdf">Loopy-SLAM: Dense Neural SLAM with Loop Closures</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Liso_Loopy-SLAM_Dense_Neural_SLAM_with_Loop_Closures_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">20</td>
<td style="text-align: center;">2024-04-10</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Liu_HRVDA_High-Resolution_Visual_Document_Assistant_CVPR_2024_paper.pdf">HRVDA: High-Resolution Visual Document Assistant</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Liu_HRVDA_High-Resolution_Visual_Document_Assistant_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">20</td>
<td style="text-align: center;">2023-12-11</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Huang_DreamControl_Control-Based_Text-to-3D_Generation_with_3D_Self-Prior_CVPR_2024_paper.pdf">DreamControl: Control-Based Text-to-3D Generation with 3D Self-Prior</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Huang_DreamControl_Control-Based_Text-to-3D_Generation_with_3D_Self-Prior_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">19</td>
<td style="text-align: center;">2023-11-27</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Xia_Text2Loc_3D_Point_Cloud_Localization_from_Natural_Language_CVPR_2024_paper.pdf">Text2Loc: 3D Point Cloud Localization from Natural Language</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Xia_Text2Loc_3D_Point_Cloud_Localization_from_Natural_Language_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">19</td>
<td style="text-align: center;">2024-01-02</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Ding_Holistic_Autonomous_Driving_Understanding_by_Birds-Eye-View_Injected_Multi-Modal_Large_Models_CVPR_2024_paper.pdf">Holistic Autonomous Driving Understanding by Bird's-Eye-View Injected Multi-Modal Large<br />Models</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Ding_Holistic_Autonomous_Driving_Understanding_by_Birds-Eye-View_Injected_Multi-Modal_Large_Models_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">19</td>
<td style="text-align: center;">2023-03-25</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Rozenberszki_UnScene3D_Unsupervised_3D_Instance_Segmentation_for_Indoor_Scenes_CVPR_2024_paper.pdf">UnScene3D: Unsupervised 3D Instance Segmentation for Indoor Scenes</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Rozenberszki_UnScene3D_Unsupervised_3D_Instance_Segmentation_for_Indoor_Scenes_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">19</td>
<td style="text-align: center;">2023-11-20</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Li_DiffAvatar_Simulation-Ready_Garment_Optimization_with_Differentiable_Simulation_CVPR_2024_paper.pdf">DiffAvatar: Simulation-Ready Garment Optimization with Differentiable Simulation</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Li_DiffAvatar_Simulation-Ready_Garment_Optimization_with_Differentiable_Simulation_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">19</td>
<td style="text-align: center;">2023-12-07</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Guo_Smooth_Diffusion_Crafting_Smooth_Latent_Spaces_in_Diffusion_Models_CVPR_2024_paper.pdf">Smooth Diffusion: Crafting Smooth Latent Spaces in Diffusion Models</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Guo_Smooth_Diffusion_Crafting_Smooth_Latent_Spaces_in_Diffusion_Models_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">19</td>
<td style="text-align: center;">2023-11-30</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Tu_MotionEditor_Editing_Video_Motion_via_Content-Aware_Diffusion_CVPR_2024_paper.pdf">MotionEditor: Editing Video Motion via Content-Aware Diffusion</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Tu_MotionEditor_Editing_Video_Motion_via_Content-Aware_Diffusion_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">19</td>
<td style="text-align: center;">2023-11-20</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Ying_OmniSeg3D_Omniversal_3D_Segmentation_via_Hierarchical_Contrastive_Learning_CVPR_2024_paper.pdf">OmniSeg3D: Omniversal 3D Segmentation via Hierarchical Contrastive Learning</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Ying_OmniSeg3D_Omniversal_3D_Segmentation_via_Hierarchical_Contrastive_Learning_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">19</td>
<td style="text-align: center;">2023-08-19</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Qin_Noisy-Correspondence_Learning_for_Text-to-Image_Person_Re-identification_CVPR_2024_paper.pdf">Noisy-Correspondence Learning for Text-to-Image Person Re-identification</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Qin_Noisy-Correspondence_Learning_for_Text-to-Image_Person_Re-identification_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">19</td>
<td style="text-align: center;">2024-04-08</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Li_PromptAD_Learning_Prompts_with_only_Normal_Samples_for_Few-Shot_Anomaly_CVPR_2024_paper.pdf">PromptAD: Learning Prompts with only Normal Samples for Few-Shot<br />Anomaly Detection</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Li_PromptAD_Learning_Prompts_with_only_Normal_Samples_for_Few-Shot_Anomaly_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">19</td>
<td style="text-align: center;">2023-12-11</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Meral_CONFORM_Contrast_is_All_You_Need_for_High-Fidelity_Text-to-Image_Diffusion_CVPR_2024_paper.pdf">CONFORM: Contrast is All You Need for High-Fidelity Text-to-Image<br />Diffusion Models</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Meral_CONFORM_Contrast_is_All_You_Need_for_High-Fidelity_Text-to-Image_Diffusion_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">19</td>
<td style="text-align: center;">2023-12-20</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Silva-Rodriguez_A_Closer_Look_at_the_Few-Shot_Adaptation_of_Large_Vision-Language_CVPR_2024_paper.pdf">A Closer Look at the Few-Shot Adaptation of Large<br />Vision-Language Models</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Silva-Rodriguez_A_Closer_Look_at_the_Few-Shot_Adaptation_of_Large_Vision-Language_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">19</td>
<td style="text-align: center;">2023-12-05</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Hu_Visual_Program_Distillation_Distilling_Tools_and_Programmatic_Reasoning_into_Vision-Language_CVPR_2024_paper.pdf">Visual Program Distillation: Distilling Tools and Programmatic Reasoning into<br />Vision-Language Models</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Hu_Visual_Program_Distillation_Distilling_Tools_and_Programmatic_Reasoning_into_Vision-Language_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">19</td>
<td style="text-align: center;">2024-03-05</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Qi_SNIFFER_Multimodal_Large_Language_Model_for_Explainable_Out-of-Context_Misinformation_Detection_CVPR_2024_paper.pdf">SNIFFER: Multimodal Large Language Model for Explainable Out-of-Context Misinformation<br />Detection</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Qi_SNIFFER_Multimodal_Large_Language_Model_for_Explainable_Out-of-Context_Misinformation_Detection_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">19</td>
<td style="text-align: center;">2023-12-31</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Wang_Taming_Mode_Collapse_in_Score_Distillation_for_Text-to-3D_Generation_CVPR_2024_paper.pdf">Taming Mode Collapse in Score Distillation for Text-to-3D Generation</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Wang_Taming_Mode_Collapse_in_Score_Distillation_for_Text-to-3D_Generation_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">19</td>
<td style="text-align: center;">2023-08-22</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Vecchio_MatFuse_Controllable_Material_Generation_with_Diffusion_Models_CVPR_2024_paper.pdf">MatFuse: Controllable Material Generation with Diffusion Models</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Vecchio_MatFuse_Controllable_Material_Generation_with_Diffusion_Models_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">19</td>
<td style="text-align: center;">2023-12-26</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Woo_HarmonyView_Harmonizing_Consistency_and_Diversity_in_One-Image-to-3D_CVPR_2024_paper.pdf">HarmonyView: Harmonizing Consistency and Diversity in One-Image-to-3D</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Woo_HarmonyView_Harmonizing_Consistency_and_Diversity_in_One-Image-to-3D_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">19</td>
<td style="text-align: center;">2023-12-07</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Weber_NeRFiller_Completing_Scenes_via_Generative_3D_Inpainting_CVPR_2024_paper.pdf">NeRFiller: Completing Scenes via Generative 3D Inpainting</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Weber_NeRFiller_Completing_Scenes_via_Generative_3D_Inpainting_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">19</td>
<td style="text-align: center;">2023-12-10</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Zhang_SIFU_Side-view_Conditioned_Implicit_Function_for_Real-world_Usable_Clothed_Human_CVPR_2024_paper.pdf">SIFU: Side-view Conditioned Implicit Function for Real-world Usable Clothed<br />Human Reconstruction</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Zhang_SIFU_Side-view_Conditioned_Implicit_Function_for_Real-world_Usable_Clothed_Human_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">19</td>
<td style="text-align: center;">2023-05-31</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Shao_Control4D_Efficient_4D_Portrait_Editing_with_Text_CVPR_2024_paper.pdf">Control4D: Efficient 4D Portrait Editing with Text</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Shao_Control4D_Efficient_4D_Portrait_Editing_with_Text_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">18</td>
<td style="text-align: center;">2024-04-11</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Wen_GoMAvatar_Efficient_Animatable_Human_Modeling_from_Monocular_Video_Using_Gaussians-on-Mesh_CVPR_2024_paper.pdf">GoMAvatar: Efficient Animatable Human Modeling from Monocular Video Using<br />Gaussians-on-Mesh</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Wen_GoMAvatar_Efficient_Animatable_Human_Modeling_from_Monocular_Video_Using_Gaussians-on-Mesh_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">18</td>
<td style="text-align: center;">2024-03-08</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Noman_Rethinking_Transformers_Pre-training_for_Multi-Spectral_Satellite_Imagery_CVPR_2024_paper.pdf">Rethinking Transformers Pre-training for Multi-Spectral Satellite Imagery</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Noman_Rethinking_Transformers_Pre-training_for_Multi-Spectral_Satellite_Imagery_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">18</td>
<td style="text-align: center;">2024-03-12</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Chen_PeLK_Parameter-efficient_Large_Kernel_ConvNets_with_Peripheral_Convolution_CVPR_2024_paper.pdf">PeLK: Parameter-efficient Large Kernel ConvNets with Peripheral Convolution</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Chen_PeLK_Parameter-efficient_Large_Kernel_ConvNets_with_Peripheral_Convolution_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">18</td>
<td style="text-align: center;">2024-04-29</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Wang_An_Aggregation-Free_Federated_Learning_for_Tackling_Data_Heterogeneity_CVPR_2024_paper.pdf">An Aggregation-Free Federated Learning for Tackling Data Heterogeneity</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Wang_An_Aggregation-Free_Federated_Learning_for_Tackling_Data_Heterogeneity_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">18</td>
<td style="text-align: center;">2023-12-21</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Jain_VCoder_Versatile_Vision_Encoders_for_Multimodal_Large_Language_Models_CVPR_2024_paper.pdf">VCoder: Versatile Vision Encoders for Multimodal Large Language Models</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Jain_VCoder_Versatile_Vision_Encoders_for_Multimodal_Large_Language_Models_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">18</td>
<td style="text-align: center;">2024-04-01</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Chen_Towards_Memorization-Free_Diffusion_Models_CVPR_2024_paper.pdf">Towards Memorization-Free Diffusion Models</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Chen_Towards_Memorization-Free_Diffusion_Models_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">18</td>
<td style="text-align: center;">2023-12-12</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Papalampidi_A_Simple_Recipe_for_Contrastively_Pre-training_Video-First_Encoders_Beyond_16_CVPR_2024_paper.pdf">A Simple Recipe for Contrastively Pre-training Video-First Encoders Beyond<br />16 Frames</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Papalampidi_A_Simple_Recipe_for_Contrastively_Pre-training_Video-First_Encoders_Beyond_16_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">18</td>
<td style="text-align: center;">2024-06-16</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Delitzas_SceneFun3D_Fine-Grained_Functionality_and_Affordance_Understanding_in_3D_Scenes_CVPR_2024_paper.pdf">SceneFun3D: Fine-Grained Functionality and Affordance Understanding in 3D Scenes</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Delitzas_SceneFun3D_Fine-Grained_Functionality_and_Affordance_Understanding_in_3D_Scenes_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">18</td>
<td style="text-align: center;">2023-05-19</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Guesmi_DAP_A_Dynamic_Adversarial_Patch_for_Evading_Person_Detectors_CVPR_2024_paper.pdf">DAP: A Dynamic Adversarial Patch for Evading Person Detectors</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Guesmi_DAP_A_Dynamic_Adversarial_Patch_for_Evading_Person_Detectors_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">18</td>
<td style="text-align: center;">2023-12-06</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Ren_XCube_Large-Scale_3D_Generative_Modeling_using_Sparse_Voxel_Hierarchies_CVPR_2024_paper.pdf">XCube: Large-Scale 3D Generative Modeling using Sparse Voxel Hierarchies</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Ren_XCube_Large-Scale_3D_Generative_Modeling_using_Sparse_Voxel_Hierarchies_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">18</td>
<td style="text-align: center;">2024-01-04</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Li_Learning_the_3D_Fauna_of_the_Web_CVPR_2024_paper.pdf">Learning the 3D Fauna of the Web</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Li_Learning_the_3D_Fauna_of_the_Web_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">18</td>
<td style="text-align: center;">2023-10-10</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Dong_MuseChat_A_Conversational_Music_Recommendation_System_for_Videos_CVPR_2024_paper.pdf">MuseChat: A Conversational Music Recommendation System for Videos</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Dong_MuseChat_A_Conversational_Music_Recommendation_System_for_Videos_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">18</td>
<td style="text-align: center;">2024-04-05</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Tan_Koala_Key_Frame-Conditioned_Long_Video-LLM_CVPR_2024_paper.pdf">Koala: Key Frame-Conditioned Long Video-LLM</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Tan_Koala_Key_Frame-Conditioned_Long_Video-LLM_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">18</td>
<td style="text-align: center;">2024-01-01</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Xu_Retrieval-Augmented_Egocentric_Video_Captioning_CVPR_2024_paper.pdf">Retrieval-Augmented Egocentric Video Captioning</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Xu_Retrieval-Augmented_Egocentric_Video_Captioning_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">18</td>
<td style="text-align: center;">2024-01-25</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Ozguroglu_pix2gestalt_Amodal_Segmentation_by_Synthesizing_Wholes_CVPR_2024_paper.pdf">pix2gestalt: Amodal Segmentation by Synthesizing Wholes</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Ozguroglu_pix2gestalt_Amodal_Segmentation_by_Synthesizing_Wholes_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">18</td>
<td style="text-align: center;">2023-12-05</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Liu_FINER_Flexible_Spectral-bias_Tuning_in_Implicit_NEural_Representation_by_Variable-periodic_CVPR_2024_paper.pdf">FINER: Flexible Spectral-bias Tuning in Implicit NEural Representation by<br />Variable-periodic Activation Functions</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Liu_FINER_Flexible_Spectral-bias_Tuning_in_Implicit_NEural_Representation_by_Variable-periodic_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">18</td>
<td style="text-align: center;">2023-12-21</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Huang_ZeroShape_Regression-based_Zero-shot_Shape_Reconstruction_CVPR_2024_paper.pdf">ZeroShape: Regression-based Zero-shot Shape Reconstruction</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Huang_ZeroShape_Regression-based_Zero-shot_Shape_Reconstruction_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">18</td>
<td style="text-align: center;">2023-12-12</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Jain_PEEKABOO_Interactive_Video_Generation_via_Masked-Diffusion_CVPR_2024_paper.pdf">PEEKABOO: Interactive Video Generation via Masked-Diffusion</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Jain_PEEKABOO_Interactive_Video_Generation_via_Masked-Diffusion_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">18</td>
<td style="text-align: center;">2023-12-01</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Le_Moing_Dense_Optical_Tracking_Connecting_the_Dots_CVPR_2024_paper.pdf">Dense Optical Tracking: Connecting the Dots</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Le_Moing_Dense_Optical_Tracking_Connecting_the_Dots_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">18</td>
<td style="text-align: center;">2023-01-26</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Kim_Discovering_and_Mitigating_Visual_Biases_through_Keyword_Explanation_CVPR_2024_paper.pdf">Discovering and Mitigating Visual Biases through Keyword Explanation</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Kim_Discovering_and_Mitigating_Visual_Biases_through_Keyword_Explanation_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">18</td>
<td style="text-align: center;">2023-11-28</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Zhou_AvatarGPT_All-in-One_Framework_for_Motion_Understanding_Planning_Generation_and_Beyond_CVPR_2024_paper.pdf">AvatarGPT: All-in-One Framework for Motion Understanding Planning Generation and<br />Beyond</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Zhou_AvatarGPT_All-in-One_Framework_for_Motion_Understanding_Planning_Generation_and_Beyond_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">18</td>
<td style="text-align: center;">2023-06-15</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Muller_Generative_Proxemics_A_Prior_for_3D_Social_Interaction_from_Images_CVPR_2024_paper.pdf">Generative Proxemics: A Prior for 3D Social Interaction from<br />Images</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Muller_Generative_Proxemics_A_Prior_for_3D_Social_Interaction_from_Images_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">18</td>
<td style="text-align: center;">2024-03-21</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Shi_SpikingResformer_Bridging_ResNet_and_Vision_Transformer_in_Spiking_Neural_Networks_CVPR_2024_paper.pdf">SpikingResformer: Bridging ResNet and Vision Transformer in Spiking Neural<br />Networks</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Shi_SpikingResformer_Bridging_ResNet_and_Vision_Transformer_in_Spiking_Neural_Networks_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">18</td>
<td style="text-align: center;">2023-11-25</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Luo_VSCode_General_Visual_Salient_and_Camouflaged_Object_Detection_with_2D_CVPR_2024_paper.pdf">VSCode: General Visual Salient and Camouflaged Object Detection with<br />2D Prompt Learning</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Luo_VSCode_General_Visual_Salient_and_Camouflaged_Object_Detection_with_2D_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">18</td>
<td style="text-align: center;">2023-11-30</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Fan_HOLD_Category-agnostic_3D_Reconstruction_of_Interacting_Hands_and_Objects_from_CVPR_2024_paper.pdf">HOLD: Category-agnostic 3D Reconstruction of Interacting Hands and Objects<br />from Video</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Fan_HOLD_Category-agnostic_3D_Reconstruction_of_Interacting_Hands_and_Objects_from_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">18</td>
<td style="text-align: center;">2024-04-04</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/He_Decoupling_Static_and_Hierarchical_Motion_Perception_for_Referring_Video_Segmentation_CVPR_2024_paper.pdf">Decoupling Static and Hierarchical Motion Perception for Referring Video<br />Segmentation</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/He_Decoupling_Static_and_Hierarchical_Motion_Perception_for_Referring_Video_Segmentation_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">18</td>
<td style="text-align: center;">2023-12-09</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Yu_CoGS_Controllable_Gaussian_Splatting_CVPR_2024_paper.pdf">CoGS: Controllable Gaussian Splatting</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Yu_CoGS_Controllable_Gaussian_Splatting_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">18</td>
<td style="text-align: center;">2023-11-27</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Ho_SiTH_Single-view_Textured_Human_Reconstruction_with_Image-Conditioned_Diffusion_CVPR_2024_paper.pdf">SiTH: Single-view Textured Human Reconstruction with Image-Conditioned Diffusion</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Ho_SiTH_Single-view_Textured_Human_Reconstruction_with_Image-Conditioned_Diffusion_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">18</td>
<td style="text-align: center;">2024-03-11</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Qi_DEADiff_An_Efficient_Stylization_Diffusion_Model_with_Disentangled_Representations_CVPR_2024_paper.pdf">DEADiff: An Efficient Stylization Diffusion Model with Disentangled Representations</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Qi_DEADiff_An_Efficient_Stylization_Diffusion_Model_with_Disentangled_Representations_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">18</td>
<td style="text-align: center;">2023-12-11</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Zhou_Upscale-A-Video_Temporal-Consistent_Diffusion_Model_for_Real-World_Video_Super-Resolution_CVPR_2024_paper.pdf">Upscale-A-Video: Temporal-Consistent Diffusion Model for Real-World Video Super-Resolution</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Zhou_Upscale-A-Video_Temporal-Consistent_Diffusion_Model_for_Real-World_Video_Super-Resolution_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">17</td>
<td style="text-align: center;">2023-06-20</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Wang_CrossKD_Cross-Head_Knowledge_Distillation_for_Object_Detection_CVPR_2024_paper.pdf">CrossKD: Cross-Head Knowledge Distillation for Object Detection</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Wang_CrossKD_Cross-Head_Knowledge_Distillation_for_Object_Detection_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">17</td>
<td style="text-align: center;">2024-03-03</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Zhou_Dynamic_Adapter_Meets_Prompt_Tuning_Parameter-Efficient_Transfer_Learning_for_Point_CVPR_2024_paper.pdf">Dynamic Adapter Meets Prompt Tuning: Parameter-Efficient Transfer Learning for<br />Point Cloud Analysis</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Zhou_Dynamic_Adapter_Meets_Prompt_Tuning_Parameter-Efficient_Transfer_Learning_for_Point_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">17</td>
<td style="text-align: center;">2024-03-26</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Zhang_Dual_Memory_Networks_A_Versatile_Adaptation_Approach_for_Vision-Language_Models_CVPR_2024_paper.pdf">Dual Memory Networks: A Versatile Adaptation Approach for Vision-Language<br />Models</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Zhang_Dual_Memory_Networks_A_Versatile_Adaptation_Approach_for_Vision-Language_Models_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">17</td>
<td style="text-align: center;">2024-03-27</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Cheng_Unleashing_the_Potential_of_SAM_for_Medical_Adaptation_via_Hierarchical_CVPR_2024_paper.pdf">Unleashing the Potential of SAM for Medical Adaptation via<br />Hierarchical Decoding</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Cheng_Unleashing_the_Potential_of_SAM_for_Medical_Adaptation_via_Hierarchical_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">17</td>
<td style="text-align: center;">2023-12-18</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Gao_CLOVA_A_Closed-LOop_Visual_Assistant_with_Tool_Usage_and_Update_CVPR_2024_paper.pdf">CLOVA: A Closed-LOop Visual Assistant with Tool Usage and<br />Update</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Gao_CLOVA_A_Closed-LOop_Visual_Assistant_with_Tool_Usage_and_Update_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">17</td>
<td style="text-align: center;">2024-02-19</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Koch_Open3DSG_Open-Vocabulary_3D_Scene_Graphs_from_Point_Clouds_with_Queryable_CVPR_2024_paper.pdf">Open3DSG: Open-Vocabulary 3D Scene Graphs from Point Clouds with<br />Queryable Objects and Open-Set Relationships</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Koch_Open3DSG_Open-Vocabulary_3D_Scene_Graphs_from_Point_Clouds_with_Queryable_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">17</td>
<td style="text-align: center;">2022-12-06</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Gu_Rethinking_the_Objectives_of_Vector-Quantized_Tokenizers_for_Image_Synthesis_CVPR_2024_paper.pdf">Rethinking the Objectives of Vector-Quantized Tokenizers for Image Synthesis</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Gu_Rethinking_the_Objectives_of_Vector-Quantized_Tokenizers_for_Image_Synthesis_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">17</td>
<td style="text-align: center;">2023-08-25</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Liu_Residual_Denoising_Diffusion_Models_CVPR_2024_paper.pdf">Residual Denoising Diffusion Models</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Liu_Residual_Denoising_Diffusion_Models_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">17</td>
<td style="text-align: center;">2023-11-30</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Haji-Ali_ElasticDiffusion_Training-free_Arbitrary_Size_Image_Generation_through_Global-Local_Content_Separation_CVPR_2024_paper.pdf">ElasticDiffusion: Training-free Arbitrary Size Image Generation through Global-Local Content<br />Separation</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Haji-Ali_ElasticDiffusion_Training-free_Arbitrary_Size_Image_Generation_through_Global-Local_Content_Separation_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">17</td>
<td style="text-align: center;">2023-11-21</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Gal_Breathing_Life_Into_Sketches_Using_Text-to-Video_Priors_CVPR_2024_paper.pdf">Breathing Life Into Sketches Using Text-to-Video Priors</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Gal_Breathing_Life_Into_Sketches_Using_Text-to-Video_Priors_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">17</td>
<td style="text-align: center;">2023-12-05</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Kim_C3_High-Performance_and_Low-Complexity_Neural_Compression_from_a_Single_Image_CVPR_2024_paper.pdf">C3: High-Performance and Low-Complexity Neural Compression from a Single<br />Image or Video</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Kim_C3_High-Performance_and_Low-Complexity_Neural_Compression_from_a_Single_Image_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">17</td>
<td style="text-align: center;">2023-12-26</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Xu_Inter-X_Towards_Versatile_Human-Human_Interaction_Analysis_CVPR_2024_paper.pdf">Inter-X: Towards Versatile Human-Human Interaction Analysis</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Xu_Inter-X_Towards_Versatile_Human-Human_Interaction_Analysis_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">17</td>
<td style="text-align: center;">2023-12-07</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Zhang_Prompt_Highlighter_Interactive_Control_for_Multi-Modal_LLMs_CVPR_2024_paper.pdf">Prompt Highlighter: Interactive Control for Multi-Modal LLMs</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Zhang_Prompt_Highlighter_Interactive_Control_for_Multi-Modal_LLMs_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">17</td>
<td style="text-align: center;">2023-11-09</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Piergiovanni_Mirasol3B_A_Multimodal_Autoregressive_Model_for_Time-Aligned_and_Contextual_Modalities_CVPR_2024_paper.pdf">Mirasol3B: A Multimodal Autoregressive Model for Time-Aligned and Contextual<br />Modalities</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Piergiovanni_Mirasol3B_A_Multimodal_Autoregressive_Model_for_Time-Aligned_and_Contextual_Modalities_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">17</td>
<td style="text-align: center;">2024-02-27</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Sun_VRP-SAM_SAM_with_Visual_Reference_Prompt_CVPR_2024_paper.pdf">VRP-SAM: SAM with Visual Reference Prompt</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Sun_VRP-SAM_SAM_with_Visual_Reference_Prompt_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">17</td>
<td style="text-align: center;">2024-03-19</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Yang_FRESCO_Spatial-Temporal_Correspondence_for_Zero-Shot_Video_Translation_CVPR_2024_paper.pdf">FRESCO: Spatial-Temporal Correspondence for Zero-Shot Video Translation</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Yang_FRESCO_Spatial-Temporal_Correspondence_for_Zero-Shot_Video_Translation_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">17</td>
<td style="text-align: center;">2024-02-28</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Wada_Polos_Multimodal_Metric_Learning_from_Human_Feedback_for_Image_Captioning_CVPR_2024_paper.pdf">Polos: Multimodal Metric Learning from Human Feedback for Image<br />Captioning</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Wada_Polos_Multimodal_Metric_Learning_from_Human_Feedback_for_Image_Captioning_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">17</td>
<td style="text-align: center;">2023-11-26</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Yuan_Visual_Programming_for_Zero-shot_Open-Vocabulary_3D_Visual_Grounding_CVPR_2024_paper.pdf">Visual Programming for Zero-shot Open-Vocabulary 3D Visual Grounding</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Yuan_Visual_Programming_for_Zero-shot_Open-Vocabulary_3D_Visual_Grounding_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">17</td>
<td style="text-align: center;">2023-11-28</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Li_Self-Discovering_Interpretable_Diffusion_Latent_Directions_for_Responsible_Text-to-Image_Generation_CVPR_2024_paper.pdf">Self-Discovering Interpretable Diffusion Latent Directions for Responsible Text-to-Image Generation</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Li_Self-Discovering_Interpretable_Diffusion_Latent_Directions_for_Responsible_Text-to-Image_Generation_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">17</td>
<td style="text-align: center;">2023-12-27</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Xing_SVGDreamer_Text_Guided_SVG_Generation_with_Diffusion_Model_CVPR_2024_paper.pdf">SVGDreamer: Text Guided SVG Generation with Diffusion Model</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Xing_SVGDreamer_Text_Guided_SVG_Generation_with_Diffusion_Model_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">17</td>
<td style="text-align: center;">2023-09-14</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Zhang_DePT_Decoupled_Prompt_Tuning_CVPR_2024_paperOriginal.pdf">DePT: Decoupled Prompt Tuning</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Zhang_DePT_Decoupled_Prompt_Tuning_CVPR_2024_paperOriginal.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">17</td>
<td style="text-align: center;">2023-11-03</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Cai_HIPTrack_Visual_Tracking_with_Historical_Prompts_CVPR_2024_paper.pdf">HIPTrack: Visual Tracking with Historical Prompts</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Cai_HIPTrack_Visual_Tracking_with_Historical_Prompts_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">17</td>
<td style="text-align: center;">2023-11-27</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Li_EVCap_Retrieval-Augmented_Image_Captioning_with_External_Visual-Name_Memory_for_Open-World_CVPR_2024_paper.pdf">EVCap: Retrieval-Augmented Image Captioning with External Visual-Name Memory for<br />Open-World Comprehension</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Li_EVCap_Retrieval-Augmented_Image_Captioning_with_External_Visual-Name_Memory_for_Open-World_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">17</td>
<td style="text-align: center;">2023-10-23</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Kapon_MAS_Multi-view_Ancestral_Sampling_for_3D_Motion_Generation_Using_2D_CVPR_2024_paper.pdf">MAS: Multi-view Ancestral Sampling for 3D Motion Generation Using<br />2D Diffusion</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Kapon_MAS_Multi-view_Ancestral_Sampling_for_3D_Motion_Generation_Using_2D_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">16</td>
<td style="text-align: center;">2023-12-04</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Chen_ArtAdapter_Text-to-Image_Style_Transfer_using_Multi-Level_Style_Encoder_and_Explicit_CVPR_2024_paper.pdf">ArtAdapter: Text-to-Image Style Transfer using Multi-Level Style Encoder and<br />Explicit Adaptation</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Chen_ArtAdapter_Text-to-Image_Style_Transfer_using_Multi-Level_Style_Encoder_and_Explicit_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">16</td>
<td style="text-align: center;">2024-03-04</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Teney_Neural_Redshift_Random_Networks_are_not_Random_Functions_CVPR_2024_paper.pdf">Neural Redshift: Random Networks are not Random Functions</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Teney_Neural_Redshift_Random_Networks_are_not_Random_Functions_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">16</td>
<td style="text-align: center;">2023-11-23</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Koo_Posterior_Distillation_Sampling_CVPR_2024_paper.pdf">Posterior Distillation Sampling</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Koo_Posterior_Distillation_Sampling_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">16</td>
<td style="text-align: center;">2024-05-21</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Li_Nearest_is_Not_Dearest_Towards_Practical_Defense_against_Quantization-conditioned_Backdoor_CVPR_2024_paper.pdf">Nearest is Not Dearest: Towards Practical Defense against Quantization-conditioned<br />Backdoor Attacks</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Li_Nearest_is_Not_Dearest_Towards_Practical_Defense_against_Quantization-conditioned_Backdoor_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">16</td>
<td style="text-align: center;">2023-11-29</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Geng_Visual_Anagrams_Generating_Multi-View_Optical_Illusions_with_Diffusion_Models_CVPR_2024_paper.pdf">Visual Anagrams: Generating Multi-View Optical Illusions with Diffusion Models</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Geng_Visual_Anagrams_Generating_Multi-View_Optical_Illusions_with_Diffusion_Models_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">16</td>
<td style="text-align: center;">2023-12-07</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Xu_MuRF_Multi-Baseline_Radiance_Fields_CVPR_2024_paper.pdf">MuRF: Multi-Baseline Radiance Fields</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Xu_MuRF_Multi-Baseline_Radiance_Fields_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">16</td>
<td style="text-align: center;">2024-02-01</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Liang_CapHuman_Capture_Your_Moments_in_Parallel_Universes_CVPR_2024_paper.pdf">CapHuman: Capture Your Moments in Parallel Universes</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Liang_CapHuman_Capture_Your_Moments_in_Parallel_Universes_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">16</td>
<td style="text-align: center;">2023-11-29</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Shao_Generalized_Large-Scale_Data_Condensation_via_Various_Backbone_and_Statistical_Matching_CVPR_2024_paper.pdf">Generalized Large-Scale Data Condensation via Various Backbone and Statistical<br />Matching</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Shao_Generalized_Large-Scale_Data_Condensation_via_Various_Backbone_and_Statistical_Matching_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">16</td>
<td style="text-align: center;">2023-05-17</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Wu_One-Prompt_to_Segment_All_Medical_Images_CVPR_2024_paper.pdf">One-Prompt to Segment All Medical Images</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Wu_One-Prompt_to_Segment_All_Medical_Images_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">16</td>
<td style="text-align: center;">2023-12-05</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Sharma_Alchemist_Parametric_Control_of_Material_Properties_with_Diffusion_Models_CVPR_2024_paper.pdf">Alchemist: Parametric Control of Material Properties with Diffusion Models</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Sharma_Alchemist_Parametric_Control_of_Material_Properties_with_Diffusion_Models_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">16</td>
<td style="text-align: center;">2023-11-26</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Nguyen_Insect-Foundation_A_Foundation_Model_and_Large-scale_1M_Dataset_for_Visual_CVPR_2024_paper.pdf">Insect-Foundation: A Foundation Model and Large-scale 1M Dataset for<br />Visual Insect Understanding</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Nguyen_Insect-Foundation_A_Foundation_Model_and_Large-scale_1M_Dataset_for_Visual_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">16</td>
<td style="text-align: center;">2023-12-28</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Lin_Improving_Image_Restoration_through_Removing_Degradations_in_Textual_Representations_CVPR_2024_paper.pdf">Improving Image Restoration through Removing Degradations in Textual Representations</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Lin_Improving_Image_Restoration_through_Removing_Degradations_in_Textual_Representations_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">16</td>
<td style="text-align: center;">2024-01-16</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Hong_MultiPLY_A_Multisensory_Object-Centric_Embodied_Large_Language_Model_in_3D_CVPR_2024_paper.pdf">MultiPLY: A Multisensory Object-Centric Embodied Large Language Model in<br />3D World</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Hong_MultiPLY_A_Multisensory_Object-Centric_Embodied_Large_Language_Model_in_3D_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">16</td>
<td style="text-align: center;">2024-01-09</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Chen_DiffSHEG_A_Diffusion-Based_Approach_for_Real-Time_Speech-driven_Holistic_3D_Expression_CVPR_2024_paper.pdf">DiffSHEG: A Diffusion-Based Approach for Real-Time Speech-driven Holistic 3D<br />Expression and Gesture Generation</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Chen_DiffSHEG_A_Diffusion-Based_Approach_for_Real-Time_Speech-driven_Holistic_3D_Expression_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">16</td>
<td style="text-align: center;">2023-12-12</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Zhang_DiffMorpher_Unleashing_the_Capability_of_Diffusion_Models_for_Image_Morphing_CVPR_2024_paper.pdf">DiffMorpher: Unleashing the Capability of Diffusion Models for Image<br />Morphing</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Zhang_DiffMorpher_Unleashing_the_Capability_of_Diffusion_Models_for_Image_Morphing_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">16</td>
<td style="text-align: center;">2024-03-25</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Yi_Text-IF_Leveraging_Semantic_Text_Guidance_for_Degradation-Aware_and_Interactive_Image_CVPR_2024_paper.pdf">Text-IF: Leveraging Semantic Text Guidance for Degradation-Aware and Interactive<br />Image Fusion</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Yi_Text-IF_Leveraging_Semantic_Text_Guidance_for_Degradation-Aware_and_Interactive_Image_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">16</td>
<td style="text-align: center;">2024-03-24</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Hou_Salience_DETR_Enhancing_Detection_Transformer_with_Hierarchical_Salience_Filtering_Refinement_CVPR_2024_paper.pdf">Salience DETR: Enhancing Detection Transformer with Hierarchical Salience Filtering<br />Refinement</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Hou_Salience_DETR_Enhancing_Detection_Transformer_with_Hierarchical_Salience_Filtering_Refinement_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">16</td>
<td style="text-align: center;">2023-12-28</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Bai_ARTrackV2_Prompting_Autoregressive_Tracker_Where_to_Look_and_How_to_CVPR_2024_paper.pdf">ARTrackV2: Prompting Autoregressive Tracker Where to Look and How<br />to Describe</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Bai_ARTrackV2_Prompting_Autoregressive_Tracker_Where_to_Look_and_How_to_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">16</td>
<td style="text-align: center;">2024-03-12</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Li_Dynamic_Graph_Representation_with_Knowledge-aware_Attention_for_Histopathology_Whole_Slide_CVPR_2024_paper.pdf">Dynamic Graph Representation with Knowledge-aware Attention for Histopathology Whole<br />Slide Image Analysis</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Li_Dynamic_Graph_Representation_with_Knowledge-aware_Attention_for_Histopathology_Whole_Slide_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">16</td>
<td style="text-align: center;">2024-04-01</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Liu_Drag_Your_Noise_Interactive_Point-based_Editing_via_Diffusion_Semantic_Propagation_CVPR_2024_paper.pdf">Drag Your Noise: Interactive Point-based Editing via Diffusion Semantic<br />Propagation</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Liu_Drag_Your_Noise_Interactive_Point-based_Editing_via_Diffusion_Semantic_Propagation_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">16</td>
<td style="text-align: center;">2023-07-24</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Yang_CLIP-KD_An_Empirical_Study_of_CLIP_Model_Distillation_CVPR_2024_paper.pdf">CLIP-KD: An Empirical Study of CLIP Model Distillation</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Yang_CLIP-KD_An_Empirical_Study_of_CLIP_Model_Distillation_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">16</td>
<td style="text-align: center;">2024-03-05</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Li_PromptKD_Unsupervised_Prompt_Distillation_for_Vision-Language_Models_CVPR_2024_paper.pdf">PromptKD: Unsupervised Prompt Distillation for Vision-Language Models</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Li_PromptKD_Unsupervised_Prompt_Distillation_for_Vision-Language_Models_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">16</td>
<td style="text-align: center;">2023-07-14</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Zhao_SubT-MRS_Dataset_Pushing_SLAM_Towards_All-weather_Environments_CVPR_2024_paper.pdf">SubT-MRS Dataset: Pushing SLAM Towards All-weather Environments</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Zhao_SubT-MRS_Dataset_Pushing_SLAM_Towards_All-weather_Environments_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">16</td>
<td style="text-align: center;">2024-04-11</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/DInca_OpenBias_Open-set_Bias_Detection_in_Text-to-Image_Generative_Models_CVPR_2024_paper.pdf">OpenBias: Open-set Bias Detection in Text-to-Image Generative Models</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/DInca_OpenBias_Open-set_Bias_Detection_in_Text-to-Image_Generative_Models_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">16</td>
<td style="text-align: center;">2022-11-15</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Zhang_Data_Poisoning_based_Backdoor_Attacks_to_Contrastive_Learning_CVPR_2024_paper.pdf">Data Poisoning based Backdoor Attacks to Contrastive Learning</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Zhang_Data_Poisoning_based_Backdoor_Attacks_to_Contrastive_Learning_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">16</td>
<td style="text-align: center;">2024-04-06</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Yi_Diffusion_Time-step_Curriculum_for_One_Image_to_3D_Generation_CVPR_2024_paper.pdf">Diffusion Time-step Curriculum for One Image to 3D Generation</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Yi_Diffusion_Time-step_Curriculum_for_One_Image_to_3D_Generation_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">15</td>
<td style="text-align: center;">2023-11-25</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Li_Towards_Scalable_3D_Anomaly_Detection_and_Localization_A_Benchmark_via_CVPR_2024_paper.pdf">Towards Scalable 3D Anomaly Detection and Localization: A Benchmark<br />via 3D Anomaly Synthesis and A Self-Supervised Learning Network</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Li_Towards_Scalable_3D_Anomaly_Detection_and_Localization_A_Benchmark_via_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">15</td>
<td style="text-align: center;">2024-03-19</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Huang_Adapting_Visual-Language_Models_for_Generalizable_Anomaly_Detection_in_Medical_Images_CVPR_2024_paper.pdf">Adapting Visual-Language Models for Generalizable Anomaly Detection in Medical<br />Images</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Huang_Adapting_Visual-Language_Models_for_Generalizable_Anomaly_Detection_in_Medical_Images_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">15</td>
<td style="text-align: center;">2023-05-27</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Wang_Zero-TPrune_Zero-Shot_Token_Pruning_through_Leveraging_of_the_Attention_Graph_CVPR_2024_paper.pdf">Zero-TPrune: Zero-Shot Token Pruning through Leveraging of the Attention<br />Graph in Pre-Trained Transformers</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Wang_Zero-TPrune_Zero-Shot_Token_Pruning_through_Leveraging_of_the_Attention_Graph_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">15</td>
<td style="text-align: center;">2023-11-30</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Zeng_CAT-DM_Controllable_Accelerated_Virtual_Try-on_with_Diffusion_Model_CVPR_2024_paper.pdf">CAT-DM: Controllable Accelerated Virtual Try-on with Diffusion Model</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Zeng_CAT-DM_Controllable_Accelerated_Virtual_Try-on_with_Diffusion_Model_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">15</td>
<td style="text-align: center;">2023-11-22</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Feng_PIE-NeRF_Physics-based_Interactive_Elastodynamics_with_NeRF_CVPR_2024_paper.pdf">PIE-NeRF: Physics-based Interactive Elastodynamics with NeRF</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Feng_PIE-NeRF_Physics-based_Interactive_Elastodynamics_with_NeRF_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">15</td>
<td style="text-align: center;">2023-12-06</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Zhang_Improving_the_Generalization_of_Segmentation_Foundation_Model_under_Distribution_Shift_CVPR_2024_paper.pdf">Improving the Generalization of Segmentation Foundation Model under Distribution<br />Shift via Weakly Supervised Adaptation</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Zhang_Improving_the_Generalization_of_Segmentation_Foundation_Model_under_Distribution_Shift_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">15</td>
<td style="text-align: center;">2023-12-25</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Wang_A_Recipe_for_Scaling_up_Text-to-Video_Generation_with_Text-free_Videos_CVPR_2024_paper.pdf">A Recipe for Scaling up Text-to-Video Generation with Text-free<br />Videos</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Wang_A_Recipe_for_Scaling_up_Text-to-Video_Generation_with_Text-free_Videos_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">15</td>
<td style="text-align: center;">2024-02-23</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Barquero_Seamless_Human_Motion_Composition_with_Blended_Positional_Encodings_CVPR_2024_paper.pdf">Seamless Human Motion Composition with Blended Positional Encodings</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Barquero_Seamless_Human_Motion_Composition_with_Blended_Positional_Encodings_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">15</td>
<td style="text-align: center;">2023-12-04</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Li_PatchFusion_An_End-to-End_Tile-Based_Framework_for_High-Resolution_Monocular_Metric_Depth_CVPR_2024_paper.pdf">PatchFusion: An End-to-End Tile-Based Framework for High-Resolution Monocular Metric<br />Depth Estimation</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Li_PatchFusion_An_End-to-End_Tile-Based_Framework_for_High-Resolution_Monocular_Metric_Depth_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">15</td>
<td style="text-align: center;">2024-04-04</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Yan_MonoCD_Monocular_3D_Object_Detection_with_Complementary_Depths_CVPR_2024_paper.pdf">MonoCD: Monocular 3D Object Detection with Complementary Depths</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Yan_MonoCD_Monocular_3D_Object_Detection_with_Complementary_Depths_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">15</td>
<td style="text-align: center;">2023-12-20</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Wu_Fairy_Fast_Parallelized_Instruction-Guided_Video-to-Video_Synthesis_CVPR_2024_paper.pdf">Fairy: Fast Parallelized Instruction-Guided Video-to-Video Synthesis</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Wu_Fairy_Fast_Parallelized_Instruction-Guided_Video-to-Video_Synthesis_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">15</td>
<td style="text-align: center;">2023-12-07</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Ding_Text-to-3D_Generation_with_Bidirectional_Diffusion_using_both_2D_and_3D_CVPR_2024_paper.pdf">Text-to-3D Generation with Bidirectional Diffusion using both 2D and<br />3D priors</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Ding_Text-to-3D_Generation_with_Bidirectional_Diffusion_using_both_2D_and_3D_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">15</td>
<td style="text-align: center;">2023-11-30</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Yao_TCPTextual-based_Class-aware_Prompt_tuning_for_Visual-Language_Model_CVPR_2024_paper.pdf">TCP:Textual-based Class-aware Prompt tuning for Visual-Language Model</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Yao_TCPTextual-based_Class-aware_Prompt_tuning_for_Visual-Language_Model_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">15</td>
<td style="text-align: center;">2024-02-27</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Wu_VoCo_A_Simple-yet-Effective_Volume_Contrastive_Learning_Framework_for_3D_Medical_CVPR_2024_paper.pdf">VoCo: A Simple-yet-Effective Volume Contrastive Learning Framework for 3D<br />Medical Image Analysis</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Wu_VoCo_A_Simple-yet-Effective_Volume_Contrastive_Learning_Framework_for_3D_Medical_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">15</td>
<td style="text-align: center;">2024-01-08</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Zhao_MS-DETR_Efficient_DETR_Training_with_Mixed_Supervision_CVPR_2024_paper.pdf">MS-DETR: Efficient DETR Training with Mixed Supervision</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Zhao_MS-DETR_Efficient_DETR_Training_with_Mixed_Supervision_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">15</td>
<td style="text-align: center;">2024-03-20</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Wang_DetDiffusion_Synergizing_Generative_and_Perceptive_Models_for_Enhanced_Data_Generation_CVPR_2024_paper.pdf">DetDiffusion: Synergizing Generative and Perceptive Models for Enhanced Data<br />Generation and Perception</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Wang_DetDiffusion_Synergizing_Generative_and_Perceptive_Models_for_Enhanced_Data_Generation_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">15</td>
<td style="text-align: center;">2024-03-27</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Patni_ECoDepth_Effective_Conditioning_of_Diffusion_Models_for_Monocular_Depth_Estimation_CVPR_2024_paper.pdf">ECoDepth: Effective Conditioning of Diffusion Models for Monocular Depth<br />Estimation</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Patni_ECoDepth_Effective_Conditioning_of_Diffusion_Models_for_Monocular_Depth_Estimation_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">15</td>
<td style="text-align: center;">2024-01-02</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Liu_Towards_a_Simultaneous_and_Granular_Identity-Expression_Control_in_Personalized_Face_CVPR_2024_paper.pdf">Towards a Simultaneous and Granular Identity-Expression Control in Personalized<br />Face Generation</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Liu_Towards_a_Simultaneous_and_Granular_Identity-Expression_Control_in_Personalized_Face_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">15</td>
<td style="text-align: center;">2024-03-04</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Li_One_Prompt_Word_is_Enough_to_Boost_Adversarial_Robustness_for_CVPR_2024_paper.pdf">One Prompt Word is Enough to Boost Adversarial Robustness<br />for Pre-trained Vision-Language Models</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Li_One_Prompt_Word_is_Enough_to_Boost_Adversarial_Robustness_for_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">15</td>
<td style="text-align: center;">2023-10-03</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Mahmoud_Sieve_Multimodal_Dataset_Pruning_using_Image_Captioning_Models_CVPR_2024_paper.pdf">Sieve: Multimodal Dataset Pruning using Image Captioning Models</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Mahmoud_Sieve_Multimodal_Dataset_Pruning_using_Image_Captioning_Models_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">15</td>
<td style="text-align: center;">2023-12-07</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Cai_Digital_Life_Project_Autonomous_3D_Characters_with_Social_Intelligence_CVPR_2024_paper.pdf">Digital Life Project: Autonomous 3D Characters with Social Intelligence</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Cai_Digital_Life_Project_Autonomous_3D_Characters_with_Social_Intelligence_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">15</td>
<td style="text-align: center;">2024-01-16</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Liu_TACO_Benchmarking_Generalizable_Bimanual_Tool-ACtion-Object_Understanding_CVPR_2024_paper.pdf">TACO: Benchmarking Generalizable Bimanual Tool-ACtion-Object Understanding</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Liu_TACO_Benchmarking_Generalizable_Bimanual_Tool-ACtion-Object_Understanding_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">15</td>
<td style="text-align: center;">2024-03-09</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Yang_Robust_Emotion_Recognition_in_Context_Debiasing_CVPR_2024_paper.pdf">Robust Emotion Recognition in Context Debiasing</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Yang_Robust_Emotion_Recognition_in_Context_Debiasing_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">15</td>
<td style="text-align: center;">2023-12-12</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Lu_RTMO_Towards_High-Performance_One-Stage_Real-Time_Multi-Person_Pose_Estimation_CVPR_2024_paper.pdf">RTMO: Towards High-Performance One-Stage Real-Time Multi-Person Pose Estimation</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Lu_RTMO_Towards_High-Performance_One-Stage_Real-Time_Multi-Person_Pose_Estimation_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">15</td>
<td style="text-align: center;">2023-12-15</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Guo_Focus_on_Your_Instruction_Fine-grained_and_Multi-instruction_Image_Editing_by_CVPR_2024_paper.pdf">Focus on Your Instruction: Fine-grained and Multi-instruction Image Editing<br />by Attention Modulation</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Guo_Focus_on_Your_Instruction_Fine-grained_and_Multi-instruction_Image_Editing_by_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">15</td>
<td style="text-align: center;">2023-11-29</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Li_One-Shot_Open_Affordance_Learning_with_Foundation_Models_CVPR_2024_paper.pdf">One-Shot Open Affordance Learning with Foundation Models</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Li_One-Shot_Open_Affordance_Learning_with_Foundation_Models_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">15</td>
<td style="text-align: center;">2023-12-11</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Yu_DiffCast_A_Unified_Framework_via_Residual_Diffusion_for_Precipitation_Nowcasting_CVPR_2024_paper.pdf">DiffCast: A Unified Framework via Residual Diffusion for Precipitation<br />Nowcasting</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Yu_DiffCast_A_Unified_Framework_via_Residual_Diffusion_for_Precipitation_Nowcasting_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">15</td>
<td style="text-align: center;">2024-04-29</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Drobyshev_EMOPortraits_Emotion-enhanced_Multimodal_One-shot_Head_Avatars_CVPR_2024_paper.pdf">EMOPortraits: Emotion-enhanced Multimodal One-shot Head Avatars</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Drobyshev_EMOPortraits_Emotion-enhanced_Multimodal_One-shot_Head_Avatars_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">15</td>
<td style="text-align: center;">2023-12-31</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Liu_EMAGE_Towards_Unified_Holistic_Co-Speech_Gesture_Generation_via_Expressive_Masked_CVPR_2024_paper.pdf">EMAGE: Towards Unified Holistic Co-Speech Gesture Generation via Expressive<br />Masked Audio Gesture Modeling</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Liu_EMAGE_Towards_Unified_Holistic_Co-Speech_Gesture_Generation_via_Expressive_Masked_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">15</td>
<td style="text-align: center;">2024-03-25</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Zhang_TRIP_Temporal_Residual_Learning_with_Image_Noise_Prior_for_Image-to-Video_CVPR_2024_paper.pdf">TRIP: Temporal Residual Learning with Image Noise Prior for<br />Image-to-Video Diffusion Models</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Zhang_TRIP_Temporal_Residual_Learning_with_Image_Noise_Prior_for_Image-to-Video_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">15</td>
<td style="text-align: center;">2024-05-29</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Ren_NeRF_On-the-go_Exploiting_Uncertainty_for_Distractor-free_NeRFs_in_the_Wild_CVPR_2024_paper.pdf">NeRF On-the-go: Exploiting Uncertainty for Distractor-free NeRFs in the<br />Wild</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Ren_NeRF_On-the-go_Exploiting_Uncertainty_for_Distractor-free_NeRFs_in_the_Wild_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">15</td>
<td style="text-align: center;">2023-11-23</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Luo_PointOBB_Learning_Oriented_Object_Detection_via_Single_Point_Supervision_CVPR_2024_paper.pdf">PointOBB: Learning Oriented Object Detection via Single Point Supervision</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Luo_PointOBB_Learning_Oriented_Object_Detection_via_Single_Point_Supervision_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">15</td>
<td style="text-align: center;">2023-04-05</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Kahatapitiya_VicTR_Video-conditioned_Text_Representations_for_Activity_Recognition_CVPR_2024_paper.pdf">VicTR: Video-conditioned Text Representations for Activity Recognition</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Kahatapitiya_VicTR_Video-conditioned_Text_Representations_for_Activity_Recognition_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">15</td>
<td style="text-align: center;">2023-11-17</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Zhang_Multimodal_Representation_Learning_by_Alternating_Unimodal_Adaptation_CVPR_2024_paper.pdf">Multimodal Representation Learning by Alternating Unimodal Adaptation</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Zhang_Multimodal_Representation_Learning_by_Alternating_Unimodal_Adaptation_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">15</td>
<td style="text-align: center;">2024-04-06</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Guo_InitNO_Boosting_Text-to-Image_Diffusion_Models_via_Initial_Noise_Optimization_CVPR_2024_paper.pdf">InitNO: Boosting Text-to-Image Diffusion Models via Initial Noise Optimization</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Guo_InitNO_Boosting_Text-to-Image_Diffusion_Models_via_Initial_Noise_Optimization_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">15</td>
<td style="text-align: center;">2024-03-22</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Yin_IS-Fusion_Instance-Scene_Collaborative_Fusion_for_Multimodal_3D_Object_Detection_CVPR_2024_paper.pdf">IS-Fusion: Instance-Scene Collaborative Fusion for Multimodal 3D Object Detection</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Yin_IS-Fusion_Instance-Scene_Collaborative_Fusion_for_Multimodal_3D_Object_Detection_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">14</td>
<td style="text-align: center;">2023-12-06</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Zuo_UFineBench_Towards_Text-based_Person_Retrieval_with_Ultra-fine_Granularity_CVPR_2024_paper.pdf">UFineBench: Towards Text-based Person Retrieval with Ultra-fine Granularity</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Zuo_UFineBench_Towards_Text-based_Person_Retrieval_with_Ultra-fine_Granularity_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">14</td>
<td style="text-align: center;">2024-04-01</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Zhou_Streaming_Dense_Video_Captioning_CVPR_2024_paper.pdf">Streaming Dense Video Captioning</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Zhou_Streaming_Dense_Video_Captioning_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">14</td>
<td style="text-align: center;">2023-11-30</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Hong_On_Exact_Inversion_of_DPM-Solvers_CVPR_2024_paper.pdf">On Exact Inversion of DPM-Solvers</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Hong_On_Exact_Inversion_of_DPM-Solvers_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">14</td>
<td style="text-align: center;">2023-11-29</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Ma_Cam4DOcc_Benchmark_for_Camera-Only_4D_Occupancy_Forecasting_in_Autonomous_Driving_CVPR_2024_paper.pdf">Cam4DOcc: Benchmark for Camera-Only 4D Occupancy Forecasting in Autonomous<br />Driving Applications</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Ma_Cam4DOcc_Benchmark_for_Camera-Only_4D_Occupancy_Forecasting_in_Autonomous_Driving_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">14</td>
<td style="text-align: center;">2024-04-01</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Gong_LLMs_are_Good_Sign_Language_Translators_CVPR_2024_paper.pdf">LLMs are Good Sign Language Translators</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Gong_LLMs_are_Good_Sign_Language_Translators_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">14</td>
<td style="text-align: center;">2023-12-05</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Turki_HybridNeRF_Efficient_Neural_Rendering_via_Adaptive_Volumetric_Surfaces_CVPR_2024_paper.pdf">HybridNeRF: Efficient Neural Rendering via Adaptive Volumetric Surfaces</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Turki_HybridNeRF_Efficient_Neural_Rendering_via_Adaptive_Volumetric_Surfaces_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">14</td>
<td style="text-align: center;">2023-06-07</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Sun_CorrMatch_Label_Propagation_via_Correlation_Matching_for_Semi-Supervised_Semantic_Segmentation_CVPR_2024_paper.pdf">CorrMatch: Label Propagation via Correlation Matching for Semi-Supervised Semantic<br />Segmentation</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Sun_CorrMatch_Label_Propagation_via_Correlation_Matching_for_Semi-Supervised_Semantic_Segmentation_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">14</td>
<td style="text-align: center;">2024-06-16</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Zhou_Adapt_or_Perish_Adaptive_Sparse_Transformer_with_Attentive_Feature_Refinement_CVPR_2024_paper.pdf">Adapt or Perish: Adaptive Sparse Transformer with Attentive Feature<br />Refinement for Image Restoration</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Zhou_Adapt_or_Perish_Adaptive_Sparse_Transformer_with_Attentive_Feature_Refinement_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">14</td>
<td style="text-align: center;">2023-10-25</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Gokaslan_CommonCanvas_Open_Diffusion_Models_Trained_on_Creative-Commons_Images_CVPR_2024_paper.pdf">CommonCanvas: Open Diffusion Models Trained on Creative-Commons Images</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Gokaslan_CommonCanvas_Open_Diffusion_Models_Trained_on_Creative-Commons_Images_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">14</td>
<td style="text-align: center;">2023-12-20</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Ma_SpecNeRF_Gaussian_Directional_Encoding_for_Specular_Reflections_CVPR_2024_paper.pdf">SpecNeRF: Gaussian Directional Encoding for Specular Reflections</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Ma_SpecNeRF_Gaussian_Directional_Encoding_for_Specular_Reflections_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">14</td>
<td style="text-align: center;">2024-03-05</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Qi_Interactive_Continual_Learning_Fast_and_Slow_Thinking_CVPR_2024_paper.pdf">Interactive Continual Learning: Fast and Slow Thinking</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Qi_Interactive_Continual_Learning_Fast_and_Slow_Thinking_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">14</td>
<td style="text-align: center;">2024-04-11</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Wang_MindBridge_A_Cross-Subject_Brain_Decoding_Framework_CVPR_2024_paper.pdf">MindBridge: A Cross-Subject Brain Decoding Framework</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Wang_MindBridge_A_Cross-Subject_Brain_Decoding_Framework_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">14</td>
<td style="text-align: center;">2024-03-07</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Liu_Depth-aware_Test-Time_Training_for_Zero-shot_Video_Object_Segmentation_CVPR_2024_paper.pdf">Depth-aware Test-Time Training for Zero-shot Video Object Segmentation</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Liu_Depth-aware_Test-Time_Training_for_Zero-shot_Video_Object_Segmentation_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">14</td>
<td style="text-align: center;">2024-06-16</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Kumar_Revamping_Federated_Learning_Security_from_a_Defenders_Perspective_A_Unified_CVPR_2024_paper.pdf">Revamping Federated Learning Security from a Defender's Perspective: A<br />Unified Defense with Homomorphic Encrypted Data Space</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Kumar_Revamping_Federated_Learning_Security_from_a_Defenders_Perspective_A_Unified_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">14</td>
<td style="text-align: center;">2023-03-23</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Nguyen_NOPE_Novel_Object_Pose_Estimation_from_a_Single_Image_CVPR_2024_paper.pdf">NOPE: Novel Object Pose Estimation from a Single Image</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Nguyen_NOPE_Novel_Object_Pose_Estimation_from_a_Single_Image_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">14</td>
<td style="text-align: center;">2024-02-27</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Khan_CAD-SIGNet_CAD_Language_Inference_from_Point_Clouds_using_Layer-wise_Sketch_CVPR_2024_paper.pdf">CAD-SIGNet: CAD Language Inference from Point Clouds using Layer-wise<br />Sketch Instance Guided Attention</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Khan_CAD-SIGNet_CAD_Language_Inference_from_Point_Clouds_using_Layer-wise_Sketch_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">14</td>
<td style="text-align: center;">2024-05-19</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Jaume_Transcriptomics-guided_Slide_Representation_Learning_in_Computational_Pathology_CVPR_2024_paper.pdf">Transcriptomics-guided Slide Representation Learning in Computational Pathology</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Jaume_Transcriptomics-guided_Slide_Representation_Learning_in_Computational_Pathology_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">14</td>
<td style="text-align: center;">2024-01-16</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Liu_SCoFT_Self-Contrastive_Fine-Tuning_for_Equitable_Image_Generation_CVPR_2024_paper.pdf">SCoFT: Self-Contrastive Fine-Tuning for Equitable Image Generation</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Liu_SCoFT_Self-Contrastive_Fine-Tuning_for_Equitable_Image_Generation_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">14</td>
<td style="text-align: center;">2024-06-06</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Li_Matching_Anything_by_Segmenting_Anything_CVPR_2024_paper.pdf">Matching Anything by Segmenting Anything</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Li_Matching_Anything_by_Segmenting_Anything_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">14</td>
<td style="text-align: center;">2024-06-16</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Liu_DifFlow3D_Toward_Robust_Uncertainty-Aware_Scene_Flow_Estimation_with_Iterative_Diffusion-Based_CVPR_2024_paper.pdf">DifFlow3D: Toward Robust Uncertainty-Aware Scene Flow Estimation with Iterative<br />Diffusion-Based Refinement</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Liu_DifFlow3D_Toward_Robust_Uncertainty-Aware_Scene_Flow_Estimation_with_Iterative_Diffusion-Based_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">14</td>
<td style="text-align: center;">2023-12-04</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Luo_Readout_Guidance_Learning_Control_from_Diffusion_Features_CVPR_2024_paper.pdf">Readout Guidance: Learning Control from Diffusion Features</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Luo_Readout_Guidance_Learning_Control_from_Diffusion_Features_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">14</td>
<td style="text-align: center;">2024-03-21</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Liu_CFPL-FAS_Class_Free_Prompt_Learning_for_Generalizable_Face_Anti-spoofing_CVPR_2024_paper.pdf">CFPL-FAS: Class Free Prompt Learning for Generalizable Face Anti-spoofing</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Liu_CFPL-FAS_Class_Free_Prompt_Learning_for_Generalizable_Face_Anti-spoofing_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">14</td>
<td style="text-align: center;">2024-02-08</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Ganz_Question_Aware_Vision_Transformer_for_Multimodal_Reasoning_CVPR_2024_paper.pdf">Question Aware Vision Transformer for Multimodal Reasoning</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Ganz_Question_Aware_Vision_Transformer_for_Multimodal_Reasoning_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">14</td>
<td style="text-align: center;">2024-03-12</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Zhu_Beyond_Text_Frozen_Large_Language_Models_in_Visual_Signal_Comprehension_CVPR_2024_paper.pdf">Beyond Text: Frozen Large Language Models in Visual Signal<br />Comprehension</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Zhu_Beyond_Text_Frozen_Large_Language_Models_in_Visual_Signal_Comprehension_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">14</td>
<td style="text-align: center;">2023-06-21</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Ristea_Self-Distilled_Masked_Auto-Encoders_are_Efficient_Video_Anomaly_Detectors_CVPR_2024_paper.pdf">Self-Distilled Masked Auto-Encoders are Efficient Video Anomaly Detectors</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Ristea_Self-Distilled_Masked_Auto-Encoders_are_Efficient_Video_Anomaly_Detectors_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">14</td>
<td style="text-align: center;">2024-01-15</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Yan_MaskClustering_View_Consensus_based_Mask_Graph_Clustering_for_Open-Vocabulary_3D_CVPR_2024_paper.pdf">MaskClustering: View Consensus based Mask Graph Clustering for Open-Vocabulary<br />3D Instance Segmentation</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Yan_MaskClustering_View_Consensus_based_Mask_Graph_Clustering_for_Open-Vocabulary_3D_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">14</td>
<td style="text-align: center;">2024-03-15</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Li_Lodge_A_Coarse_to_Fine_Diffusion_Network_for_Long_Dance_CVPR_2024_paper.pdf">Lodge: A Coarse to Fine Diffusion Network for Long<br />Dance Generation Guided by the Characteristic Dance Primitives</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Li_Lodge_A_Coarse_to_Fine_Diffusion_Network_for_Long_Dance_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">14</td>
<td style="text-align: center;">2024-03-26</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Luo_LaRE2_Latent_Reconstruction_Error_Based_Method_for_Diffusion-Generated_Image_Detection_CVPR_2024_paper.pdf">LaRE^2: Latent Reconstruction Error Based Method for Diffusion-Generated Image<br />Detection</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Luo_LaRE2_Latent_Reconstruction_Error_Based_Method_for_Diffusion-Generated_Image_Detection_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">14</td>
<td style="text-align: center;">2024-05-07</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Dou_Tactile-Augmented_Radiance_Fields_CVPR_2024_paper.pdf">Tactile-Augmented Radiance Fields</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Dou_Tactile-Augmented_Radiance_Fields_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">14</td>
<td style="text-align: center;">2024-02-27</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Xue_Accelerating_Diffusion_Sampling_with_Optimized_Time_Steps_CVPR_2024_paper.pdf">Accelerating Diffusion Sampling with Optimized Time Steps</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Xue_Accelerating_Diffusion_Sampling_with_Optimized_Time_Steps_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">14</td>
<td style="text-align: center;">2024-03-31</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Ran_Towards_Realistic_Scene_Generation_with_LiDAR_Diffusion_Models_CVPR_2024_paper.pdf">Towards Realistic Scene Generation with LiDAR Diffusion Models</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Ran_Towards_Realistic_Scene_Generation_with_LiDAR_Diffusion_Models_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">14</td>
<td style="text-align: center;">2024-03-28</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Zhou_Test-Time_Domain_Generalization_for_Face_Anti-Spoofing_CVPR_2024_paper.pdf">Test-Time Domain Generalization for Face Anti-Spoofing</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Zhou_Test-Time_Domain_Generalization_for_Face_Anti-Spoofing_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">14</td>
<td style="text-align: center;">2024-02-12</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Song_Collaborative_Semantic_Occupancy_Prediction_with_Hybrid_Feature_Fusion_in_Connected_CVPR_2024_paper.pdf">Collaborative Semantic Occupancy Prediction with Hybrid Feature Fusion in<br />Connected Automated Vehicles</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Song_Collaborative_Semantic_Occupancy_Prediction_with_Hybrid_Feature_Fusion_in_Connected_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">14</td>
<td style="text-align: center;">2023-12-14</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Xu_Pixel-Aligned_Language_Model_CVPR_2024_paper.pdf">Pixel-Aligned Language Model</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Xu_Pixel-Aligned_Language_Model_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">14</td>
<td style="text-align: center;">2024-03-25</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Chen_VP3D_Unleashing_2D_Visual_Prompt_for_Text-to-3D_Generation_CVPR_2024_paper.pdf">VP3D: Unleashing 2D Visual Prompt for Text-to-3D Generation</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Chen_VP3D_Unleashing_2D_Visual_Prompt_for_Text-to-3D_Generation_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">14</td>
<td style="text-align: center;">2024-05-19</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Song_Morphological_Prototyping_for_Unsupervised_Slide_Representation_Learning_in_Computational_Pathology_CVPR_2024_paper.pdf">Morphological Prototyping for Unsupervised Slide Representation Learning in Computational<br />Pathology</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Song_Morphological_Prototyping_for_Unsupervised_Slide_Representation_Learning_in_Computational_Pathology_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">14</td>
<td style="text-align: center;">2024-05-02</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Liu_Multi-Space_Alignments_Towards_Universal_LiDAR_Segmentation_CVPR_2024_paper.pdf">Multi-Space Alignments Towards Universal LiDAR Segmentation</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Liu_Multi-Space_Alignments_Towards_Universal_LiDAR_Segmentation_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">14</td>
<td style="text-align: center;">2023-03-24</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Tang_DiffuScene_Denoising_Diffusion_Models_for_Generative_Indoor_Scene_Synthesis_CVPR_2024_paper.pdf">DiffuScene: Denoising Diffusion Models for Generative Indoor Scene Synthesis</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Tang_DiffuScene_Denoising_Diffusion_Models_for_Generative_Indoor_Scene_Synthesis_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">14</td>
<td style="text-align: center;">2024-04-18</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Ge_On_the_Content_Bias_in_Frechet_Video_Distance_CVPR_2024_paper.pdf">On the Content Bias in Frechet Video Distance</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Ge_On_the_Content_Bias_in_Frechet_Video_Distance_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">14</td>
<td style="text-align: center;">2023-12-14</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Shi_ZeroRF_Fast_Sparse_View_360deg_Reconstruction_with_Zero_Pretraining_CVPR_2024_paper.pdf">ZeroRF: Fast Sparse View 360deg Reconstruction with Zero Pretraining</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Shi_ZeroRF_Fast_Sparse_View_360deg_Reconstruction_with_Zero_Pretraining_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">14</td>
<td style="text-align: center;">2023-05-30</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Wang_Towards_Accurate_Post-training_Quantization_for_Diffusion_Models_CVPR_2024_paper.pdf">Towards Accurate Post-training Quantization for Diffusion Models</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Wang_Towards_Accurate_Post-training_Quantization_for_Diffusion_Models_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">14</td>
<td style="text-align: center;">2024-03-22</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Yan_Tri-Perspective_View_Decomposition_for_Geometry-Aware_Depth_Completion_CVPR_2024_paper.pdf">Tri-Perspective View Decomposition for Geometry-Aware Depth Completion</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Yan_Tri-Perspective_View_Decomposition_for_Geometry-Aware_Depth_Completion_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">14</td>
<td style="text-align: center;">2024-01-09</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Wang_Pre-trained_Model_Guided_Fine-Tuning_for_Zero-Shot_Adversarial_Robustness_CVPR_2024_paper.pdf">Pre-trained Model Guided Fine-Tuning for Zero-Shot Adversarial Robustness</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Wang_Pre-trained_Model_Guided_Fine-Tuning_for_Zero-Shot_Adversarial_Robustness_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">14</td>
<td style="text-align: center;">2024-04-03</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Li_On_the_Scalability_of_Diffusion-based_Text-to-Image_Generation_CVPR_2024_paper.pdf">On the Scalability of Diffusion-based Text-to-Image Generation</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Li_On_the_Scalability_of_Diffusion-based_Text-to-Image_Generation_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">14</td>
<td style="text-align: center;">2024-03-05</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/He_Multi-modal_Instruction_Tuned_LLMs_with_Fine-grained_Visual_Perception_CVPR_2024_paper.pdf">Multi-modal Instruction Tuned LLMs with Fine-grained Visual Perception</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/He_Multi-modal_Instruction_Tuned_LLMs_with_Fine-grained_Visual_Perception_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">14</td>
<td style="text-align: center;">2024-05-11</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Rahman_EMCAD_Efficient_Multi-scale_Convolutional_Attention_Decoding_for_Medical_Image_Segmentation_CVPR_2024_paper.pdf">EMCAD: Efficient Multi-scale Convolutional Attention Decoding for Medical Image<br />Segmentation</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Rahman_EMCAD_Efficient_Multi-scale_Convolutional_Attention_Decoding_for_Medical_Image_Segmentation_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">14</td>
<td style="text-align: center;">2023-11-27</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Lei_ViT-Lens_Towards_Omni-modal_Representations_CVPR_2024_paper.pdf">ViT-Lens: Towards Omni-modal Representations</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Lei_ViT-Lens_Towards_Omni-modal_Representations_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">14</td>
<td style="text-align: center;">2024-03-01</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Liu_Diff-Plugin_Revitalizing_Details_for_Diffusion-based_Low-level_Tasks_CVPR_2024_paper.pdf">Diff-Plugin: Revitalizing Details for Diffusion-based Low-level Tasks</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Liu_Diff-Plugin_Revitalizing_Details_for_Diffusion-based_Low-level_Tasks_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">14</td>
<td style="text-align: center;">2024-02-22</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Wang_CyberDemo_Augmenting_Simulated_Human_Demonstration_for_Real-World_Dexterous_Manipulation_CVPR_2024_paper.pdf">CyberDemo: Augmenting Simulated Human Demonstration for Real-World Dexterous Manipulation</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Wang_CyberDemo_Augmenting_Simulated_Human_Demonstration_for_Real-World_Dexterous_Manipulation_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">14</td>
<td style="text-align: center;">2024-01-04</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Saha_Improved_Zero-Shot_Classification_by_Adapting_VLMs_with_Text_Descriptions_CVPR_2024_paper.pdf">Improved Zero-Shot Classification by Adapting VLMs with Text Descriptions</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Saha_Improved_Zero-Shot_Classification_by_Adapting_VLMs_with_Text_Descriptions_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">14</td>
<td style="text-align: center;">2023-12-07</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Patel_ECLIPSE_A_Resource-Efficient_Text-to-Image_Prior_for_Image_Generations_CVPR_2024_paper.pdf">ECLIPSE: A Resource-Efficient Text-to-Image Prior for Image Generations</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Patel_ECLIPSE_A_Resource-Efficient_Text-to-Image_Prior_for_Image_Generations_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">14</td>
<td style="text-align: center;">2024-06-16</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Weng_PARA-Drive_Parallelized_Architecture_for_Real-time_Autonomous_Driving_CVPR_2024_paper.pdf">PARA-Drive: Parallelized Architecture for Real-time Autonomous Driving</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Weng_PARA-Drive_Parallelized_Architecture_for_Real-time_Autonomous_Driving_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">14</td>
<td style="text-align: center;">2024-01-16</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Zhang_Transcending_the_Limit_of_Local_Window_Advanced_Super-Resolution_Transformer_with_CVPR_2024_paper.pdf">Transcending the Limit of Local Window: Advanced Super-Resolution Transformer<br />with Adaptive Token Dictionary</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Zhang_Transcending_the_Limit_of_Local_Window_Advanced_Super-Resolution_Transformer_with_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">14</td>
<td style="text-align: center;">2024-03-17</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Peng_A_Dual-Augmentor_Framework_for_Domain_Generalization_in_3D_Human_Pose_CVPR_2024_paper.pdf">A Dual-Augmentor Framework for Domain Generalization in 3D Human<br />Pose Estimation</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Peng_A_Dual-Augmentor_Framework_for_Domain_Generalization_in_3D_Human_Pose_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">13</td>
<td style="text-align: center;">2023-06-15</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Alzayer_Seeing_the_World_through_Your_Eyes_CVPR_2024_paper.pdf">Seeing the World through Your Eyes</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Alzayer_Seeing_the_World_through_Your_Eyes_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">13</td>
<td style="text-align: center;">2024-03-19</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Wang_Zero-Reference_Low-Light_Enhancement_via_Physical_Quadruple_Priors_CVPR_2024_paper.pdf">Zero-Reference Low-Light Enhancement via Physical Quadruple Priors</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Wang_Zero-Reference_Low-Light_Enhancement_via_Physical_Quadruple_Priors_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">13</td>
<td style="text-align: center;">2024-04-05</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Retsinas_3D_Facial_Expressions_through_Analysis-by-Neural-Synthesis_CVPR_2024_paper.pdf">3D Facial Expressions through Analysis-by-Neural-Synthesis</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Retsinas_3D_Facial_Expressions_through_Analysis-by-Neural-Synthesis_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">13</td>
<td style="text-align: center;">2023-11-28</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Lin_Text-Driven_Image_Editing_via_Learnable_Regions_CVPR_2024_paper.pdf">Text-Driven Image Editing via Learnable Regions</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Lin_Text-Driven_Image_Editing_via_Learnable_Regions_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">13</td>
<td style="text-align: center;">2024-06-13</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Vuong_Language-driven_Grasp_Detection_CVPR_2024_paper.pdf">Language-driven Grasp Detection</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Vuong_Language-driven_Grasp_Detection_CVPR_2024_paper.html">link</a></td>
<td style="text-align: left;"></td>
</tr>
</tbody>
</table>
              
            </div>
          </div><footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="Footer Navigation">
        <a href="../../COLM/COLM_2024/" class="btn btn-neutral float-left" title="COLM 2024"><span class="icon icon-circle-arrow-left"></span> Previous</a>
        <a href="../../EMNLP/EMNLP_2024/" class="btn btn-neutral float-right" title="EMNLP 2024">Next <span class="icon icon-circle-arrow-right"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
          
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="Versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    
    
      <span><a href="../../COLM/COLM_2024/" style="color: #fcfcfc">&laquo; Previous</a></span>
    
    
      <span><a href="../../EMNLP/EMNLP_2024/" style="color: #fcfcfc">Next &raquo;</a></span>
    
  </span>
</div>
    <script src="../../js/jquery-3.6.0.min.js"></script>
    <script>var base_url = "../..";</script>
    <script src="../../js/theme_extra.js"></script>
    <script src="../../js/theme.js"></script>
      <script src="../../search/main.js"></script>
    <script>
        jQuery(function () {
            SphinxRtdTheme.Navigation.enable(true);
        });
    </script>

</body>
</html>
